{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(0.,80.,0.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.sin(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as plb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b05eed0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ/h/cZoCbO/FOxoAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x103c43fd0>"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_rand = y + 0.1 * np.random.randn(len(y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,y_rand)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b0de090>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWCoQxjRNxgMhjrE/wfsznHHqYSA2gAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a4660d0>"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(optimize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on package scipy.optimize in scipy:\n",
        "\n",
        "NAME\n",
        "    scipy.optimize\n",
        "\n",
        "FILE\n",
        "    /Users/mlightma/anaconda/lib/python2.7/site-packages/scipy/optimize/__init__.py\n",
        "\n",
        "DESCRIPTION\n",
        "    =====================================================\n",
        "    Optimization and root finding (:mod:`scipy.optimize`)\n",
        "    =====================================================\n",
        "    \n",
        "    .. currentmodule:: scipy.optimize\n",
        "    \n",
        "    Optimization\n",
        "    ============\n",
        "    \n",
        "    General-purpose\n",
        "    ---------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize - Unified interface for minimizers of multivariate functions\n",
        "       fmin - Nelder-Mead Simplex algorithm\n",
        "       fmin_powell - Powell's (modified) level set method\n",
        "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm\n",
        "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno)\n",
        "       fmin_ncg - Line-search Newton Conjugate Gradient\n",
        "       leastsq - Minimize the sum of squares of M equations in N unknowns\n",
        "    \n",
        "    Constrained (multivariate)\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer\n",
        "       fmin_tnc - Truncated Newton code\n",
        "       fmin_cobyla - Constrained optimization by linear approximation\n",
        "       fmin_slsqp - Minimization using sequential least-squares programming\n",
        "       nnls - Linear least-squares problem with non-negativity constraint\n",
        "    \n",
        "    Global\n",
        "    ------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       anneal - Simulated annealing\n",
        "       basinhopping - Basinhopping stochastic optimizer\n",
        "       brute - Brute force searching optimizer\n",
        "    \n",
        "    Scalar function minimizers\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize_scalar - Unified interface for minimizers of univariate functions\n",
        "       fminbound - Bounded minimization of a scalar function\n",
        "       brent - 1-D function minimization using Brent method\n",
        "       golden - 1-D function minimization using Golden Section method\n",
        "       bracket - Bracket a minimum, given two starting points\n",
        "    \n",
        "    Rosenbrock function\n",
        "    -------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       rosen - The Rosenbrock function.\n",
        "       rosen_der - The derivative of the Rosenbrock function.\n",
        "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
        "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
        "    \n",
        "    Fitting\n",
        "    =======\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       curve_fit -- Fit curve to a set of points\n",
        "    \n",
        "    Root finding\n",
        "    ============\n",
        "    \n",
        "    Scalar functions\n",
        "    ----------------\n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       brentq - quadratic interpolation Brent method\n",
        "       brenth - Brent method, modified by Harris with hyperbolic extrapolation\n",
        "       ridder - Ridder's method\n",
        "       bisect - Bisection method\n",
        "       newton - Secant method or Newton's method\n",
        "    \n",
        "    Fixed point finding:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fixed_point - Single-variable fixed-point solver\n",
        "    \n",
        "    Multidimensional\n",
        "    ----------------\n",
        "    \n",
        "    General nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       root - Unified interface for nonlinear solvers of multivariate functions\n",
        "       fsolve - Non-linear multi-variable equation solver\n",
        "       broyden1 - Broyden's first method\n",
        "       broyden2 - Broyden's second method\n",
        "    \n",
        "    Large-scale nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       newton_krylov\n",
        "       anderson\n",
        "    \n",
        "    Simple iterations:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       excitingmixing\n",
        "       linearmixing\n",
        "       diagbroyden\n",
        "    \n",
        "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
        "    \n",
        "    Utility Functions\n",
        "    =================\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       line_search - Return a step that satisfies the strong Wolfe conditions\n",
        "       check_grad - Check the supplied derivative using finite differences\n",
        "    \n",
        "       show_options - Show specific options optimization solvers\n",
        "\n",
        "PACKAGE CONTENTS\n",
        "    _basinhopping\n",
        "    _cobyla\n",
        "    _lbfgsb\n",
        "    _minimize\n",
        "    _minpack\n",
        "    _nnls\n",
        "    _root\n",
        "    _slsqp\n",
        "    _trustregion\n",
        "    _trustregion_dogleg\n",
        "    _trustregion_ncg\n",
        "    _tstutils\n",
        "    _zeros\n",
        "    anneal\n",
        "    cobyla\n",
        "    lbfgsb\n",
        "    linesearch\n",
        "    minpack\n",
        "    minpack2\n",
        "    moduleTNC\n",
        "    nnls\n",
        "    nonlin\n",
        "    optimize\n",
        "    setup\n",
        "    slsqp\n",
        "    tnc\n",
        "    zeros\n",
        "\n",
        "CLASSES\n",
        "    __builtin__.dict(__builtin__.object)\n",
        "        scipy.optimize.optimize.Result\n",
        "    exceptions.UserWarning(exceptions.Warning)\n",
        "        scipy.optimize.optimize.OptimizeWarning\n",
        "    \n",
        "    class OptimizeWarning(exceptions.UserWarning)\n",
        "     |  Method resolution order:\n",
        "     |      OptimizeWarning\n",
        "     |      exceptions.UserWarning\n",
        "     |      exceptions.Warning\n",
        "     |      exceptions.Exception\n",
        "     |      exceptions.BaseException\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __delattr__(...)\n",
        "     |      x.__delattr__('name') <==> del x.name\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __getslice__(...)\n",
        "     |      x.__getslice__(i, j) <==> x[i:j]\n",
        "     |      \n",
        "     |      Use of negative indices is not supported.\n",
        "     |  \n",
        "     |  __reduce__(...)\n",
        "     |  \n",
        "     |  __repr__(...)\n",
        "     |      x.__repr__() <==> repr(x)\n",
        "     |  \n",
        "     |  __setattr__(...)\n",
        "     |      x.__setattr__('name', value) <==> x.name = value\n",
        "     |  \n",
        "     |  __setstate__(...)\n",
        "     |  \n",
        "     |  __str__(...)\n",
        "     |      x.__str__() <==> str(x)\n",
        "     |  \n",
        "     |  __unicode__(...)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |  \n",
        "     |  args\n",
        "     |  \n",
        "     |  message\n",
        "    \n",
        "    class Result(__builtin__.dict)\n",
        "     |  Represents the optimization result.\n",
        "     |  \n",
        "     |  Attributes\n",
        "     |  ----------\n",
        "     |  x : ndarray\n",
        "     |      The solution of the optimization.\n",
        "     |  success : bool\n",
        "     |      Whether or not the optimizer exited successfully.\n",
        "     |  status : int\n",
        "     |      Termination status of the optimizer. Its value depends on the\n",
        "     |      underlying solver. Refer to `message` for details.\n",
        "     |  message : str\n",
        "     |      Description of the cause of the termination.\n",
        "     |  fun, jac, hess, hess_inv : ndarray\n",
        "     |      Values of objective function, Jacobian, Hessian or its inverse (if\n",
        "     |      available). The Hessians may be approximations, see the documentation\n",
        "     |      of the function in question.\n",
        "     |  nfev, njev, nhev : int\n",
        "     |      Number of evaluations of the objective functions and of its\n",
        "     |      Jacobian and Hessian.\n",
        "     |  nit : int\n",
        "     |      Number of iterations performed by the optimizer.\n",
        "     |  maxcv : float\n",
        "     |      The maximum constraint violation.\n",
        "     |  \n",
        "     |  Notes\n",
        "     |  -----\n",
        "     |  There may be additional attributes not listed above depending of the\n",
        "     |  specific solver. Since this class is essentially a subclass of dict\n",
        "     |  with attribute accessors, one can see which attributes are available\n",
        "     |  using the `keys()` method.\n",
        "     |  \n",
        "     |  Method resolution order:\n",
        "     |      Result\n",
        "     |      __builtin__.dict\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __delattr__ = __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __getattr__(self, name)\n",
        "     |  \n",
        "     |  __repr__(self)\n",
        "     |  \n",
        "     |  __setattr__ = __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |      dictionary for instance variables (if defined)\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __cmp__(...)\n",
        "     |      x.__cmp__(y) <==> cmp(x,y)\n",
        "     |  \n",
        "     |  __contains__(...)\n",
        "     |      D.__contains__(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __eq__(...)\n",
        "     |      x.__eq__(y) <==> x==y\n",
        "     |  \n",
        "     |  __ge__(...)\n",
        "     |      x.__ge__(y) <==> x>=y\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __gt__(...)\n",
        "     |      x.__gt__(y) <==> x>y\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  __iter__(...)\n",
        "     |      x.__iter__() <==> iter(x)\n",
        "     |  \n",
        "     |  __le__(...)\n",
        "     |      x.__le__(y) <==> x<=y\n",
        "     |  \n",
        "     |  __len__(...)\n",
        "     |      x.__len__() <==> len(x)\n",
        "     |  \n",
        "     |  __lt__(...)\n",
        "     |      x.__lt__(y) <==> x<y\n",
        "     |  \n",
        "     |  __ne__(...)\n",
        "     |      x.__ne__(y) <==> x!=y\n",
        "     |  \n",
        "     |  __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  __sizeof__(...)\n",
        "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
        "     |  \n",
        "     |  clear(...)\n",
        "     |      D.clear() -> None.  Remove all items from D.\n",
        "     |  \n",
        "     |  copy(...)\n",
        "     |      D.copy() -> a shallow copy of D\n",
        "     |  \n",
        "     |  fromkeys(...)\n",
        "     |      dict.fromkeys(S[,v]) -> New dict with keys from S and values equal to v.\n",
        "     |      v defaults to None.\n",
        "     |  \n",
        "     |  get(...)\n",
        "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
        "     |  \n",
        "     |  has_key(...)\n",
        "     |      D.has_key(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  items(...)\n",
        "     |      D.items() -> list of D's (key, value) pairs, as 2-tuples\n",
        "     |  \n",
        "     |  iteritems(...)\n",
        "     |      D.iteritems() -> an iterator over the (key, value) items of D\n",
        "     |  \n",
        "     |  iterkeys(...)\n",
        "     |      D.iterkeys() -> an iterator over the keys of D\n",
        "     |  \n",
        "     |  itervalues(...)\n",
        "     |      D.itervalues() -> an iterator over the values of D\n",
        "     |  \n",
        "     |  keys(...)\n",
        "     |      D.keys() -> list of D's keys\n",
        "     |  \n",
        "     |  pop(...)\n",
        "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
        "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
        "     |  \n",
        "     |  popitem(...)\n",
        "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
        "     |      2-tuple; but raise KeyError if D is empty.\n",
        "     |  \n",
        "     |  setdefault(...)\n",
        "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
        "     |  \n",
        "     |  update(...)\n",
        "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
        "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
        "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
        "     |      In either case, this is followed by: for k in F: D[k] = F[k]\n",
        "     |  \n",
        "     |  values(...)\n",
        "     |      D.values() -> list of D's values\n",
        "     |  \n",
        "     |  viewitems(...)\n",
        "     |      D.viewitems() -> a set-like object providing a view on D's items\n",
        "     |  \n",
        "     |  viewkeys(...)\n",
        "     |      D.viewkeys() -> a set-like object providing a view on D's keys\n",
        "     |  \n",
        "     |  viewvalues(...)\n",
        "     |      D.viewvalues() -> an object providing a view on D's values\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __hash__ = None\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "\n",
        "FUNCTIONS\n",
        "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using (extended) Anderson mixing.\n",
        "        \n",
        "        The Jacobian is formed by for a 'best' solution in the space\n",
        "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
        "        inversions and MxN multiplications are required. [Ey]_\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        M : float, optional\n",
        "            Number of previous vectors to retain. Defaults to 5.\n",
        "        w0 : float, optional\n",
        "            Regularization parameter for numerical stability.\n",
        "            Compared to unity, good values of the order of 0.01.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
        "    \n",
        "    anneal(func, x0, args=(), schedule='fast', full_output=0, T0=None, Tf=1e-12, maxeval=None, maxaccept=None, maxiter=400, boltzmann=1.0, learn_rate=0.5, feps=1e-06, quench=1.0, m=1.0, n=1.0, lower=-100, upper=100, dwell=50, disp=True)\n",
        "        Minimize a function using simulated annealing.\n",
        "        \n",
        "        Uses simulated annealing, a random algorithm that uses no derivative\n",
        "        information from the function being optimized. Other names for this\n",
        "        family of approaches include: \"Monte Carlo\", \"Metropolis\",\n",
        "        \"Metropolis-Hastings\", `etc`. They all involve (a) evaluating the\n",
        "        objective function on a random set of points, (b) keeping those that\n",
        "        pass their randomized evaluation critera, (c) cooling (`i.e.`,\n",
        "        tightening) the evaluation critera, and (d) repeating until their\n",
        "        termination critera are met.  In practice they have been used mainly in\n",
        "        discrete rather than in continuous optimization.\n",
        "        \n",
        "        Available annealing schedules are 'fast', 'cauchy' and 'boltzmann'.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized.  Must be in the form\n",
        "            `f(x, *args)`, where `x` is the argument in the form of a 1-D array\n",
        "            and `args` is a  tuple of any additional fixed parameters needed to\n",
        "            completely specify the function.\n",
        "        x0: 1-D array\n",
        "            An initial guess at the optimizing argument of `func`.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely\n",
        "            specify the objective function.\n",
        "        schedule : str, optional\n",
        "            The annealing schedule to use.  Must be one of 'fast', 'cauchy' or\n",
        "            'boltzmann'.  See `Notes`.\n",
        "        full_output : bool, optional\n",
        "            If `full_output`, then return all values listed in the Returns\n",
        "            section. Otherwise, return just the `xmin` and `status` values.\n",
        "        T0 : float, optional\n",
        "            The initial \"temperature\".  If None, then estimate it as 1.2 times\n",
        "            the largest cost-function deviation over random points in the\n",
        "            box-shaped region specified by the `lower, upper` input parameters.\n",
        "        Tf : float, optional\n",
        "            Final goal temperature.  Cease iterations if the temperature\n",
        "            falls below `Tf`.\n",
        "        maxeval : int, optional\n",
        "            Cease iterations if the number of function evaluations exceeds\n",
        "            `maxeval`.\n",
        "        maxaccept : int, optional\n",
        "            Cease iterations if the number of points accepted exceeds `maxaccept`.\n",
        "            See `Notes` for the probabilistic acceptance criteria used.\n",
        "        maxiter : int, optional\n",
        "            Cease iterations if the number of cooling iterations exceeds `maxiter`.\n",
        "        learn_rate : float, optional\n",
        "            Scale constant for tuning the probabilistc acceptance criteria.\n",
        "        boltzmann : float, optional\n",
        "            Boltzmann constant in the probabilistic acceptance criteria\n",
        "            (increase for less stringent criteria at each temperature).\n",
        "        feps : float, optional\n",
        "            Cease iterations if the relative errors in the function value over the\n",
        "            last four coolings is below `feps`.\n",
        "        quench, m, n : floats, optional\n",
        "            Parameters to alter the `fast` simulated annealing schedule.\n",
        "            See `Notes`.\n",
        "        lower, upper : floats or 1-D arrays, optional\n",
        "            Lower and upper bounds on the argument `x`.  If floats are provided,\n",
        "            they apply to all components of `x`.\n",
        "        dwell : int, optional\n",
        "            The number of times to execute the inner loop at each value of the\n",
        "            temperature.  See `Notes`.\n",
        "        disp : bool, optional\n",
        "            Print a descriptive convergence message if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            The point where the lowest function value was found.\n",
        "        Jmin : float\n",
        "            The objective function value at `xmin`.\n",
        "        T : float\n",
        "            The temperature at termination of the iterations.\n",
        "        feval : int\n",
        "            Number of function evaluations used.\n",
        "        iters : int\n",
        "            Number of cooling iterations used.\n",
        "        accept : int\n",
        "            Number of tests accepted.\n",
        "        status : int\n",
        "            A code indicating the reason for termination:\n",
        "        \n",
        "            - 0 : Points no longer changing.\n",
        "            - 1 : Cooled to final temperature.\n",
        "            - 2 : Maximum function evaluations reached.\n",
        "            - 3 : Maximum cooling iterations reached.\n",
        "            - 4 : Maximum accepted query locations reached.\n",
        "            - 5 : Final point not the minimum amongst encountered points.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        basinhopping : another (more performant) global optimizer\n",
        "        brute : brute-force global optimizer\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Simulated annealing is a random algorithm which uses no derivative\n",
        "        information from the function being optimized. In practice it has\n",
        "        been more useful in discrete optimization than continuous\n",
        "        optimization, as there are usually better algorithms for continuous\n",
        "        optimization problems.\n",
        "        \n",
        "        Some experimentation by trying the different temperature\n",
        "        schedules and altering their parameters is likely required to\n",
        "        obtain good performance.\n",
        "        \n",
        "        The randomness in the algorithm comes from random sampling in numpy.\n",
        "        To obtain the same results you can call `numpy.random.seed` with the\n",
        "        same seed immediately before calling `anneal`.\n",
        "        \n",
        "        We give a brief description of how the three temperature schedules\n",
        "        generate new points and vary their temperature.  Temperatures are\n",
        "        only updated with iterations in the outer loop.  The inner loop is\n",
        "        over loop over ``xrange(dwell)``, and new points are generated for\n",
        "        every iteration in the inner loop.  Whether the proposed new points\n",
        "        are accepted is probabilistic.\n",
        "        \n",
        "        For readability, let ``d`` denote the dimension of the inputs to func.\n",
        "        Also, let ``x_old`` denote the previous state, and ``k`` denote the\n",
        "        iteration number of the outer loop.  All other variables not\n",
        "        defined below are input variables to `anneal` itself.\n",
        "        \n",
        "        In the 'fast' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(0, 1, size = d)\n",
        "            y = sgn(u - 0.5) * T * ((1 + 1/T)**abs(2*u - 1) - 1.0)\n",
        "        \n",
        "            xc = y * (upper - lower)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            c = n * exp(-n * quench)\n",
        "            T_new = T0 * exp(-c * k**quench)\n",
        "        \n",
        "        In the 'cauchy' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(-pi/2, pi/2, size=d)\n",
        "            xc = learn_rate * T * tan(u)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            T_new = T0 / (1 + k)\n",
        "        \n",
        "        In the 'boltzmann' schedule the updates are::\n",
        "        \n",
        "            std = minimum(sqrt(T) * ones(d), (upper - lower) / (3*learn_rate))\n",
        "            y ~ Normal(0, std, size = d)\n",
        "            x_new = x_old + learn_rate * y\n",
        "        \n",
        "            T_new = T0 / log(1 + k)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        [1] P. J. M. van Laarhoven and E. H. L. Aarts, \"Simulated Annealing: Theory\n",
        "            and Applications\", Kluwer Academic Publishers, 1987.\n",
        "        \n",
        "        [2] W.H. Press et al., \"Numerical Recipies: The Art of Scientific Computing\",\n",
        "            Cambridge U. Press, 1987.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        *Example 1.* We illustrate the use of `anneal` to seek the global minimum\n",
        "        of a function of two variables that is equal to the sum of a positive-\n",
        "        definite quadratic and two deep \"Gaussian-shaped\" craters.  Specifically,\n",
        "        define the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``, ``params``, and the functions are\n",
        "        as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        >>> x0 = np.array([2., 2.])     # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(555)   # Seeded to allow replication.\n",
        "        >>> res = optimize.anneal(f, x0, args=params, schedule='boltzmann',\n",
        "                                  full_output=True, maxiter=500, lower=-10,\n",
        "                                  upper=10, dwell=250, disp=True)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res[0]  # obtained minimum\n",
        "        array([-1.03914194,  1.81330654])\n",
        "        >>> res[1]  # function value at minimum\n",
        "        -3.3817...\n",
        "        \n",
        "        So this run settled on the point [-1.039, 1.813] with a minimum function\n",
        "        value of about -3.382.  The final temperature was about 212. The run used\n",
        "        125301 function evaluations, 501 iterations (including the initial guess as\n",
        "        a iteration), and accepted 61162 points. The status flag of 3 also\n",
        "        indicates that `maxiter` was reached.\n",
        "        \n",
        "        This problem's true global minimum lies near the point [-1.057, 1.808]\n",
        "        and has a value of about -3.409.  So these `anneal` results are pretty\n",
        "        good and could be used as the starting guess in a local optimizer to\n",
        "        seek a more exact local minimum.\n",
        "        \n",
        "        *Example 2.* To minimize the same objective function using\n",
        "        the `minimize` approach, we need to (a) convert the options to an\n",
        "        \"options dictionary\" using the keys prescribed for this method,\n",
        "        (b) call the `minimize` function with the name of the method (which\n",
        "        in this case is 'Anneal'), and (c) take account of the fact that\n",
        "        the returned value will be a `Result` object (`i.e.`, a dictionary,\n",
        "        as defined in `optimize.py`).\n",
        "        \n",
        "        All of the allowable options for 'Anneal' when using the `minimize`\n",
        "        approach are listed in the ``myopts`` dictionary given below, although\n",
        "        in practice only the non-default values would be needed.  Some of their\n",
        "        names differ from those used in the `anneal` approach.  We can proceed\n",
        "        as follows:\n",
        "        \n",
        "        >>> myopts = {\n",
        "                'schedule'     : 'boltzmann',   # Non-default value.\n",
        "                'maxfev'       : None,  # Default, formerly `maxeval`.\n",
        "                'maxiter'      : 500,   # Non-default value.\n",
        "                'maxaccept'    : None,  # Default value.\n",
        "                'ftol'         : 1e-6,  # Default, formerly `feps`.\n",
        "                'T0'           : None,  # Default value.\n",
        "                'Tf'           : 1e-12, # Default value.\n",
        "                'boltzmann'    : 1.0,   # Default value.\n",
        "                'learn_rate'   : 0.5,   # Default value.\n",
        "                'quench'       : 1.0,   # Default value.\n",
        "                'm'            : 1.0,   # Default value.\n",
        "                'n'            : 1.0,   # Default value.\n",
        "                'lower'        : -10,   # Non-default value.\n",
        "                'upper'        : +10,   # Non-default value.\n",
        "                'dwell'        : 250,   # Non-default value.\n",
        "                'disp'         : True   # Default value.\n",
        "                }\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(777)  # Seeded to allow replication.\n",
        "        >>> res2 = optimize.minimize(f, x0, args=params, method='Anneal',\n",
        "                                     options=myopts)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res2\n",
        "          status: 3\n",
        "         success: False\n",
        "          accept: 61742\n",
        "            nfev: 125301\n",
        "               T: 214.20624873839623\n",
        "             fun: -3.4084065576676053\n",
        "               x: array([-1.05757366,  1.8071427 ])\n",
        "         message: 'Maximum cooling iterations reached'\n",
        "         nit: 501\n",
        "    \n",
        "    approx_fprime(xk, f, epsilon, *args)\n",
        "        Finite-difference approximation of the gradient of a scalar function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        xk : array_like\n",
        "            The coordinate vector at which to determine the gradient of `f`.\n",
        "        f : callable\n",
        "            The function of which to determine the gradient (partial derivatives).\n",
        "            Should take `xk` as first argument, other arguments to `f` can be\n",
        "            supplied in ``*args``.  Should return a scalar, the value of the\n",
        "            function at `xk`.\n",
        "        epsilon : array_like\n",
        "            Increment to `xk` to use for determining the function gradient.\n",
        "            If a scalar, uses the same finite difference delta for all partial\n",
        "            derivatives.  If an array, should contain one value per element of\n",
        "            `xk`.\n",
        "        \\*args : args, optional\n",
        "            Any other arguments that are to be passed to `f`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        grad : ndarray\n",
        "            The partial derivatives of `f` to `xk`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        check_grad : Check correctness of gradient function against approx_fprime.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The function gradient is determined by the forward finite difference\n",
        "        formula::\n",
        "        \n",
        "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
        "            f'[i] = ---------------------------------\n",
        "                                epsilon[i]\n",
        "        \n",
        "        The main use of `approx_fprime` is in scalar function optimizers like\n",
        "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c0, c1):\n",
        "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
        "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
        "        \n",
        "        >>> x = np.ones(2)\n",
        "        >>> c0, c1 = (1, 200)\n",
        "        >>> eps = np.sqrt(np.finfo(np.float).eps)\n",
        "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
        "        array([   2.        ,  400.00004198])\n",
        "    \n",
        "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None)\n",
        "        Find the global minimum of a function using the basin-hopping algorithm\n",
        "        \n",
        "        .. versionadded:: 0.12.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            Function to be optimized.  ``args`` can be passed as an optional item\n",
        "            in the dict ``minimizer_kwargs``\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        niter : integer, optional\n",
        "            The number of basin hopping iterations\n",
        "        T : float, optional\n",
        "            The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
        "            \"temperatures\" mean that larger jumps in function value will be\n",
        "            accepted.  For best results ``T`` should be comparable to the\n",
        "            separation\n",
        "            (in function value) between local minima.\n",
        "        stepsize : float, optional\n",
        "            initial step size for use in the random displacement.\n",
        "        minimizer_kwargs : dict, optional\n",
        "            Extra keyword arguments to be passed to the minimizer\n",
        "            ``scipy.optimize.minimize()`` Some important options could be:\n",
        "                method : str\n",
        "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
        "                args : tuple\n",
        "                    Extra arguments passed to the objective function (``func``) and\n",
        "                    its derivatives (Jacobian, Hessian).\n",
        "        \n",
        "        take_step : callable ``take_step(x)``, optional\n",
        "            Replace the default step taking routine with this routine.  The default\n",
        "            step taking routine is a random displacement of the coordinates, but\n",
        "            other step taking algorithms may be better for some systems.\n",
        "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
        "            If this attribute exists, then ``basinhopping`` will adjust\n",
        "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
        "            search.\n",
        "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
        "            Define a test which will be used to judge whether or not to accept the\n",
        "            step.  This will be used in addition to the Metropolis test based on\n",
        "            \"temperature\" ``T``.  The acceptable return values are True,\n",
        "            False, or ``\"force accept\"``.  If the latter, then this will\n",
        "            override any other tests in order to accept the step.  This can be\n",
        "            used, for example, to forcefully escape from a local minimum that\n",
        "            ``basinhopping`` is trapped in.\n",
        "        callback : callable, ``callback(x, f, accept)``, optional\n",
        "            A callback function which will be called for all minimum found.  ``x``\n",
        "            and ``f`` are the coordinates and function value of the trial minima,\n",
        "            and ``accept`` is whether or not that minima was accepted.  This can be\n",
        "            used, for example, to save the lowest N minima found.  Also,\n",
        "            ``callback`` can be used to specify a user defined stop criterion by\n",
        "            optionally returning True to stop the ``basinhopping`` routine.\n",
        "        interval : integer, optional\n",
        "            interval for how often to update the ``stepsize``\n",
        "        disp : bool, optional\n",
        "            Set to True to print status messages\n",
        "        niter_success : integer, optional\n",
        "            Stop the run if the global minimum candidate remains the same for this\n",
        "            number of iterations.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.  Important\n",
        "            attributes are: ``x`` the solution array, ``fun`` the value of the\n",
        "            function at the solution, and ``message`` which describes the cause of\n",
        "            the termination. See `Result` for a description of other attributes.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize :\n",
        "            The local minimization function called once for each basinhopping step.\n",
        "            ``minimizer_kwargs`` is passed to this routine.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
        "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
        "        [4]_.  The algorithm in its current form was described by David Wales and\n",
        "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
        "        \n",
        "        The algorithm is iterative with each cycle composed of the following\n",
        "        features\n",
        "        \n",
        "        1) random perturbation of the coordinates\n",
        "        \n",
        "        2) local minimization\n",
        "        \n",
        "        3) accept or reject the new coordinates based on the minimized function\n",
        "           value\n",
        "        \n",
        "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
        "        Carlo algorithms, although there are many other possibilities [3]_.\n",
        "        \n",
        "        This global minimization method has been shown to be extremely efficient\n",
        "        for a wide variety of problems in physics and chemistry.  It is\n",
        "        particularly useful when the function has many minima separated by large\n",
        "        barriers. See the Cambridge Cluster Database\n",
        "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
        "        that have been optimized primarily using basin-hopping.  This database\n",
        "        includes minimization problems exceeding 300 degrees of freedom.\n",
        "        \n",
        "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
        "        a Fortran implementation of basin-hopping.  This implementation has many\n",
        "        different variations of the procedure described above, including more\n",
        "        advanced step taking algorithms and alternate acceptance criterion.\n",
        "        \n",
        "        For stochastic global optimization there is no way to determine if the true\n",
        "        global minimum has actually been found. Instead, as a consistency check,\n",
        "        the algorithm can be run from a number of different random starting points\n",
        "        to ensure the lowest minimum found in each example has converged to the\n",
        "        global minimum.  For this reason ``basinhopping`` will by default simply\n",
        "        run for the number of iterations ``niter`` and return the lowest minimum\n",
        "        found.  It is left to the user to ensure that this is in fact the global\n",
        "        minimum.\n",
        "        \n",
        "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
        "        depends on the problem being solved.  Ideally it should be comparable to\n",
        "        the typical separation between local minima of the function being\n",
        "        optimized.  ``basinhopping`` will, by default, adjust ``stepsize`` to find\n",
        "        an optimal value, but this may take many iterations.  You will get quicker\n",
        "        results if you set a sensible value for ``stepsize``.\n",
        "        \n",
        "        Choosing ``T``: The parameter ``T`` is the temperature used in the\n",
        "        metropolis criterion.  Basinhopping steps are accepted with probability\n",
        "        ``1`` if ``func(xnew) < func(xold)``, or otherwise with probability::\n",
        "        \n",
        "            exp( -(func(xnew) - func(xold)) / T )\n",
        "        \n",
        "        So, for best results, ``T`` should to be comparable to the typical\n",
        "        difference in function value between between local minima\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
        "            Cambridge, UK.\n",
        "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
        "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
        "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
        "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
        "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
        "            1987, 84, 6611.\n",
        "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
        "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following example is a one-dimensional minimization problem,  with many\n",
        "        local minima superimposed on a parabola.\n",
        "        \n",
        "        >>> func = lambda x: cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
        "        >>> x0=[1.]\n",
        "        \n",
        "        Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
        "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
        "        use and how to set up that minimizer.  This parameter will be passed to\n",
        "        ``scipy.optimize.minimize()``.\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
        "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
        "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
        "        \n",
        "        Next consider a two-dimensional minimization problem. Also, this time we\n",
        "        will use gradient information to significantly speed up the search.\n",
        "        \n",
        "        >>> def func2d(x):\n",
        "        ...     f = cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
        "        ...                                                         0.2) * x[0]\n",
        "        ...     df = np.zeros(2)\n",
        "        ...     df[0] = -14.5 * sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
        "        ...     df[1] = 2. * x[1] + 0.2\n",
        "        ...     return f, df\n",
        "        \n",
        "        We'll also use a different local minimization algorithm.  Also we must tell\n",
        "        the minimizer that our function returns both energy and gradient (jacobian)\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
        "        >>> x0 = [1.0, 1.0]\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Here is an example using a custom step taking routine.  Imagine you want\n",
        "        the first coordinate to take larger steps then the rest of the coordinates.\n",
        "        This can be implemented like so:\n",
        "        \n",
        "        >>> class MyTakeStep(object):\n",
        "        ...    def __init__(self, stepsize=0.5):\n",
        "        ...        self.stepsize = stepsize\n",
        "        ...    def __call__(self, x):\n",
        "        ...        s = self.stepsize\n",
        "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
        "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
        "        ...        return x\n",
        "        \n",
        "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
        "        of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
        "        before\n",
        "        \n",
        "        >>> mytakestep = MyTakeStep()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200, take_step=mytakestep)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Now let's do an example using a custom callback function which prints the\n",
        "        value of every minimum found\n",
        "        \n",
        "        >>> def print_fun(x, f, accepted):\n",
        "        ...         print(\"at minima %.4f accepted %d\" % (f, int(accepted)))\n",
        "        \n",
        "        We'll run it for only 10 basinhopping steps this time.\n",
        "        \n",
        "        >>> np.random.seed(1)\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, callback=print_fun)\n",
        "        at minima 0.4159 accepted 1\n",
        "        at minima -0.9073 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 2.2945 accepted 0\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        \n",
        "        \n",
        "        The minima at -1.0109 is actually the global minimum, found already on the\n",
        "        8th iteration.\n",
        "        \n",
        "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
        "        \n",
        "        >>> class MyBounds(object):\n",
        "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
        "        ...         self.xmax = np.array(xmax)\n",
        "        ...         self.xmin = np.array(xmin)\n",
        "        ...     def __call__(self, **kwargs):\n",
        "        ...         x = kwargs[\"x_new\"]\n",
        "        ...         tmax = bool(np.all(x <= self.xmax))\n",
        "        ...         tmin = bool(np.all(x >= self.xmin))\n",
        "        ...         return tmax and tmin\n",
        "        \n",
        "        >>> mybounds = MyBounds()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, accept_test=mybounds)\n",
        "    \n",
        "    bisect(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of a function within an interval.\n",
        "        \n",
        "        Basic bisection routine to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. `f(a)` and `f(b)` can not have the same signs.\n",
        "        Slow but sure.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  `f` must be continuous, and\n",
        "            f(a) and f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within `xtol` of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in `maxiter` iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
        "            a `RootResults` object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        fsolve : n-dimensional root-finding\n",
        "    \n",
        "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
        "        Bracket the minimum of the function.\n",
        "        \n",
        "        Given a function and distinct initial points, search in the\n",
        "        downhill direction (as defined by the initital points) and return\n",
        "        new points xa, xb, xc that bracket the minimum of the function\n",
        "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
        "        solution will satisfy xa<=x<=xb\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to minimize.\n",
        "        xa, xb : float, optional\n",
        "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
        "        args : tuple, optional\n",
        "            Additional arguments (if present), passed to `func`.\n",
        "        grow_limit : float, optional\n",
        "            Maximum grow limit.  Defaults to 110.0\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Defaults to 1000.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xa, xb, xc : float\n",
        "            Bracket.\n",
        "        fa, fb, fc : float\n",
        "            Objective function values in bracket.\n",
        "        funcalls : int\n",
        "            Number of function evaluations made.\n",
        "    \n",
        "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
        "        Given a function of one-variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        args\n",
        "            Additional arguments (if present).\n",
        "        brack : tuple\n",
        "            Triple (a,b,c) where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,c)\n",
        "            then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that the obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            Stop if between iteration change is less than `tol`.\n",
        "        full_output : bool\n",
        "            If True, return all output args (xmin, fval, iter,\n",
        "            funcalls).\n",
        "        maxiter : int\n",
        "            Maximum number of iterations in solution.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            Optimum point.\n",
        "        fval : float\n",
        "            Optimum value.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of objective function evaluations made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Brent' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses inverse parabolic interpolation when possible to speed up\n",
        "        convergence of golden section method.\n",
        "    \n",
        "    brenth(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of f in [a,b].\n",
        "        \n",
        "        A variation on the classic Brent routine to find a zero of the function f\n",
        "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
        "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
        "        f(a) and f(b) can not have the same signs. Generally on a par with the\n",
        "        brent routine, but not as heavily tested.  It is a safe version of the\n",
        "        secant method that uses hyperbolic extrapolation. The version here is by\n",
        "        Chuck Harris.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        fmin, fmin_powell, fmin_cg,\n",
        "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
        "        \n",
        "        leastsq : nonlinear least squares minimizer\n",
        "        \n",
        "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
        "        \n",
        "        anneal, brute : global optimizers\n",
        "        \n",
        "        fminbound, brent, golden, bracket : local scalar minimizers\n",
        "        \n",
        "        fsolve : n-dimensional root-finding\n",
        "        \n",
        "        brentq, brenth, ridder, bisect, newton : one-dimensional root-finding\n",
        "        \n",
        "        fixed_point : scalar fixed-point finder\n",
        "    \n",
        "    brentq(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in given interval.\n",
        "        \n",
        "        Return float, a zero of `f` between `a` and `b`.  `f` must be a continuous\n",
        "        function, and [a,b] must be a sign changing interval.\n",
        "        \n",
        "        Description:\n",
        "        Uses the classic Brent (1973) method to find a zero of the function `f` on\n",
        "        the sign changing interval [a , b].  Generally considered the best of the\n",
        "        rootfinding routines here.  It is a safe version of the secant method that\n",
        "        uses inverse quadratic extrapolation.  Brent's method combines root\n",
        "        bracketing, interval bisection, and inverse quadratic interpolation.  It is\n",
        "        sometimes known as the van Wijngaarden-Deker-Brent method.  Brent (1973)\n",
        "        claims convergence is guaranteed for functions computable within [a,b].\n",
        "        \n",
        "        [Brent1973]_ provides the classic description of the algorithm.  Another\n",
        "        description can be found in a recent edition of Numerical Recipes, including\n",
        "        [PressEtal1992]_.  Another description is at\n",
        "        http://mathworld.wolfram.com/BrentsMethod.html.  It should be easy to\n",
        "        understand the algorithm just by reading our code.  Our code diverges a bit\n",
        "        from standard presentations: we choose a different formula for the\n",
        "        extrapolation step.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        multivariate local optimizers\n",
        "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
        "        nonlinear least squares minimizer\n",
        "          `leastsq`\n",
        "        constrained multivariate optimizers\n",
        "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
        "        global optimizers\n",
        "          `anneal`, `basinhopping`, `brute`\n",
        "        local scalar minimizers\n",
        "          `fminbound`, `brent`, `golden`, `bracket`\n",
        "        n-dimensional root-finding\n",
        "          `fsolve`\n",
        "        one-dimensional root-finding\n",
        "          `brentq`, `brenth`, `ridder`, `bisect`, `newton`\n",
        "        scalar fixed-point finder\n",
        "          `fixed_point`\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Brent1973]\n",
        "           Brent, R. P.,\n",
        "           *Algorithms for Minimization Without Derivatives*.\n",
        "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
        "        \n",
        "        .. [PressEtal1992]\n",
        "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
        "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
        "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
        "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
        "    \n",
        "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \\\"Broyden's good method\\\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
        "        \n",
        "        which corresponds to Broyden's first Jacobian update\n",
        "        \n",
        "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \\\"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \"Broyden's bad method\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
        "        \n",
        "        corresponding to Broyden's second method.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin>, disp=False)\n",
        "        Minimize a function over a given range by brute force.\n",
        "        \n",
        "        Uses the \"brute force\" method, i.e. computes the function's value\n",
        "        at each point of a multidimensional grid of points, to find the global\n",
        "        minimum of the function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized. Must be in the\n",
        "            form ``f(x, *args)``, where ``x`` is the argument in\n",
        "            the form of a 1-D array and ``args`` is a tuple of any\n",
        "            additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        ranges : tuple\n",
        "            Each component of the `ranges` tuple must be either a\n",
        "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
        "            The program uses these to create the grid of points on which\n",
        "            the objective function will be computed. See `Note 2` for\n",
        "            more detail.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        Ns : int, optional\n",
        "            Number of grid points along the axes, if not otherwise\n",
        "            specified. See `Note2`.\n",
        "        full_output : bool, optional\n",
        "            If True, return the evaluation grid and the objective function's\n",
        "            values on it.\n",
        "        finish : callable, optional\n",
        "            An optimization function that is called with the result of brute force\n",
        "            minimization as initial guess.  `finish` should take the initial guess\n",
        "            as positional argument, and take `args`, `full_output` and `disp`\n",
        "            as keyword arguments.  Use None if no \"polishing\" function is to be\n",
        "            used.  See Notes for more details.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : ndarray\n",
        "            A 1-D array containing the coordinates of a point at which the\n",
        "            objective function had its minimum value. (See `Note 1` for\n",
        "            which point is returned.)\n",
        "        fval : float\n",
        "            Function value at the point `x0`.\n",
        "        grid : tuple\n",
        "            Representation of the evaluation grid.  It has the same\n",
        "            length as `x0`. (Returned when `full_output` is True.)\n",
        "        Jout : ndarray\n",
        "            Function values at each point of the evaluation\n",
        "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
        "            when `full_output` is True.)\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        anneal : Another approach to seeking the global minimum of\n",
        "        multivariate, multimodal functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
        "        of the objective function occurs.  If `finish` is None, that is the\n",
        "        point returned.  When the global minimum occurs within (or not very far\n",
        "        outside) the grid's boundaries, and the grid is fine enough, that\n",
        "        point will be in the neighborhood of the gobal minimum.\n",
        "        \n",
        "        However, users often employ some other optimization program to\n",
        "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
        "        (local) minimum near `brute's` best gridpoint.\n",
        "        The `brute` function's `finish` option provides a convenient way to do\n",
        "        that.  Any polishing program used must take `brute's` output as its\n",
        "        initial guess as a positional argument, and take `brute's` input values\n",
        "        for `args` and `full_output` as keyword arguments, otherwise an error\n",
        "        will be raised.\n",
        "        \n",
        "        `brute` assumes that the `finish` function returns a tuple in the form:\n",
        "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing value\n",
        "        of the argument, ``Jmin`` is the minimum value of the objective function,\n",
        "        \"...\" may be some other returned values (which are not used by `brute`),\n",
        "        and ``statuscode`` is the status code of the `finish` program.\n",
        "        \n",
        "        Note that when `finish` is not None, the values returned are those\n",
        "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
        "        while `brute` confines its search to the input grid points,\n",
        "        the `finish` program's results usually will not coincide with any\n",
        "        gridpoint, and may fall outside the grid's boundary.\n",
        "        \n",
        "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
        "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
        "        Each component of the `ranges` tuple can be either a slice object or a\n",
        "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
        "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
        "        range, `brute` internally converts it to a slice object that interpolates\n",
        "        `Ns` points from its low-value to its high-value, inclusive.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        We illustrate the use of `brute` to seek the global minimum of a function\n",
        "        of two variables that is given as the sum of a positive-definite\n",
        "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
        "        the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
        "        are as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        Thus, the objective function may have local minima near the minimum\n",
        "        of each of the three functions of which it is composed.  To\n",
        "        use `fmin` to polish its gridpoint result, we may then continue as\n",
        "        follows:\n",
        "        \n",
        "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
        "        >>> from scipy import optimize\n",
        "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
        "                                      finish=optimize.fmin)\n",
        "        >>> resbrute[0]  # global minimum\n",
        "        array([-1.05665192,  1.80834843])\n",
        "        >>> resbrute[1]  # function value at global minimum\n",
        "        -3.4085818767\n",
        "        \n",
        "        Note that if `finish` had been set to None, we would have gotten the\n",
        "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
        "    \n",
        "    check_grad(func, grad, x0, *args)\n",
        "        Check the correctness of a gradient function by comparing it against a\n",
        "        (forward) finite-difference approximation of the gradient.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x0,*args)\n",
        "            Function whose derivative is to be checked.\n",
        "        grad : callable grad(x0, *args)\n",
        "            Gradient of `func`.\n",
        "        x0 : ndarray\n",
        "            Points to check `grad` against forward difference approximation of grad\n",
        "            using `func`.\n",
        "        args : \\*args, optional\n",
        "            Extra arguments passed to `func` and `grad`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        err : float\n",
        "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
        "            difference between ``grad(x0, *args)`` and the finite difference\n",
        "            approximation of `grad` using func at the points `x0`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        approx_fprime\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The step size used for the finite difference approximation is\n",
        "        `sqrt(numpy.finfo(float).eps)`, which is approximately 1.49e-08.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> def func(x): return x[0]**2 - 0.5 * x[1]**3\n",
        "        >>> def grad(x): return [2 * x[0], -1.5 * x[1]**2]\n",
        "        >>> check_grad(func, grad, [1.5, -1.5])\n",
        "        2.9802322387695312e-08\n",
        "    \n",
        "    curve_fit(f, xdata, ydata, p0=None, sigma=None, **kw)\n",
        "        Use non-linear least squares to fit a function, f, to data.\n",
        "        \n",
        "        Assumes ``ydata = f(xdata, *params) + eps``\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable\n",
        "            The model function, f(x, ...).  It must take the independent\n",
        "            variable as the first argument and the parameters to fit as\n",
        "            separate remaining arguments.\n",
        "        xdata : An N-length sequence or an (k,N)-shaped array\n",
        "            for functions with k predictors.\n",
        "            The independent variable where the data is measured.\n",
        "        ydata : N-length sequence\n",
        "            The dependent data --- nominally f(xdata, ...)\n",
        "        p0 : None, scalar, or M-length sequence\n",
        "            Initial guess for the parameters.  If None, then the initial\n",
        "            values will all be 1 (if the number of parameters for the function\n",
        "            can be determined using introspection, otherwise a ValueError\n",
        "            is raised).\n",
        "        sigma : None or N-length sequence\n",
        "            If not None, this vector will be used as relative weights in the\n",
        "            least-squares problem.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        popt : array\n",
        "            Optimal values for the parameters so that the sum of the squared error\n",
        "            of ``f(xdata, *popt) - ydata`` is minimized\n",
        "        pcov : 2d array\n",
        "            The estimated covariance of popt.  The diagonals provide the variance\n",
        "            of the parameter estimate.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        leastsq\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The algorithm uses the Levenberg-Marquardt algorithm through `leastsq`.\n",
        "        Additional keyword arguments are passed directly to that algorithm.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> import numpy as np\n",
        "        >>> from scipy.optimize import curve_fit\n",
        "        >>> def func(x, a, b, c):\n",
        "        ...     return a*np.exp(-b*x) + c\n",
        "        \n",
        "        >>> x = np.linspace(0,4,50)\n",
        "        >>> y = func(x, 2.5, 1.3, 0.5)\n",
        "        >>> yn = y + 0.2*np.random.normal(size=len(x))\n",
        "        \n",
        "        >>> popt, pcov = curve_fit(func, x, yn)\n",
        "    \n",
        "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
        "        \n",
        "        The Jacobian approximation is derived from previous iterations, by\n",
        "        retaining only the diagonal of Broyden matrices.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
        "        \n",
        "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial Jacobian approximation is (-1/alpha).\n",
        "        alphamax : float, optional\n",
        "            The entries of the diagonal Jacobian are kept in the range\n",
        "            ``[alpha, alphamax]``.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500)\n",
        "        Find a fixed point of the function.\n",
        "        \n",
        "        Given a function of one or more variables and a starting point, find a\n",
        "        fixed-point of the function: i.e. where ``func(x0) == x0``.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            Function to evaluate.\n",
        "        x0 : array_like\n",
        "            Fixed point of function.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to `func`.\n",
        "        xtol : float, optional\n",
        "            Convergence tolerance, defaults to 1e-08.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations, defaults to 500.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses Steffensen's Method using Aitken's ``Del^2`` convergence acceleration.\n",
        "        See Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c1, c2):\n",
        "        ....    return np.sqrt(c1/(x+c2))\n",
        "        >>> c1 = np.array([10,12.])\n",
        "        >>> c2 = np.array([3, 5.])\n",
        "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
        "        array([ 1.4920333 ,  1.37228132])\n",
        "    \n",
        "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the downhill simplex algorithm.\n",
        "        \n",
        "        This algorithm only uses function values, not derivatives or second\n",
        "        derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            The objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        xtol : float, optional\n",
        "            Relative error in xopt acceptable for convergence.\n",
        "        ftol : number, optional\n",
        "            Relative error in func(xopt) acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : number, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            Set to True if fopt and warnflag outputs are desired.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        retall : bool, optional\n",
        "            Set to True to return list of solutions at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter that minimizes function.\n",
        "        fopt : float\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        iter : int\n",
        "            Number of iterations performed.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            1 : Maximum number of function evaluations made.\n",
        "            2 : Maximum number of iterations reached.\n",
        "        allvecs : list\n",
        "            Solution at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Nelder-Mead' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "        one or more variables.\n",
        "        \n",
        "        This algorithm has a long history of successful use in applications.\n",
        "        But it will usually be slower than an algorithm that uses first or\n",
        "        second derivative information. In practice it can have poor\n",
        "        performance in high-dimensional problems and is not robust to\n",
        "        minimizing complicated functions. Additionally, there currently is no\n",
        "        complete theory describing when the algorithm will successfully\n",
        "        converge to the minimum, or how fast it will if it does.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "               minimization\", The Computer Journal, 7, pp. 308-313\n",
        "        \n",
        "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "               Harlow, UK, pp. 191-208.\n",
        "    \n",
        "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the BFGS algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable f'(x,*args), optional\n",
        "            Gradient of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f and fprime.\n",
        "        gtol : float, optional\n",
        "            Gradient norm must be less than gtol before succesful termination.\n",
        "        norm : float, optional\n",
        "            Order of norm (Inf is max, -Inf is min)\n",
        "        epsilon : int or ndarray, optional\n",
        "            If fprime is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function to call after each\n",
        "            iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
        "            in addition to xopt.\n",
        "        disp : bool, optional\n",
        "            Print convergence message if True.\n",
        "        retall : bool, optional\n",
        "            Return a list of results at each iteration if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
        "        fopt : float\n",
        "            Minimum value.\n",
        "        gopt : ndarray\n",
        "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
        "        Bopt : ndarray\n",
        "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
        "        func_calls : int\n",
        "            Number of function_calls made.\n",
        "        grad_calls : int\n",
        "            Number of gradient calls made.\n",
        "        warnflag : integer\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "            2 : Gradient and/or function calls not changing.\n",
        "        allvecs  :  list\n",
        "            Results at each iteration.  Only returned if retall is True.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'BFGS' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Optimize the function, f, whose gradient is given by fprime\n",
        "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
        "        and Shanno (BFGS)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
        "    \n",
        "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable, ``f(x, *args)``\n",
        "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
        "            the variables that are to be changed in the search for a minimum, and\n",
        "            `args` are the other (fixed) parameters of `f`.\n",
        "        x0 : ndarray\n",
        "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
        "            It must be a 1-D array of values.\n",
        "        fprime : callable, ``fprime(x, *args)``, optional\n",
        "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
        "            are as described above for `f`. The returned value must be a 1-D array.\n",
        "            Defaults to None, in which case the gradient is approximated\n",
        "            numerically (see `epsilon`, below).\n",
        "        args : tuple, optional\n",
        "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
        "            additional fixed parameters are needed to completely specify the\n",
        "            functions `f` and `fprime`.\n",
        "        gtol : float, optional\n",
        "            Stop when the norm of the gradient is less than `gtol`.\n",
        "        norm : float, optional\n",
        "            Order to use for the norm of the gradient\n",
        "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
        "        epsilon : float or ndarray, optional\n",
        "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
        "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
        "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
        "            1.5e-8.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
        "        full_output : bool, optional\n",
        "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
        "            addition to `xopt`.  See the Returns section below for additional\n",
        "            information on optional return values.\n",
        "        disp : bool, optional\n",
        "            If True, return a convergence message, followed by `xopt`.\n",
        "        retall : bool, optional\n",
        "            If True, add to the returned values the results of each iteration.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each iteration.\n",
        "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float, optional\n",
        "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
        "        func_calls : int, optional\n",
        "            The number of function_calls made.  Only returned if `full_output`\n",
        "            is True.\n",
        "        grad_calls : int, optional\n",
        "            The number of gradient calls made. Only returned if `full_output` is\n",
        "            True.\n",
        "        warnflag : int, optional\n",
        "            Integer value with warning status, only returned if `full_output` is\n",
        "            True.\n",
        "        \n",
        "            0 : Success.\n",
        "        \n",
        "            1 : The maximum number of iterations was exceeded.\n",
        "        \n",
        "            2 : Gradient and/or function calls were not changing.  May indicate\n",
        "                that precision was lost, i.e., the routine did not converge.\n",
        "        \n",
        "        allvecs : list of ndarray, optional\n",
        "            List of arrays, containing the results at each iteration.\n",
        "            Only returned if `retall` is True.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize : common interface to all `scipy.optimize` algorithms for\n",
        "                   unconstrained and constrained minimization of multivariate\n",
        "                   functions.  It provides an alternative way to call\n",
        "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
        "        [1]_.\n",
        "        \n",
        "        Conjugate gradient methods tend to work better when:\n",
        "        \n",
        "        1. `f` has a unique global minimizing point, and no local minima or\n",
        "           other stationary points,\n",
        "        2. `f` is, at least locally, reasonably well approximated by a\n",
        "           quadratic function of the variables,\n",
        "        3. `f` is continuous and has a continuous gradient,\n",
        "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
        "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
        "           minimizing point, `xopt`.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Example 1: seek the minimum value of the expression\n",
        "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
        "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
        "        \n",
        "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
        "        >>> def f(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
        "        >>> def gradf(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
        "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
        "        ...     return np.asarray((gu, gv))\n",
        "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
        "        >>> print 'res1 = ', res1\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 1.617021\n",
        "                 Iterations: 2\n",
        "                 Function evaluations: 5\n",
        "                 Gradient evaluations: 5\n",
        "        res1 =  [-1.80851064 -0.25531915]\n",
        "        \n",
        "        Example 2: solve the same problem using the `minimize` function.\n",
        "        (This `myopts` dictionary shows all of the available options,\n",
        "        although in practice only non-default values would be needed.\n",
        "        The returned value will be a dictionary.)\n",
        "        \n",
        "        >>> opts = {'maxiter' : None,    # default value.\n",
        "        ...         'disp' : True,    # non-default value.\n",
        "        ...         'gtol' : 1e-5,    # default value.\n",
        "        ...         'norm' : np.inf,  # default value.\n",
        "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
        "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
        "        ...                          method='CG', options=opts)\n",
        "        Optimization terminated successfully.\n",
        "                Current function value: 1.617021\n",
        "                Iterations: 2\n",
        "                Function evaluations: 5\n",
        "                Gradient evaluations: 5\n",
        "        >>> res2.x  # minimum found\n",
        "        array([-1.80851064 -0.25531915])\n",
        "    \n",
        "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, iprint=1, maxfun=1000, disp=None)\n",
        "        Minimize a function using the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
        "        implentation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            Function to minimize. In the form func(x, \\*args).\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        cons : sequence\n",
        "            Constraint functions; must all be ``>=0`` (a single function\n",
        "            if only 1 constraint). Each function takes the parameters `x`\n",
        "            as its first argument.\n",
        "        args : tuple\n",
        "            Extra arguments to pass to function.\n",
        "        consargs : tuple\n",
        "            Extra arguments to pass to constraint functions (default of None means\n",
        "            use same extra arguments as those passed to func).\n",
        "            Use ``()`` for no extra arguments.\n",
        "        rhobeg :\n",
        "            Reasonable initial changes to the variables.\n",
        "        rhoend :\n",
        "            Final accuracy in the optimization (not precisely guaranteed). This\n",
        "            is a lower bound on the size of the trust region.\n",
        "        iprint : {0, 1, 2, 3}\n",
        "            Controls the frequency of output; 0 implies no output.  Deprecated.\n",
        "        disp : {0, 1, 2, 3}\n",
        "            Over-rides the iprint interface.  Preferred.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The argument that minimises `f`.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'COBYLA' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm is based on linear approximations to the objective\n",
        "        function and each constraint. We briefly describe the algorithm.\n",
        "        \n",
        "        Suppose the function is being minimized over k variables. At the\n",
        "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
        "        an approximate solution x_j, and a radius RHO_j.\n",
        "        (i.e. linear plus a constant) approximations to the objective\n",
        "        function and constraint functions such that their function values\n",
        "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
        "        This gives a linear program to solve (where the linear approximations\n",
        "        of the constraint functions are constrained to be non-negative).\n",
        "        \n",
        "        However the linear approximations are likely only good\n",
        "        approximations near the current simplex, so the linear program is\n",
        "        given the further requirement that the solution, which\n",
        "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
        "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
        "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
        "        like a trust region algorithm.\n",
        "        \n",
        "        Additionally, the linear program may be inconsistent, or the\n",
        "        approximation may give poor improvement. For details about\n",
        "        how these issues are resolved, as well as how the points v_i are\n",
        "        updated, refer to the source code or the references below.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
        "        the objective and constraint functions by linear interpolation.\", in\n",
        "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
        "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
        "        \n",
        "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
        "        calculations\", Acta Numerica 7, 287-336\n",
        "        \n",
        "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
        "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
        "        \n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Minimize the objective function f(x,y) = x*y subject\n",
        "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
        "        \n",
        "            >>> def objective(x):\n",
        "            ...     return x[0]*x[1]\n",
        "            ...\n",
        "            >>> def constr1(x):\n",
        "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
        "            ...\n",
        "            >>> def constr2(x):\n",
        "            ...     return x[1]\n",
        "            ...\n",
        "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
        "        \n",
        "               Normal return from subroutine COBYLA\n",
        "        \n",
        "               NFVALS =   64   F =-5.000000E-01    MAXCV = 1.998401E-14\n",
        "               X =-7.071069E-01   7.071067E-01\n",
        "            array([-0.70710685,  0.70710671])\n",
        "        \n",
        "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
        "    \n",
        "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None)\n",
        "        Minimize a function func using the L-BFGS-B algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Function to minimise.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable fprime(x,*args)\n",
        "            The gradient of `func`.  If None, then `func` returns the function\n",
        "            value and the gradient (``f, g = func(x, *args)``), unless\n",
        "            `approx_grad` is True in which case `func` returns only ``f``.\n",
        "        args : sequence\n",
        "            Arguments to pass to `func` and `fprime`.\n",
        "        approx_grad : bool\n",
        "            Whether to approximate the gradient numerically (in which case\n",
        "            `func` returns only the function value).\n",
        "        bounds : list\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        m : int\n",
        "            The maximum number of variable metric corrections\n",
        "            used to define the limited memory matrix. (The limited memory BFGS\n",
        "            method does not store the full hessian but uses this many terms in an\n",
        "            approximation to it.)\n",
        "        factr : float\n",
        "            The iteration stops when\n",
        "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
        "            where ``eps`` is the machine precision, which is automatically\n",
        "            generated by the code. Typical values for `factr` are: 1e12 for\n",
        "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
        "            high accuracy.\n",
        "        pgtol : float\n",
        "            The iteration will stop when\n",
        "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
        "            where ``pg_i`` is the i-th component of the projected gradient.\n",
        "        epsilon : float\n",
        "            Step size used when `approx_grad` is True, for numerically\n",
        "            calculating the gradient\n",
        "        iprint : int\n",
        "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
        "            ``iprint == 0`` means write messages to stdout; ``iprint > 1`` in\n",
        "            addition means write logging information to a file named\n",
        "            ``iterate.dat`` in the current working directory.\n",
        "        disp : int, optional\n",
        "            If zero, then no output.  If a positive number, then this over-rides\n",
        "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        maxiter : int\n",
        "            Maximum number of iterations.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : array_like\n",
        "            Estimated position of the minimum.\n",
        "        f : float\n",
        "            Value of `func` at the minimum.\n",
        "        d : dict\n",
        "            Information dictionary.\n",
        "        \n",
        "            * d['warnflag'] is\n",
        "        \n",
        "              - 0 if converged,\n",
        "              - 1 if too many function evaluations or too many iterations,\n",
        "              - 2 if stopped for another reason, given in d['task']\n",
        "        \n",
        "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
        "            * d['funcalls'] is the number of function calls made.\n",
        "            * d['nit'] is the number of iterations.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'L-BFGS-B' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        License of L-BFGS-B (FORTRAN code):\n",
        "        \n",
        "        The version included here (in fortran code) is 3.0\n",
        "        (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd,\n",
        "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
        "        condition for use:\n",
        "        \n",
        "        This software is freely available, but we expect that all publications\n",
        "        describing work using this software, or all commercial products using it,\n",
        "        quote at least one of the references given below. This software is released\n",
        "        under the BSD License.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
        "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
        "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
        "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
        "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
        "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
        "          ACM Transactions on Mathematical Software, 38, 1.\n",
        "    \n",
        "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Unconstrained minimization of a function using the Newton-CG method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable ``f(x, *args)``\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable ``f'(x, *args)``\n",
        "            Gradient of f.\n",
        "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
        "            Function which computes the Hessian of f times an\n",
        "            arbitrary vector, p.\n",
        "        fhess : callable ``fhess(x, *args)``, optional\n",
        "            Function to compute the Hessian matrix of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
        "            (the same set of extra arguments is supplied to all of\n",
        "            these functions).\n",
        "        epsilon : float or ndarray, optional\n",
        "            If fhess is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function which is called after\n",
        "            each iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        avextol : float, optional\n",
        "            Convergence is assumed when the average relative error in\n",
        "            the minimizer falls below this amount.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True, return the optional outputs.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence message.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of results at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float\n",
        "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
        "        fcalls : int\n",
        "            Number of function calls made.\n",
        "        gcalls : int\n",
        "            Number of gradient calls made.\n",
        "        hcalls : int\n",
        "            Number of hessian calls made.\n",
        "        warnflag : int\n",
        "            Warnings generated by the algorithm.\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "        allvecs : list\n",
        "            The result at each iteration, if retall is True (see below).\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Newton-CG' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
        "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
        "        nor `fhess_p` is provided, then the hessian product will be\n",
        "        approximated using finite differences on `fprime`. `fhess_p`\n",
        "        must compute the hessian times an arbitrary vector. If it is not\n",
        "        given, finite-differences on `fprime` are used to compute\n",
        "        it.\n",
        "        \n",
        "        Newton-CG methods are also called truncated Newton methods. This\n",
        "        function differs from scipy.optimize.fmin_tnc because\n",
        "        \n",
        "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
        "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
        "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
        "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
        "            or box constrained minimization. (Box constraints give\n",
        "            lower and upper bounds for each variable seperately.)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
        "    \n",
        "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
        "        Minimize a function using modified Powell's method. This method\n",
        "        only uses function values, not derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each\n",
        "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        direc : ndarray, optional\n",
        "            Initial direction set.\n",
        "        xtol : float, optional\n",
        "            Line-search error tolerance.\n",
        "        ftol : float, optional\n",
        "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            If True, fopt, xi, direc, iter, funcalls, and\n",
        "            warnflag are returned.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence messages.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of the solution at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter which minimizes `func`.\n",
        "        fopt : number\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        direc : ndarray\n",
        "            Current direction set.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            Integer warning flag:\n",
        "                1 : Maximum number of function evaluations.\n",
        "                2 : Maximum number of iterations.\n",
        "        allvecs : list\n",
        "            List of solutions at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to unconstrained minimization algorithms for\n",
        "            multivariate functions. See the 'Powell' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a modification of Powell's method to find the minimum of\n",
        "        a function of N variables. Powell's method is a conjugate\n",
        "        direction method.\n",
        "        \n",
        "        The algorithm has two loops. The outer loop\n",
        "        merely iterates over the inner loop. The inner loop minimizes\n",
        "        over each current direction in the direction set. At the end\n",
        "        of the inner loop, if certain conditions are met, the direction\n",
        "        that gave the largest decrease is dropped and replaced with\n",
        "        the difference between the current estiamted x and the estimated\n",
        "        x from the beginning of the inner-loop.\n",
        "        \n",
        "        The technical conditions for replacing the direction of greatest\n",
        "        increase amount to checking that\n",
        "        \n",
        "        1. No further gain can be made along the direction of greatest increase\n",
        "           from that iteration.\n",
        "        2. The direction of greatest increase accounted for a large sufficient\n",
        "           fraction of the decrease in the function value from that iteration of\n",
        "           the inner loop.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
        "        function of several variables without calculating derivatives,\n",
        "        Computer Journal, 7 (2):155-162.\n",
        "        \n",
        "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
        "        Numerical Recipes (any edition), Cambridge University Press\n",
        "    \n",
        "    fmin_slsqp(func, x0, eqcons=[], f_eqcons=None, ieqcons=[], f_ieqcons=None, bounds=[], fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08)\n",
        "        Minimize a function using Sequential Least SQuares Programming\n",
        "        \n",
        "        Python interface function for the SLSQP Optimization subroutine\n",
        "        originally implemented by Dieter Kraft.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        x0 : 1-D ndarray of float\n",
        "            Initial guess for the independent variable(s).\n",
        "        eqcons : list\n",
        "            A list of functions of length n such that\n",
        "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_eqcons : callable f(x,*args)\n",
        "            Returns a 1-D array in which each element must equal 0.0 in a\n",
        "            successfully optimized problem.  If f_eqcons is specified,\n",
        "            eqcons is ignored.\n",
        "        ieqcons : list\n",
        "            A list of functions of length n such that\n",
        "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_ieqcons : callable f(x,*args)\n",
        "            Returns a 1-D ndarray in which each element must be greater or\n",
        "            equal to 0.0 in a successfully optimized problem.  If\n",
        "            f_ieqcons is specified, ieqcons is ignored.\n",
        "        bounds : list\n",
        "            A list of tuples specifying the lower and upper bound\n",
        "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
        "            Infinite values will be interpreted as large floating values.\n",
        "        fprime : callable `f(x,*args)`\n",
        "            A function that evaluates the partial derivatives of func.\n",
        "        fprime_eqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of equality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
        "        fprime_ieqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of inequality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
        "        args : sequence\n",
        "            Additional arguments passed to func and fprime.\n",
        "        iter : int\n",
        "            The maximum number of iterations.\n",
        "        acc : float\n",
        "            Requested accuracy.\n",
        "        iprint : int\n",
        "            The verbosity of fmin_slsqp :\n",
        "        \n",
        "            * iprint <= 0 : Silent operation\n",
        "            * iprint == 1 : Print summary upon completion (default)\n",
        "            * iprint >= 2 : Print status of each iterate and summary\n",
        "        disp : int\n",
        "            Over-rides the iprint interface (preferred).\n",
        "        full_output : bool\n",
        "            If False, return only the minimizer of func (default).\n",
        "            Otherwise, output final objective function and summary\n",
        "            information.\n",
        "        epsilon : float\n",
        "            The step size for finite-difference derivative estimates.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray of float\n",
        "            The final minimizer of func.\n",
        "        fx : ndarray of float, if full_output is true\n",
        "            The final value of the objective function.\n",
        "        its : int, if full_output is true\n",
        "            The number of iterations.\n",
        "        imode : int, if full_output is true\n",
        "            The exit mode from the optimizer (see below).\n",
        "        smode : string, if full_output is true\n",
        "            Message describing the exit mode from the optimizer.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'SLSQP' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Exit modes are defined as follows ::\n",
        "        \n",
        "            -1 : Gradient evaluation required (g & a)\n",
        "             0 : Optimization terminated successfully.\n",
        "             1 : Function evaluation required (f & c)\n",
        "             2 : More equality constraints than independent variables\n",
        "             3 : More than 3*n iterations in LSQ subproblem\n",
        "             4 : Inequality constraints incompatible\n",
        "             5 : Singular matrix E in LSQ subproblem\n",
        "             6 : Singular matrix C in LSQ subproblem\n",
        "             7 : Rank-deficient equality constraint subproblem HFTI\n",
        "             8 : Positive directional derivative for linesearch\n",
        "             9 : Iteration limit exceeded\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
        "    \n",
        "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
        "        Minimize a function with variables subject to bounds, using\n",
        "        gradient information in a truncated Newton algorithm. This\n",
        "        method wraps a C implementation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``func(x, *args)``\n",
        "            Function to minimize.  Must do one of:\n",
        "        \n",
        "            1. Return f and g, where f is the value of the function and g its\n",
        "               gradient (a list of floats).\n",
        "        \n",
        "            2. Return the function value but supply gradient function\n",
        "               seperately as `fprime`.\n",
        "        \n",
        "            3. Return the function value and set ``approx_grad=True``.\n",
        "        \n",
        "            If the function returns None, the minimization\n",
        "            is aborted.\n",
        "        x0 : array_like\n",
        "            Initial estimate of minimum.\n",
        "        fprime : callable ``fprime(x, *args)``\n",
        "            Gradient of `func`. If None, then either `func` must return the\n",
        "            function value and the gradient (``f,g = func(x, *args)``)\n",
        "            or `approx_grad` must be True.\n",
        "        args : tuple\n",
        "            Arguments to pass to function.\n",
        "        approx_grad : bool\n",
        "            If true, approximate the gradient numerically.\n",
        "        bounds : list\n",
        "            (min, max) pairs for each element in x0, defining the\n",
        "            bounds on that parameter. Use None or +/-inf for one of\n",
        "            min or max when there is no bound in that direction.\n",
        "        epsilon : float\n",
        "            Used if approx_grad is True. The stepsize in a finite\n",
        "            difference approximation for fprime.\n",
        "        scale : array_like\n",
        "            Scaling factors to apply to each variable.  If None, the\n",
        "            factors are up-low for interval bounded variables and\n",
        "            1+|x| for the others.  Defaults to None.\n",
        "        offset : array_like\n",
        "            Value to substract from each variable.  If None, the\n",
        "            offsets are (up+low)/2 for interval bounded variables\n",
        "            and x for the others.\n",
        "        messages :\n",
        "            Bit mask used to select messages display during\n",
        "            minimization values defined in the MSGS dict.  Defaults to\n",
        "            MGS_ALL.\n",
        "        disp : int\n",
        "            Integer interface to messages.  0 = no message, 5 = all messages\n",
        "        maxCGit : int\n",
        "            Maximum number of hessian*vector evaluations per main\n",
        "            iteration.  If maxCGit == 0, the direction chosen is\n",
        "            -gradient if maxCGit < 0, maxCGit is set to\n",
        "            max(1,min(50,n/2)).  Defaults to -1.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluation.  if None, maxfun is\n",
        "            set to max(100, 10*len(x0)).  Defaults to None.\n",
        "        eta : float\n",
        "            Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "            Defaults to -1.\n",
        "        stepmx : float\n",
        "            Maximum step for the line search.  May be increased during\n",
        "            call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "        accuracy : float\n",
        "            Relative precision for finite difference calculations.  If\n",
        "            <= machine_precision, set to sqrt(machine_precision).\n",
        "            Defaults to 0.\n",
        "        fmin : float\n",
        "            Minimum function value estimate.  Defaults to 0.\n",
        "        ftol : float\n",
        "            Precision goal for the value of f in the stoping criterion.\n",
        "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "        xtol : float\n",
        "            Precision goal for the value of x in the stopping\n",
        "            criterion (after applying x scaling factors).  If xtol <\n",
        "            0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "            -1.\n",
        "        pgtol : float\n",
        "            Precision goal for the value of the projected gradient in\n",
        "            the stopping criterion (after applying x scaling factors).\n",
        "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
        "            Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "        rescale : float\n",
        "            Scaling factor (in log10) used to trigger f value\n",
        "            rescaling.  If 0, rescale at each iteration.  If a large\n",
        "            value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution.\n",
        "        nfeval : int\n",
        "            The number of function evaluations.\n",
        "        rc : int\n",
        "            Return code as defined in the RCSTRINGS dict.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'TNC' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The underlying algorithm is truncated Newton, also called\n",
        "        Newton Conjugate-Gradient. This method differs from\n",
        "        scipy.optimize.fmin_ncg in that\n",
        "        \n",
        "        1. It wraps a C implementation of the algorithm\n",
        "        2. It allows each variable to be given an upper and lower bound.\n",
        "        \n",
        "        The algorithm incoporates the bound constraints by determining\n",
        "        the descent direction as in an unconstrained truncated Newton,\n",
        "        but never taking a step-size large enough to leave the space\n",
        "        of feasible x's. The algorithm keeps track of a set of\n",
        "        currently active constraints, and ignores them when computing\n",
        "        the minimum allowable step size. (The x's associated with the\n",
        "        active constraint are kept fixed.) If the maximum allowable\n",
        "        step size is zero then a new constraint is added. At the end\n",
        "        of each iteration one of the constraints may be deemed no\n",
        "        longer active and removed. A constraint is considered\n",
        "        no longer active is if it is currently active\n",
        "        but the gradient for that variable points inward from the\n",
        "        constraint. The specific constraint removed is the one\n",
        "        associated with the variable of largest index whose\n",
        "        constraint is no longer active.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
        "        \n",
        "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
        "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
        "    \n",
        "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
        "        Bounded minimization for scalar functions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized (must accept and return scalars).\n",
        "        x1, x2 : float or array scalar\n",
        "            The optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to function.\n",
        "        xtol : float, optional\n",
        "            The convergence tolerance.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations allowed.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        disp : int, optional\n",
        "            If non-zero, print messages.\n",
        "                0 : no message printing.\n",
        "                1 : non-convergence notification messages only.\n",
        "                2 : print a message on convergence too.\n",
        "                3 : print iteration results.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters (over given interval) which minimize the\n",
        "            objective function.\n",
        "        fval : number\n",
        "            The function value at the minimum point.\n",
        "        ierr : int\n",
        "            An error flag (0 if converged, 1 if maximum number of\n",
        "            function calls reached).\n",
        "        numfunc : int\n",
        "          The number of function calls made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Bounded' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Finds a local minimizer of the scalar function `func` in the\n",
        "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
        "        for auto-bracketing).\n",
        "    \n",
        "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
        "        Find the roots of a function.\n",
        "        \n",
        "        Return the roots of the (non-linear) equations defined by\n",
        "        ``func(x) = 0`` given a starting estimate.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            A function that takes at least one (possibly vector) argument.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the roots of ``func(x) = 0``.\n",
        "        args : tuple, optional\n",
        "            Any extra arguments to `func`.\n",
        "        fprime : callable(x), optional\n",
        "            A function to compute the Jacobian of `func` with derivatives\n",
        "            across the rows. By default, the Jacobian will be estimated.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        col_deriv : bool, optional\n",
        "            Specify whether the Jacobian function computes derivatives down\n",
        "            the columns (faster, because there is no transpose operation).\n",
        "        xtol : float\n",
        "            The calculation will terminate if the relative error between two\n",
        "            consecutive iterates is at most `xtol`.\n",
        "        maxfev : int, optional\n",
        "            The maximum number of calls to the function. If zero, then\n",
        "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "            in `x0`.\n",
        "        band : tuple, optional\n",
        "            If set to a two-sequence containing the number of sub- and\n",
        "            super-diagonals within the band of the Jacobi matrix, the\n",
        "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "        epsfcn : float, optional\n",
        "            A suitable step length for the forward-difference\n",
        "            approximation of the Jacobian (for ``fprime=None``). If\n",
        "            `epsfcn` is less than the machine precision, it is assumed\n",
        "            that the relative errors in the functions are of the order of\n",
        "            the machine precision.\n",
        "        factor : float, optional\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``).  Should be in the interval\n",
        "            ``(0.1, 100)``.\n",
        "        diag : sequence, optional\n",
        "            N positive entries that serve as a scale factors for the\n",
        "            variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for\n",
        "            an unsuccessful call).\n",
        "        infodict : dict\n",
        "            A dictionary of optional outputs with the keys:\n",
        "        \n",
        "            ``nfev``\n",
        "                number of function calls\n",
        "            ``njev``\n",
        "                number of Jacobian calls\n",
        "            ``fvec``\n",
        "                function evaluated at the output\n",
        "            ``fjac``\n",
        "                the orthogonal matrix, q, produced by the QR\n",
        "                factorization of the final approximate Jacobian\n",
        "                matrix, stored column wise\n",
        "            ``r``\n",
        "                upper triangular matrix produced by QR factorization\n",
        "                of the same matrix\n",
        "            ``qtf``\n",
        "                the vector ``(transpose(q) * fvec)``\n",
        "        \n",
        "        ier : int\n",
        "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
        "            to `mesg` for more information.\n",
        "        mesg : str\n",
        "            If no solution is found, `mesg` details the cause of failure.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        root : Interface to root finding algorithms for multivariate\n",
        "        functions. See the 'hybr' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
        "    \n",
        "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0)\n",
        "        Return the minimum of a function of one variable.\n",
        "        \n",
        "        Given a function of one variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            Objective function to minimize.\n",
        "        args : tuple\n",
        "            Additional arguments (if present), passed to func.\n",
        "        brack : tuple\n",
        "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,\n",
        "            c), then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            x tolerance stop criterion\n",
        "        full_output : bool\n",
        "            If True, return optional outputs.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Golden' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses analog of bisection method to decrease the bracketed\n",
        "        interval.\n",
        "    \n",
        "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
        "        Minimize the sum of squares of a set of equations.\n",
        "        \n",
        "        ::\n",
        "        \n",
        "            x = arg min(sum(func(y)**2,axis=0))\n",
        "                     y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            should take at least one (possibly length N vector) argument and\n",
        "            returns M floating point numbers.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the minimization.\n",
        "        args : tuple\n",
        "            Any extra arguments to func are placed in this tuple.\n",
        "        Dfun : callable\n",
        "            A function or method to compute the Jacobian of func with derivatives\n",
        "            across the rows. If this is None, the Jacobian will be estimated.\n",
        "        full_output : bool\n",
        "            non-zero to return all optional outputs.\n",
        "        col_deriv : bool\n",
        "            non-zero to specify that the Jacobian function computes derivatives\n",
        "            down the columns (faster, because there is no transpose operation).\n",
        "        ftol : float\n",
        "            Relative error desired in the sum of squares.\n",
        "        xtol : float\n",
        "            Relative error desired in the approximate solution.\n",
        "        gtol : float\n",
        "            Orthogonality desired between the function vector and the columns of\n",
        "            the Jacobian.\n",
        "        maxfev : int\n",
        "            The maximum number of calls to the function. If zero, then 100*(N+1) is\n",
        "            the maximum where N is the number of elements in x0.\n",
        "        epsfcn : float\n",
        "            A suitable step length for the forward-difference approximation of the\n",
        "            Jacobian (for Dfun=None). If epsfcn is less than the machine precision,\n",
        "            it is assumed that the relative errors in the functions are of the\n",
        "            order of the machine precision.\n",
        "        factor : float\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "        diag : sequence\n",
        "            N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for an unsuccessful\n",
        "            call).\n",
        "        cov_x : ndarray\n",
        "            Uses the fjac and ipvt optional outputs to construct an\n",
        "            estimate of the jacobian around the solution. None if a\n",
        "            singular matrix encountered (indicates very flat curvature in\n",
        "            some direction).  This matrix must be multiplied by the\n",
        "            residual variance to get the covariance of the\n",
        "            parameter estimates -- see curve_fit.\n",
        "        infodict : dict\n",
        "            a dictionary of optional outputs with the key s:\n",
        "        \n",
        "            ``nfev``\n",
        "                The number of function calls\n",
        "            ``fvec``\n",
        "                The function evaluated at the output\n",
        "            ``fjac``\n",
        "                A permutation of the R matrix of a QR\n",
        "                factorization of the final approximate\n",
        "                Jacobian matrix, stored column wise.\n",
        "                Together with ipvt, the covariance of the\n",
        "                estimate can be approximated.\n",
        "            ``ipvt``\n",
        "                An integer array of length N which defines\n",
        "                a permutation matrix, p, such that\n",
        "                fjac*p = q*r, where r is upper triangular\n",
        "                with diagonal elements of nonincreasing\n",
        "                magnitude. Column j of p is column ipvt(j)\n",
        "                of the identity matrix.\n",
        "            ``qtf``\n",
        "                The vector (transpose(q) * fvec).\n",
        "        \n",
        "        mesg : str\n",
        "            A string message giving information about the cause of failure.\n",
        "        ier : int\n",
        "            An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\n",
        "            found.  Otherwise, the solution was not found. In either case, the\n",
        "            optional output variable 'mesg' gives more information.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
        "        \n",
        "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
        "        objective function.\n",
        "        This approximation assumes that the objective function is based on the\n",
        "        difference between some observed target data (ydata) and a (non-linear)\n",
        "        function of the parameters `f(xdata, params)` ::\n",
        "        \n",
        "               func(params) = ydata - f(xdata, params)\n",
        "        \n",
        "        so that the objective function is ::\n",
        "        \n",
        "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
        "             params\n",
        "    \n",
        "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)\n",
        "        Find alpha that satisfies strong Wolfe conditions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function.\n",
        "        myfprime : callable f'(x,*args)\n",
        "            Objective function gradient.\n",
        "        xk : ndarray\n",
        "            Starting point.\n",
        "        pk : ndarray\n",
        "            Search direction.\n",
        "        gfk : ndarray, optional\n",
        "            Gradient value for x=xk (xk being the current parameter\n",
        "            estimate). Will be recomputed if omitted.\n",
        "        old_fval : float, optional\n",
        "            Function value for x=xk. Will be recomputed if omitted.\n",
        "        old_old_fval : float, optional\n",
        "            Function value for the point preceding x=xk\n",
        "        args : tuple, optional\n",
        "            Additional arguments passed to objective function.\n",
        "        c1 : float, optional\n",
        "            Parameter for Armijo condition rule.\n",
        "        c2 : float, optional\n",
        "            Parameter for curvature condition rule.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        alpha0 : float\n",
        "            Alpha for which ``x_new = x0 + alpha * pk``.\n",
        "        fc : int\n",
        "            Number of function evaluations made.\n",
        "        gc : int\n",
        "            Number of gradient evaluations made.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses the line search algorithm to enforce strong Wolfe\n",
        "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
        "        1999, pg. 59-60.\n",
        "        \n",
        "        For the zoom phase it uses an algorithm by [...].\n",
        "    \n",
        "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a scalar Jacobian approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            The Jacobian approximation is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    minimize(fun, x0, args=(), method='BFGS', jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
        "        Minimization of scalar function of one or more variables.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its\n",
        "            derivatives (Jacobian, Hessian).\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Nelder-Mead'\n",
        "                - 'Powell'\n",
        "                - 'CG'\n",
        "                - 'BFGS'\n",
        "                - 'Newton-CG'\n",
        "                - 'Anneal'\n",
        "                - 'L-BFGS-B'\n",
        "                - 'TNC'\n",
        "                - 'COBYLA'\n",
        "                - 'SLSQP'\n",
        "                - 'dogleg'\n",
        "                - 'trust-ncg'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            Jacobian of objective function. Only for CG, BFGS, Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of the\n",
        "            objective. In this case, it must accept the same arguments as `fun`.\n",
        "        hess, hessp : callable, optional\n",
        "            Hessian of objective function or Hessian of objective function\n",
        "            times an arbitrary vector p.  Only for Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
        "            provided, then `hessp` will be ignored.  If neither `hess` nor\n",
        "            `hessp` is provided, then the hessian product will be approximated\n",
        "            using finite differences on `jac`. `hessp` must compute the Hessian\n",
        "            times an arbitrary vector.\n",
        "        bounds : sequence, optional\n",
        "            Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        constraints : dict or sequence of dict, optional\n",
        "            Constraints definition (only for COBYLA and SLSQP).\n",
        "            Each constraint is defined in a dictionary with fields:\n",
        "                type : str\n",
        "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
        "                fun : callable\n",
        "                    The function defining the constraint.\n",
        "                jac : callable, optional\n",
        "                    The Jacobian of `fun` (only for SLSQP).\n",
        "                args : sequence, optional\n",
        "                    Extra arguments to be passed to the function and Jacobian.\n",
        "            Equality constraint means that the constraint function result is to\n",
        "            be zero whereas inequality means that it is to be non-negative.\n",
        "            Note that COBYLA only supports inequality constraints.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. All methods accept the following\n",
        "            generic options:\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "            For method-specific options, see `show_options('minimize', method)`.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *BFGS*.\n",
        "        \n",
        "        **Unconstrained minimization**\n",
        "        \n",
        "        Method *Nelder-Mead* uses the Simplex algorithm [1]_, [2]_. This\n",
        "        algorithm has been successful in many applications but other algorithms\n",
        "        using the first and/or second derivatives information might be preferred\n",
        "        for their better performances and robustness in general.\n",
        "        \n",
        "        Method *Powell* is a modification of Powell's method [3]_, [4]_ which\n",
        "        is a conjugate direction method. It performs sequential one-dimensional\n",
        "        minimizations along each vector of the directions set (`direc` field in\n",
        "        `options` and `info`), which is updated at each iteration of the main\n",
        "        minimization loop. The function need not be differentiable, and no\n",
        "        derivatives are taken.\n",
        "        \n",
        "        Method *CG* uses a nonlinear conjugate gradient algorithm by Polak and\n",
        "        Ribiere, a variant of the Fletcher-Reeves method described in [5]_ pp.\n",
        "        120-122. Only the first derivatives are used.\n",
        "        \n",
        "        Method *BFGS* uses the quasi-Newton method of Broyden, Fletcher,\n",
        "        Goldfarb, and Shanno (BFGS) [5]_ pp. 136. It uses the first derivatives\n",
        "        only. BFGS has proven good performance even for non-smooth\n",
        "        optimizations. This method also returns an approximation of the Hessian\n",
        "        inverse, stored as `hess_inv` in the Result object.\n",
        "        \n",
        "        Method *Newton-CG* uses a Newton-CG algorithm [5]_ pp. 168 (also known\n",
        "        as the truncated Newton method). It uses a CG method to the compute the\n",
        "        search direction. See also *TNC* method for a box-constrained\n",
        "        minimization with a similar algorithm.\n",
        "        \n",
        "        Method *Anneal* uses simulated annealing, which is a probabilistic\n",
        "        metaheuristic algorithm for global optimization. It uses no derivative\n",
        "        information from the function being optimized.\n",
        "        \n",
        "        Method *dogleg* uses the dog-leg trust-region algorithm [5]_\n",
        "        for unconstrained minimization. This algorithm requires the gradient\n",
        "        and Hessian; furthermore the Hessian is required to be positive definite.\n",
        "        \n",
        "        Method *trust-ncg* uses the Newton conjugate gradient trust-region\n",
        "        algorithm [5]_ for unconstrained minimization. This algorithm requires\n",
        "        the gradient and either the Hessian or a function that computes the\n",
        "        product of the Hessian with a given vector.\n",
        "        \n",
        "        **Constrained minimization**\n",
        "        \n",
        "        Method *L-BFGS-B* uses the L-BFGS-B algorithm [6]_, [7]_ for bound\n",
        "        constrained minimization.\n",
        "        \n",
        "        Method *TNC* uses a truncated Newton algorithm [5]_, [8]_ to minimize a\n",
        "        function with variables subject to bounds. This algorithm uses\n",
        "        gradient information; it is also called Newton Conjugate-Gradient. It\n",
        "        differs from the *Newton-CG* method described above as it wraps a C\n",
        "        implementation and allows each variable to be given upper and lower\n",
        "        bounds.\n",
        "        \n",
        "        Method *COBYLA* uses the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method [9]_, [10]_, [11]_. The algorithm is\n",
        "        based on linear approximations to the objective function and each\n",
        "        constraint. The method wraps a FORTRAN implementation of the algorithm.\n",
        "        \n",
        "        Method *SLSQP* uses Sequential Least SQuares Programming to minimize a\n",
        "        function of several variables with any combination of bounds, equality\n",
        "        and inequality constraints. The method wraps the SLSQP Optimization\n",
        "        subroutine originally implemented by Dieter Kraft [12]_. Note that the\n",
        "        wrapper handles infinite values in bounds by converting them into large\n",
        "        floating values.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
        "            Minimization. The Computer Journal 7: 308-13.\n",
        "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
        "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
        "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
        "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
        "            191-208.\n",
        "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
        "           a function of several variables without calculating derivatives. The\n",
        "           Computer Journal 7: 155-162.\n",
        "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
        "           Numerical Recipes (any edition), Cambridge University Press.\n",
        "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
        "           Springer New York.\n",
        "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
        "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
        "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
        "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
        "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
        "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
        "           550-560.\n",
        "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
        "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
        "        .. [9] Powell, M J D. A direct search optimization method that models\n",
        "           the objective and constraint functions by linear interpolation.\n",
        "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
        "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
        "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
        "           calculations. 1998. Acta Numerica 7: 287-336.\n",
        "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
        "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
        "           2007/NA03\n",
        "        .. [12] Kraft, D. A software package for sequential quadratic\n",
        "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
        "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
        "        function (and its respective derivatives) is implemented in `rosen`\n",
        "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
        "        \n",
        "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
        "        \n",
        "        A simple application of the *Nelder-Mead* method is:\n",
        "        \n",
        "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
        "        >>> res = minimize(rosen, x0, method='Nelder-Mead')\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        \n",
        "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
        "        options:\n",
        "        \n",
        "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
        "        ...                options={'gtol': 1e-6, 'disp': True})\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 0.000000\n",
        "                 Iterations: 52\n",
        "                 Function evaluations: 64\n",
        "                 Gradient evaluations: 64\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        >>> print res.message\n",
        "        Optimization terminated successfully.\n",
        "        >>> res.hess\n",
        "        [[ 0.00749589  0.01255155  0.02396251  0.04750988  0.09495377]\n",
        "         [ 0.01255155  0.02510441  0.04794055  0.09502834  0.18996269]\n",
        "         [ 0.02396251  0.04794055  0.09631614  0.19092151  0.38165151]\n",
        "         [ 0.04750988  0.09502834  0.19092151  0.38341252  0.7664427 ]\n",
        "         [ 0.09495377  0.18996269  0.38165151  0.7664427   1.53713523]]\n",
        "        \n",
        "        \n",
        "        Next, consider a minimization problem with several constraints (namely\n",
        "        Example 16.4 from [5]_). The objective function is:\n",
        "        \n",
        "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
        "        \n",
        "        There are three constraints defined as:\n",
        "        \n",
        "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
        "        \n",
        "        And variables must be positive, hence the following bounds:\n",
        "        \n",
        "        >>> bnds = ((0, None), (0, None))\n",
        "        \n",
        "        The optimization problem is solved using the SLSQP method as:\n",
        "        \n",
        "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
        "        ...                constraints=cons)\n",
        "        \n",
        "        It should converge to the theoretical solution (1.4 ,1.7).\n",
        "    \n",
        "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
        "        Minimization of scalar function of one variable.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "            Scalar function, must return a scalar.\n",
        "        bracket : sequence, optional\n",
        "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
        "            interval and can either have three items `(a, b, c)` so that `a < b\n",
        "            < c` and `fun(b) < fun(a), fun(c)` or two items `a` and `c` which\n",
        "            are assumed to be a starting interval for a downhill bracket search\n",
        "            (see `bracket`); it doesn't always mean that the obtained solution\n",
        "            will satisfy `a <= x <= c`.\n",
        "        bounds : sequence, optional\n",
        "            For method 'bounded', `bounds` is mandatory and must have two items\n",
        "            corresponding to the optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Brent'\n",
        "                - 'Bounded'\n",
        "                - 'Golden'\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options.\n",
        "                xtol : float\n",
        "                    Relative error in solution `xopt` acceptable for\n",
        "                    convergence.\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for scalar multivariate\n",
        "            functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *Brent*.\n",
        "        \n",
        "        Method *Brent* uses Brent's algorithm to find a local minimum.\n",
        "        The algorithm uses inverse parabolic interpolation when possible to\n",
        "        speed up convergence of the golden section method.\n",
        "        \n",
        "        Method *Golden* uses the golden section search technique. It uses\n",
        "        analog of the bisection method to decrease the bracketed interval. It\n",
        "        is usually preferable to use the *Brent* method.\n",
        "        \n",
        "        Method *Bounded* can perform bounded minimization. It uses the Brent\n",
        "        method to find a local minimum in the interval x1 < xopt < x2.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Consider the problem of minimizing the following function.\n",
        "        \n",
        "        >>> def f(x):\n",
        "        ...     return (x - 2) * x * (x + 2)**2\n",
        "        \n",
        "        Using the *Brent* method, we find the local minimum as:\n",
        "        \n",
        "        >>> from scipy.optimize import minimize_scalar\n",
        "        >>> res = minimize_scalar(f)\n",
        "        >>> res.x\n",
        "        1.28077640403\n",
        "        \n",
        "        Using the *Bounded* method, we find a local minimum with specified\n",
        "        bounds as:\n",
        "        \n",
        "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
        "        >>> res.x\n",
        "        -2.0000002026\n",
        "    \n",
        "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)\n",
        "        Find a zero using the Newton-Raphson or secant method.\n",
        "        \n",
        "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
        "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
        "        is provided, otherwise the secant method is used.  If the second order\n",
        "        derivate `fprime2` of `func` is provided, parabolic Halley's method\n",
        "        is used.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            The function whose zero is wanted. It must be a function of a\n",
        "            single variable of the form f(x,a,b,c...), where a,b,c... are extra\n",
        "            arguments that can be passed in the `args` parameter.\n",
        "        x0 : float\n",
        "            An initial estimate of the zero that should be somewhere near the\n",
        "            actual zero.\n",
        "        fprime : function, optional\n",
        "            The derivative of the function when available and convenient. If it\n",
        "            is None (default), then the secant method is used.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to be used in the function call.\n",
        "        tol : float, optional\n",
        "            The allowable error of the zero value.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations.\n",
        "        fprime2 : function, optional\n",
        "            The second order derivative of the function when available and\n",
        "            convenient. If it is None (default), then the normal Newton-Raphson\n",
        "            or the secant method is used. If it is given, parabolic Halley's\n",
        "            method is used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        zero : float\n",
        "            Estimated location where function is zero.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, ridder, bisect\n",
        "        fsolve : find zeroes in n dimensions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The convergence rate of the Newton-Raphson method is quadratic,\n",
        "        the Halley method is cubic, and the secant method is\n",
        "        sub-quadratic.  This means that if the function is well behaved\n",
        "        the actual error in the estimated zero is approximately the square\n",
        "        (cube for Halley) of the requested tolerance up to roundoff\n",
        "        error. However, the stopping criterion used here is the step size\n",
        "        and there is no guarantee that a zero has been found. Consequently\n",
        "        the result should be verified. Safer algorithms are brentq,\n",
        "        brenth, ridder, and bisect, but they all require that the root\n",
        "        first be bracketed in an interval where the function changes\n",
        "        sign. The brentq algorithm is recommended for general use in one\n",
        "        dimensional problems when such an interval has been found.\n",
        "    \n",
        "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
        "        \n",
        "        This method is suitable for solving large-scale problems.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        rdiff : float, optional\n",
        "            Relative step size to use in numerical differentiation.\n",
        "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
        "            Krylov method to use to approximate the Jacobian.\n",
        "            Can be a string, or a function implementing the same interface as\n",
        "            the iterative solvers in `scipy.sparse.linalg`.\n",
        "        \n",
        "            The default is `scipy.sparse.linalg.lgmres`.\n",
        "        inner_M : LinearOperator or InverseJacobian\n",
        "            Preconditioner for the inner Krylov iteration.\n",
        "            Note that you can use also inverse Jacobians as (adaptive)\n",
        "            preconditioners. For example,\n",
        "        \n",
        "            >>> jac = BroydenFirst()\n",
        "            >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "            If the preconditioner has a method named 'update', it will be called\n",
        "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
        "            the current point, and ``f`` the current function value.\n",
        "        inner_tol, inner_maxiter, ...\n",
        "            Parameters to pass on to the \\\"inner\\\" Krylov solver.\n",
        "            See `scipy.sparse.linalg.gmres` for details.\n",
        "        outer_k : int, optional\n",
        "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
        "            See `scipy.sparse.linalg.lgmres` for details.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.sparse.linalg.gmres\n",
        "        scipy.sparse.linalg.lgmres\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This function implements a Newton-Krylov solver. The basic idea is\n",
        "        to compute the inverse of the Jacobian with an iterative Krylov\n",
        "        method. These methods require only evaluating the Jacobian-vector\n",
        "        products, which are conveniently approximated by numerical\n",
        "        differentiation:\n",
        "        \n",
        "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
        "        \n",
        "        Due to the use of iterative matrix inverses, these methods can\n",
        "        deal with large nonlinear problems.\n",
        "        \n",
        "        Scipy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
        "        solvers to choose from. The default here is `lgmres`, which is a\n",
        "        variant of restarted GMRES iteration that reuses some of the\n",
        "        information obtained in the previous Newton steps to invert\n",
        "        Jacobians in subsequent steps.\n",
        "        \n",
        "        For a review on Newton-Krylov methods, see for example [KK]_,\n",
        "        and for the LGMRES sparse inverse method, see [BJM]_.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [KK] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2003).\n",
        "        .. [BJM] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
        "                 SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
        "    \n",
        "    nnls(A, b)\n",
        "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
        "        for a FORTAN non-negative least squares solver.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : ndarray\n",
        "            Matrix ``A`` as shown above.\n",
        "        b : ndarray\n",
        "            Right-hand side vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            Solution vector.\n",
        "        rnorm : float\n",
        "            The residual, ``|| Ax-b ||_2``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The FORTRAN code was published in the book below. The algorithm\n",
        "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
        "        conditions for the non-negative least squares problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
        "    \n",
        "    ridder(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in an interval.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.\n",
        "            In particular, ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton : one-dimensional root-finding\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
        "        generally as fast as the Brent rountines. [Ridders1979]_ provides the\n",
        "        classic description and source of the algorithm. A description can also be\n",
        "        found in any recent edition of Numerical Recipes.\n",
        "        \n",
        "        The routine used here diverges slightly from standard presentations in\n",
        "        order to be a bit more careful of tolerance.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ridders1979]\n",
        "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
        "           Single Root of a Real Continuous Function.\"\n",
        "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
        "    \n",
        "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
        "        Find a root of a vector function.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            A vector function to find a root of.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its Jacobian.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'hybr'\n",
        "                - 'lm'\n",
        "                - 'broyden1'\n",
        "                - 'broyden2'\n",
        "                - 'anderson'\n",
        "                - 'linearmixing'\n",
        "                - 'diagbroyden'\n",
        "                - 'excitingmixing'\n",
        "                - 'krylov'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
        "            this case, it must accept the same arguments as `fun`.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. E.g. `xtol` or `maxiter`, see\n",
        "            ``show_options('root', method)`` for details.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : Result\n",
        "            The solution represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the algorithm exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *hybr*.\n",
        "        \n",
        "        Method *hybr* uses a modification of the Powell hybrid method as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Method *lm* solves the system of nonlinear equations in a least squares\n",
        "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
        "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
        "        with backtracking or full line searches [2]_. Each method corresponds\n",
        "        to a particular Jacobian approximations. See `nonlin` for details.\n",
        "        \n",
        "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
        "          known as Broyden's good method.\n",
        "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
        "          is known as Broyden's bad method.\n",
        "        - Method *anderson* uses (extended) Anderson mixing.\n",
        "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
        "          is suitable for large-scale problem.\n",
        "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
        "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
        "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
        "          approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "            The algorithms implemented for methods *diagbroyden*,\n",
        "            *linearmixing* and *excitingmixing* may be useful for specific\n",
        "            problems, but whether they will work may depend strongly on the\n",
        "            problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
        "           1980. User Guide for MINPACK-1.\n",
        "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
        "            Equations. Society for Industrial and Applied Mathematics.\n",
        "            <http://www.siam.org/books/kelley/>\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following functions define a system of nonlinear equations and its\n",
        "        jacobian.\n",
        "        \n",
        "        >>> def fun(x):\n",
        "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
        "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
        "        \n",
        "        >>> def jac(x):\n",
        "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
        "        ...                       -1.5 * (x[0] - x[1])**2],\n",
        "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
        "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
        "        \n",
        "        A solution can be obtained as follows.\n",
        "        \n",
        "        >>> from scipy import optimize\n",
        "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
        "        >>> sol.x\n",
        "        array([ 0.8411639,  0.1588361])\n",
        "    \n",
        "    rosen(x)\n",
        "        The Rosenbrock function.\n",
        "        \n",
        "        The function computed is::\n",
        "        \n",
        "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Rosenbrock function is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        f : float\n",
        "            The value of the Rosenbrock function.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen_der, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_der(x)\n",
        "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the derivative is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_der : (N,) ndarray\n",
        "            The gradient of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess(x)\n",
        "        The Hessian matrix of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess_prod(x, p)\n",
        "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        p : array_like\n",
        "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess_prod : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
        "            by the vector `p`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess\n",
        "    \n",
        "    show_options(solver, method=None)\n",
        "        Show documentation for additional options of optimization solvers.\n",
        "        \n",
        "        These are method-specific options that can be supplied through the\n",
        "        ``options`` dict.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        solver : str\n",
        "            Type of optimization solver. One of {`minimize`, `root`}.\n",
        "        method : str, optional\n",
        "            If not given, shows all methods of the specified solver. Otherwise,\n",
        "            show only the options for the specified method. Valid values\n",
        "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
        "            'minimize').\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \n",
        "        ** minimize options\n",
        "        \n",
        "        * BFGS options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Nelder-Mead options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "        \n",
        "        * Newton-CG options:\n",
        "            xtol : float\n",
        "                Average relative error in solution `xopt` acceptable for\n",
        "                convergence.\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * CG options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Powell options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            direc : ndarray\n",
        "                Initial set of direction vectors for the Powell method.\n",
        "        \n",
        "        * Anneal options:\n",
        "            ftol : float\n",
        "                Relative error in ``fun(x)`` acceptable for convergence.\n",
        "            schedule : str\n",
        "                Annealing schedule to use. One of: 'fast', 'cauchy' or\n",
        "                'boltzmann'.\n",
        "            T0 : float\n",
        "                Initial Temperature (estimated as 1.2 times the largest\n",
        "                cost-function deviation over random points in the range).\n",
        "            Tf : float\n",
        "                Final goal temperature.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            maxaccept : int\n",
        "                Maximum changes to accept.\n",
        "            boltzmann : float\n",
        "                Boltzmann constant in acceptance test (increase for less\n",
        "                stringent test at each temperature).\n",
        "            learn_rate : float\n",
        "                Scale constant for adjusting guesses.\n",
        "            quench, m, n : float\n",
        "                Parameters to alter fast_sa schedule.\n",
        "            lower, upper : float or ndarray\n",
        "                Lower and upper bounds on `x`.\n",
        "            dwell : int\n",
        "                The number of times to search the space at each temperature.\n",
        "        \n",
        "        * L-BFGS-B options:\n",
        "            ftol : float\n",
        "                The iteration stops when ``(f^k -\n",
        "                f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
        "            gtol : float\n",
        "                The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
        "                <= gtol`` where ``pg_i`` is the i-th component of the\n",
        "                projected gradient.\n",
        "            maxcor : int\n",
        "                The maximum number of variable metric corrections used to\n",
        "                define the limited memory matrix. (The limited memory BFGS\n",
        "                method does not store the full hessian but uses this many terms\n",
        "                in an approximation to it.)\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * TNC options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stoping criterion.\n",
        "                If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "            xtol : float\n",
        "                Precision goal for the value of x in the stopping\n",
        "                criterion (after applying x scaling factors).  If xtol <\n",
        "                0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "                -1.\n",
        "            gtol : float\n",
        "                Precision goal for the value of the projected gradient in\n",
        "                the stopping criterion (after applying x scaling factors).\n",
        "                If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
        "                Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "            scale : list of floats\n",
        "                Scaling factors to apply to each variable.  If None, the\n",
        "                factors are up-low for interval bounded variables and\n",
        "                1+|x] fo the others.  Defaults to None\n",
        "            offset : float\n",
        "                Value to substract from each variable.  If None, the\n",
        "                offsets are (up+low)/2 for interval bounded variables\n",
        "                and x for the others.\n",
        "            maxCGit : int\n",
        "                Maximum number of hessian*vector evaluations per main\n",
        "                iteration.  If maxCGit == 0, the direction chosen is\n",
        "                -gradient if maxCGit < 0, maxCGit is set to\n",
        "                max(1,min(50,n/2)).  Defaults to -1.\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluation.  if None, `maxiter` is\n",
        "                set to max(100, 10*len(x0)).  Defaults to None.\n",
        "            eta : float\n",
        "                Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "                Defaults to -1.\n",
        "            stepmx : float\n",
        "                Maximum step for the line search.  May be increased during\n",
        "                call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "            accuracy : float\n",
        "                Relative precision for finite difference calculations.  If\n",
        "                <= machine_precision, set to sqrt(machine_precision).\n",
        "                Defaults to 0.\n",
        "            minfev : float\n",
        "                Minimum function value estimate.  Defaults to 0.\n",
        "            rescale : float\n",
        "                Scaling factor (in log10) used to trigger f value\n",
        "                rescaling.  If 0, rescale at each iteration.  If a large\n",
        "                value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        \n",
        "        * COBYLA options:\n",
        "            tol : float\n",
        "                Final accuracy in the optimization (not precisely guaranteed).\n",
        "                This is a lower bound on the size of the trust region.\n",
        "            rhobeg : float\n",
        "                Reasonable initial changes to the variables.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * SLSQP options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stopping criterion.\n",
        "            eps : float\n",
        "                Step size used for numerical approximation of the jacobian.\n",
        "            maxiter : int\n",
        "                Maximum number of iterations.\n",
        "        \n",
        "        * dogleg options:\n",
        "            initial_trust_radius : float\n",
        "                Initial trust-region radius.\n",
        "            max_trust_radius : float\n",
        "                Maximum value of the trust-region radius. No steps that are longer\n",
        "                than this value will be proposed.\n",
        "            eta : float\n",
        "                Trust region related acceptance stringency for proposed steps.\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "        \n",
        "        * trust-ncg options:\n",
        "            see dogleg options.\n",
        "        \n",
        "        ** root options\n",
        "        \n",
        "        * hybrd options:\n",
        "            col_deriv : bool\n",
        "                Specify whether the Jacobian function computes derivatives down\n",
        "                the columns (faster, because there is no transpose operation).\n",
        "            xtol : float\n",
        "                The calculation will terminate if the relative error between\n",
        "                two consecutive iterates is at most `xtol`.\n",
        "            maxfev : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "                in `x0`.\n",
        "            band : sequence\n",
        "                If set to a two-sequence containing the number of sub- and\n",
        "                super-diagonals within the band of the Jacobi matrix, the\n",
        "                Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation\n",
        "                of the Jacobian (for ``fprime=None``). If `epsfcn` is less than\n",
        "                the machine precision, it is assumed that the relative errors\n",
        "                in the functions are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound (``factor * ||\n",
        "                diag * x||``).  Should be in the interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the\n",
        "                variables.\n",
        "        \n",
        "        * LM options:\n",
        "            col_deriv : bool\n",
        "                non-zero to specify that the Jacobian function computes derivatives\n",
        "                down the columns (faster, because there is no transpose operation).\n",
        "            ftol : float\n",
        "                Relative error desired in the sum of squares.\n",
        "            xtol : float\n",
        "                Relative error desired in the approximate solution.\n",
        "            gtol : float\n",
        "                Orthogonality desired between the function vector and the columns\n",
        "                of the Jacobian.\n",
        "            maxiter : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                100*(N+1) is the maximum where N is the number of elements in x0.\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation of\n",
        "                the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
        "                precision, it is assumed that the relative errors in the functions\n",
        "                are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound\n",
        "                (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        * Broyden1 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Broyden2 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Anderson options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    M : float, optional\n",
        "                        Number of previous vectors to retain. Defaults to 5.\n",
        "                    w0 : float, optional\n",
        "                        Regularization parameter for numerical stability.\n",
        "                        Compared to unity, good values of the order of 0.01.\n",
        "        \n",
        "        * LinearMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * DiagBroyden options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * ExcitingMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial Jacobian approximation is (-1/alpha).\n",
        "                    alphamax : float, optional\n",
        "                        The entries of the diagonal Jacobian are kept in the range\n",
        "                        ``[alpha, alphamax]``.\n",
        "        \n",
        "        * Krylov options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    rdiff : float, optional\n",
        "                        Relative step size to use in numerical differentiation.\n",
        "                    method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or\n",
        "                        function\n",
        "                        Krylov method to use to approximate the Jacobian.\n",
        "                        Can be a string, or a function implementing the same\n",
        "                        interface as the iterative solvers in\n",
        "                        `scipy.sparse.linalg`.\n",
        "        \n",
        "                        The default is `scipy.sparse.linalg.lgmres`.\n",
        "                    inner_M : LinearOperator or InverseJacobian\n",
        "                        Preconditioner for the inner Krylov iteration.\n",
        "                        Note that you can use also inverse Jacobians as (adaptive)\n",
        "                        preconditioners. For example,\n",
        "        \n",
        "                        >>> jac = BroydenFirst()\n",
        "                        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "                        If the preconditioner has a method named 'update', it will\n",
        "                        be called as ``update(x, f)`` after each nonlinear step,\n",
        "                        with ``x`` giving the current point, and ``f`` the current\n",
        "                        function value.\n",
        "                    inner_tol, inner_maxiter, ...\n",
        "                        Parameters to pass on to the \"inner\" Krylov solver.\n",
        "                        See `scipy.sparse.linalg.gmres` for details.\n",
        "                    outer_k : int, optional\n",
        "                        Size of the subspace kept across LGMRES nonlinear\n",
        "                        iterations.\n",
        "        \n",
        "                        See `scipy.sparse.linalg.lgmres` for details.\n",
        "\n",
        "DATA\n",
        "    __all__ = ['OptimizeWarning', 'Result', 'absolute_import', 'anderson',...\n",
        "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(optimize.fmin)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function fmin in module scipy.optimize.optimize:\n",
        "\n",
        "fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "    Minimize a function using the downhill simplex algorithm.\n",
        "    \n",
        "    This algorithm only uses function values, not derivatives or second\n",
        "    derivatives.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    func : callable func(x,*args)\n",
        "        The objective function to be minimized.\n",
        "    x0 : ndarray\n",
        "        Initial guess.\n",
        "    args : tuple, optional\n",
        "        Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "    callback : callable, optional\n",
        "        Called after each iteration, as callback(xk), where xk is the\n",
        "        current parameter vector.\n",
        "    xtol : float, optional\n",
        "        Relative error in xopt acceptable for convergence.\n",
        "    ftol : number, optional\n",
        "        Relative error in func(xopt) acceptable for convergence.\n",
        "    maxiter : int, optional\n",
        "        Maximum number of iterations to perform.\n",
        "    maxfun : number, optional\n",
        "        Maximum number of function evaluations to make.\n",
        "    full_output : bool, optional\n",
        "        Set to True if fopt and warnflag outputs are desired.\n",
        "    disp : bool, optional\n",
        "        Set to True to print convergence messages.\n",
        "    retall : bool, optional\n",
        "        Set to True to return list of solutions at each iteration.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    xopt : ndarray\n",
        "        Parameter that minimizes function.\n",
        "    fopt : float\n",
        "        Value of function at minimum: ``fopt = func(xopt)``.\n",
        "    iter : int\n",
        "        Number of iterations performed.\n",
        "    funcalls : int\n",
        "        Number of function calls made.\n",
        "    warnflag : int\n",
        "        1 : Maximum number of function evaluations made.\n",
        "        2 : Maximum number of iterations reached.\n",
        "    allvecs : list\n",
        "        Solution at each iteration.\n",
        "    \n",
        "    See also\n",
        "    --------\n",
        "    minimize: Interface to minimization algorithms for multivariate\n",
        "        functions. See the 'Nelder-Mead' `method` in particular.\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "    one or more variables.\n",
        "    \n",
        "    This algorithm has a long history of successful use in applications.\n",
        "    But it will usually be slower than an algorithm that uses first or\n",
        "    second derivative information. In practice it can have poor\n",
        "    performance in high-dimensional problems and is not robust to\n",
        "    minimizing complicated functions. Additionally, there currently is no\n",
        "    complete theory describing when the algorithm will successfully\n",
        "    converge to the minimum, or how fast it will if it does.\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "           minimization\", The Computer Journal, 7, pp. 308-313\n",
        "    \n",
        "    .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "           Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "           1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "           Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "           Harlow, UK, pp. 191-208.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f_test(x):\n",
      "    return np.dot(x,x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(f_test, np.array([1,2]), xtol = 1.E-10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 85\n",
        "         Function evaluations: 164\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 156,
       "text": [
        "array([  3.31355231e-11,  -1.82983207e-11])"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (y_rand - A * sin(b*x))**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_rand_cut = y_rand[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_cut = x[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def objective_func(A):\n",
      "    return sum((y_rand - A[0] * sin(A[1] * x))**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_fit = optimize.fmin_cg(objective_func, np.array([1,0.9]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: 84.718225\n",
        "         Iterations: 1\n",
        "         Function evaluations: 32\n",
        "         Gradient evaluations: 5\n"
       ]
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 177,
       "text": [
        "array([ 0.96965634,  1.00018575])"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "objective_func(A_fit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 178,
       "text": [
        "84.718225273137918"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x, y_rand, 'b', linestyle = ':')\n",
      "plb.plot(x, A_fit[0] * sin(A_fit[1] * x), 'r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 179,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b1d18d0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b1d1750>"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise:\n",
      "# Generalize the code in fitting_sine_wave.py\n",
      "# to generate sample data with a phase shift\n",
      "#\n",
      "# y = Amp * sin(freq * (x + delta))\n",
      "#\n",
      "# and to fit to a curve of this form.  In other\n",
      "# words, you will have three parameters to be\n",
      "# optimized: Amp, freq, delta, rather than just\n",
      "# the first two."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}