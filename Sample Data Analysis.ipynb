{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(0.,80.,0.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.sin(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as plb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b05eed0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ/h/cZoCbO/FOxoAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x103c43fd0>"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_rand = y + 0.1 * np.random.randn(len(y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,y_rand)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b0de090>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWCoQxjRNxgMhjrE/wfsznHHqYSA2gAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a4660d0>"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(optimize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on package scipy.optimize in scipy:\n",
        "\n",
        "NAME\n",
        "    scipy.optimize\n",
        "\n",
        "FILE\n",
        "    /Users/mlightma/anaconda/lib/python2.7/site-packages/scipy/optimize/__init__.py\n",
        "\n",
        "DESCRIPTION\n",
        "    =====================================================\n",
        "    Optimization and root finding (:mod:`scipy.optimize`)\n",
        "    =====================================================\n",
        "    \n",
        "    .. currentmodule:: scipy.optimize\n",
        "    \n",
        "    Optimization\n",
        "    ============\n",
        "    \n",
        "    General-purpose\n",
        "    ---------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize - Unified interface for minimizers of multivariate functions\n",
        "       fmin - Nelder-Mead Simplex algorithm\n",
        "       fmin_powell - Powell's (modified) level set method\n",
        "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm\n",
        "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno)\n",
        "       fmin_ncg - Line-search Newton Conjugate Gradient\n",
        "       leastsq - Minimize the sum of squares of M equations in N unknowns\n",
        "    \n",
        "    Constrained (multivariate)\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer\n",
        "       fmin_tnc - Truncated Newton code\n",
        "       fmin_cobyla - Constrained optimization by linear approximation\n",
        "       fmin_slsqp - Minimization using sequential least-squares programming\n",
        "       nnls - Linear least-squares problem with non-negativity constraint\n",
        "    \n",
        "    Global\n",
        "    ------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       anneal - Simulated annealing\n",
        "       basinhopping - Basinhopping stochastic optimizer\n",
        "       brute - Brute force searching optimizer\n",
        "    \n",
        "    Scalar function minimizers\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize_scalar - Unified interface for minimizers of univariate functions\n",
        "       fminbound - Bounded minimization of a scalar function\n",
        "       brent - 1-D function minimization using Brent method\n",
        "       golden - 1-D function minimization using Golden Section method\n",
        "       bracket - Bracket a minimum, given two starting points\n",
        "    \n",
        "    Rosenbrock function\n",
        "    -------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       rosen - The Rosenbrock function.\n",
        "       rosen_der - The derivative of the Rosenbrock function.\n",
        "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
        "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
        "    \n",
        "    Fitting\n",
        "    =======\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       curve_fit -- Fit curve to a set of points\n",
        "    \n",
        "    Root finding\n",
        "    ============\n",
        "    \n",
        "    Scalar functions\n",
        "    ----------------\n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       brentq - quadratic interpolation Brent method\n",
        "       brenth - Brent method, modified by Harris with hyperbolic extrapolation\n",
        "       ridder - Ridder's method\n",
        "       bisect - Bisection method\n",
        "       newton - Secant method or Newton's method\n",
        "    \n",
        "    Fixed point finding:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fixed_point - Single-variable fixed-point solver\n",
        "    \n",
        "    Multidimensional\n",
        "    ----------------\n",
        "    \n",
        "    General nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       root - Unified interface for nonlinear solvers of multivariate functions\n",
        "       fsolve - Non-linear multi-variable equation solver\n",
        "       broyden1 - Broyden's first method\n",
        "       broyden2 - Broyden's second method\n",
        "    \n",
        "    Large-scale nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       newton_krylov\n",
        "       anderson\n",
        "    \n",
        "    Simple iterations:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       excitingmixing\n",
        "       linearmixing\n",
        "       diagbroyden\n",
        "    \n",
        "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
        "    \n",
        "    Utility Functions\n",
        "    =================\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       line_search - Return a step that satisfies the strong Wolfe conditions\n",
        "       check_grad - Check the supplied derivative using finite differences\n",
        "    \n",
        "       show_options - Show specific options optimization solvers\n",
        "\n",
        "PACKAGE CONTENTS\n",
        "    _basinhopping\n",
        "    _cobyla\n",
        "    _lbfgsb\n",
        "    _minimize\n",
        "    _minpack\n",
        "    _nnls\n",
        "    _root\n",
        "    _slsqp\n",
        "    _trustregion\n",
        "    _trustregion_dogleg\n",
        "    _trustregion_ncg\n",
        "    _tstutils\n",
        "    _zeros\n",
        "    anneal\n",
        "    cobyla\n",
        "    lbfgsb\n",
        "    linesearch\n",
        "    minpack\n",
        "    minpack2\n",
        "    moduleTNC\n",
        "    nnls\n",
        "    nonlin\n",
        "    optimize\n",
        "    setup\n",
        "    slsqp\n",
        "    tnc\n",
        "    zeros\n",
        "\n",
        "CLASSES\n",
        "    __builtin__.dict(__builtin__.object)\n",
        "        scipy.optimize.optimize.Result\n",
        "    exceptions.UserWarning(exceptions.Warning)\n",
        "        scipy.optimize.optimize.OptimizeWarning\n",
        "    \n",
        "    class OptimizeWarning(exceptions.UserWarning)\n",
        "     |  Method resolution order:\n",
        "     |      OptimizeWarning\n",
        "     |      exceptions.UserWarning\n",
        "     |      exceptions.Warning\n",
        "     |      exceptions.Exception\n",
        "     |      exceptions.BaseException\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __delattr__(...)\n",
        "     |      x.__delattr__('name') <==> del x.name\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __getslice__(...)\n",
        "     |      x.__getslice__(i, j) <==> x[i:j]\n",
        "     |      \n",
        "     |      Use of negative indices is not supported.\n",
        "     |  \n",
        "     |  __reduce__(...)\n",
        "     |  \n",
        "     |  __repr__(...)\n",
        "     |      x.__repr__() <==> repr(x)\n",
        "     |  \n",
        "     |  __setattr__(...)\n",
        "     |      x.__setattr__('name', value) <==> x.name = value\n",
        "     |  \n",
        "     |  __setstate__(...)\n",
        "     |  \n",
        "     |  __str__(...)\n",
        "     |      x.__str__() <==> str(x)\n",
        "     |  \n",
        "     |  __unicode__(...)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |  \n",
        "     |  args\n",
        "     |  \n",
        "     |  message\n",
        "    \n",
        "    class Result(__builtin__.dict)\n",
        "     |  Represents the optimization result.\n",
        "     |  \n",
        "     |  Attributes\n",
        "     |  ----------\n",
        "     |  x : ndarray\n",
        "     |      The solution of the optimization.\n",
        "     |  success : bool\n",
        "     |      Whether or not the optimizer exited successfully.\n",
        "     |  status : int\n",
        "     |      Termination status of the optimizer. Its value depends on the\n",
        "     |      underlying solver. Refer to `message` for details.\n",
        "     |  message : str\n",
        "     |      Description of the cause of the termination.\n",
        "     |  fun, jac, hess, hess_inv : ndarray\n",
        "     |      Values of objective function, Jacobian, Hessian or its inverse (if\n",
        "     |      available). The Hessians may be approximations, see the documentation\n",
        "     |      of the function in question.\n",
        "     |  nfev, njev, nhev : int\n",
        "     |      Number of evaluations of the objective functions and of its\n",
        "     |      Jacobian and Hessian.\n",
        "     |  nit : int\n",
        "     |      Number of iterations performed by the optimizer.\n",
        "     |  maxcv : float\n",
        "     |      The maximum constraint violation.\n",
        "     |  \n",
        "     |  Notes\n",
        "     |  -----\n",
        "     |  There may be additional attributes not listed above depending of the\n",
        "     |  specific solver. Since this class is essentially a subclass of dict\n",
        "     |  with attribute accessors, one can see which attributes are available\n",
        "     |  using the `keys()` method.\n",
        "     |  \n",
        "     |  Method resolution order:\n",
        "     |      Result\n",
        "     |      __builtin__.dict\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __delattr__ = __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __getattr__(self, name)\n",
        "     |  \n",
        "     |  __repr__(self)\n",
        "     |  \n",
        "     |  __setattr__ = __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |      dictionary for instance variables (if defined)\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __cmp__(...)\n",
        "     |      x.__cmp__(y) <==> cmp(x,y)\n",
        "     |  \n",
        "     |  __contains__(...)\n",
        "     |      D.__contains__(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __eq__(...)\n",
        "     |      x.__eq__(y) <==> x==y\n",
        "     |  \n",
        "     |  __ge__(...)\n",
        "     |      x.__ge__(y) <==> x>=y\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __gt__(...)\n",
        "     |      x.__gt__(y) <==> x>y\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  __iter__(...)\n",
        "     |      x.__iter__() <==> iter(x)\n",
        "     |  \n",
        "     |  __le__(...)\n",
        "     |      x.__le__(y) <==> x<=y\n",
        "     |  \n",
        "     |  __len__(...)\n",
        "     |      x.__len__() <==> len(x)\n",
        "     |  \n",
        "     |  __lt__(...)\n",
        "     |      x.__lt__(y) <==> x<y\n",
        "     |  \n",
        "     |  __ne__(...)\n",
        "     |      x.__ne__(y) <==> x!=y\n",
        "     |  \n",
        "     |  __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  __sizeof__(...)\n",
        "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
        "     |  \n",
        "     |  clear(...)\n",
        "     |      D.clear() -> None.  Remove all items from D.\n",
        "     |  \n",
        "     |  copy(...)\n",
        "     |      D.copy() -> a shallow copy of D\n",
        "     |  \n",
        "     |  fromkeys(...)\n",
        "     |      dict.fromkeys(S[,v]) -> New dict with keys from S and values equal to v.\n",
        "     |      v defaults to None.\n",
        "     |  \n",
        "     |  get(...)\n",
        "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
        "     |  \n",
        "     |  has_key(...)\n",
        "     |      D.has_key(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  items(...)\n",
        "     |      D.items() -> list of D's (key, value) pairs, as 2-tuples\n",
        "     |  \n",
        "     |  iteritems(...)\n",
        "     |      D.iteritems() -> an iterator over the (key, value) items of D\n",
        "     |  \n",
        "     |  iterkeys(...)\n",
        "     |      D.iterkeys() -> an iterator over the keys of D\n",
        "     |  \n",
        "     |  itervalues(...)\n",
        "     |      D.itervalues() -> an iterator over the values of D\n",
        "     |  \n",
        "     |  keys(...)\n",
        "     |      D.keys() -> list of D's keys\n",
        "     |  \n",
        "     |  pop(...)\n",
        "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
        "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
        "     |  \n",
        "     |  popitem(...)\n",
        "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
        "     |      2-tuple; but raise KeyError if D is empty.\n",
        "     |  \n",
        "     |  setdefault(...)\n",
        "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
        "     |  \n",
        "     |  update(...)\n",
        "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
        "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
        "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
        "     |      In either case, this is followed by: for k in F: D[k] = F[k]\n",
        "     |  \n",
        "     |  values(...)\n",
        "     |      D.values() -> list of D's values\n",
        "     |  \n",
        "     |  viewitems(...)\n",
        "     |      D.viewitems() -> a set-like object providing a view on D's items\n",
        "     |  \n",
        "     |  viewkeys(...)\n",
        "     |      D.viewkeys() -> a set-like object providing a view on D's keys\n",
        "     |  \n",
        "     |  viewvalues(...)\n",
        "     |      D.viewvalues() -> an object providing a view on D's values\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __hash__ = None\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "\n",
        "FUNCTIONS\n",
        "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using (extended) Anderson mixing.\n",
        "        \n",
        "        The Jacobian is formed by for a 'best' solution in the space\n",
        "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
        "        inversions and MxN multiplications are required. [Ey]_\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        M : float, optional\n",
        "            Number of previous vectors to retain. Defaults to 5.\n",
        "        w0 : float, optional\n",
        "            Regularization parameter for numerical stability.\n",
        "            Compared to unity, good values of the order of 0.01.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
        "    \n",
        "    anneal(func, x0, args=(), schedule='fast', full_output=0, T0=None, Tf=1e-12, maxeval=None, maxaccept=None, maxiter=400, boltzmann=1.0, learn_rate=0.5, feps=1e-06, quench=1.0, m=1.0, n=1.0, lower=-100, upper=100, dwell=50, disp=True)\n",
        "        Minimize a function using simulated annealing.\n",
        "        \n",
        "        Uses simulated annealing, a random algorithm that uses no derivative\n",
        "        information from the function being optimized. Other names for this\n",
        "        family of approaches include: \"Monte Carlo\", \"Metropolis\",\n",
        "        \"Metropolis-Hastings\", `etc`. They all involve (a) evaluating the\n",
        "        objective function on a random set of points, (b) keeping those that\n",
        "        pass their randomized evaluation critera, (c) cooling (`i.e.`,\n",
        "        tightening) the evaluation critera, and (d) repeating until their\n",
        "        termination critera are met.  In practice they have been used mainly in\n",
        "        discrete rather than in continuous optimization.\n",
        "        \n",
        "        Available annealing schedules are 'fast', 'cauchy' and 'boltzmann'.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized.  Must be in the form\n",
        "            `f(x, *args)`, where `x` is the argument in the form of a 1-D array\n",
        "            and `args` is a  tuple of any additional fixed parameters needed to\n",
        "            completely specify the function.\n",
        "        x0: 1-D array\n",
        "            An initial guess at the optimizing argument of `func`.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely\n",
        "            specify the objective function.\n",
        "        schedule : str, optional\n",
        "            The annealing schedule to use.  Must be one of 'fast', 'cauchy' or\n",
        "            'boltzmann'.  See `Notes`.\n",
        "        full_output : bool, optional\n",
        "            If `full_output`, then return all values listed in the Returns\n",
        "            section. Otherwise, return just the `xmin` and `status` values.\n",
        "        T0 : float, optional\n",
        "            The initial \"temperature\".  If None, then estimate it as 1.2 times\n",
        "            the largest cost-function deviation over random points in the\n",
        "            box-shaped region specified by the `lower, upper` input parameters.\n",
        "        Tf : float, optional\n",
        "            Final goal temperature.  Cease iterations if the temperature\n",
        "            falls below `Tf`.\n",
        "        maxeval : int, optional\n",
        "            Cease iterations if the number of function evaluations exceeds\n",
        "            `maxeval`.\n",
        "        maxaccept : int, optional\n",
        "            Cease iterations if the number of points accepted exceeds `maxaccept`.\n",
        "            See `Notes` for the probabilistic acceptance criteria used.\n",
        "        maxiter : int, optional\n",
        "            Cease iterations if the number of cooling iterations exceeds `maxiter`.\n",
        "        learn_rate : float, optional\n",
        "            Scale constant for tuning the probabilistc acceptance criteria.\n",
        "        boltzmann : float, optional\n",
        "            Boltzmann constant in the probabilistic acceptance criteria\n",
        "            (increase for less stringent criteria at each temperature).\n",
        "        feps : float, optional\n",
        "            Cease iterations if the relative errors in the function value over the\n",
        "            last four coolings is below `feps`.\n",
        "        quench, m, n : floats, optional\n",
        "            Parameters to alter the `fast` simulated annealing schedule.\n",
        "            See `Notes`.\n",
        "        lower, upper : floats or 1-D arrays, optional\n",
        "            Lower and upper bounds on the argument `x`.  If floats are provided,\n",
        "            they apply to all components of `x`.\n",
        "        dwell : int, optional\n",
        "            The number of times to execute the inner loop at each value of the\n",
        "            temperature.  See `Notes`.\n",
        "        disp : bool, optional\n",
        "            Print a descriptive convergence message if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            The point where the lowest function value was found.\n",
        "        Jmin : float\n",
        "            The objective function value at `xmin`.\n",
        "        T : float\n",
        "            The temperature at termination of the iterations.\n",
        "        feval : int\n",
        "            Number of function evaluations used.\n",
        "        iters : int\n",
        "            Number of cooling iterations used.\n",
        "        accept : int\n",
        "            Number of tests accepted.\n",
        "        status : int\n",
        "            A code indicating the reason for termination:\n",
        "        \n",
        "            - 0 : Points no longer changing.\n",
        "            - 1 : Cooled to final temperature.\n",
        "            - 2 : Maximum function evaluations reached.\n",
        "            - 3 : Maximum cooling iterations reached.\n",
        "            - 4 : Maximum accepted query locations reached.\n",
        "            - 5 : Final point not the minimum amongst encountered points.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        basinhopping : another (more performant) global optimizer\n",
        "        brute : brute-force global optimizer\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Simulated annealing is a random algorithm which uses no derivative\n",
        "        information from the function being optimized. In practice it has\n",
        "        been more useful in discrete optimization than continuous\n",
        "        optimization, as there are usually better algorithms for continuous\n",
        "        optimization problems.\n",
        "        \n",
        "        Some experimentation by trying the different temperature\n",
        "        schedules and altering their parameters is likely required to\n",
        "        obtain good performance.\n",
        "        \n",
        "        The randomness in the algorithm comes from random sampling in numpy.\n",
        "        To obtain the same results you can call `numpy.random.seed` with the\n",
        "        same seed immediately before calling `anneal`.\n",
        "        \n",
        "        We give a brief description of how the three temperature schedules\n",
        "        generate new points and vary their temperature.  Temperatures are\n",
        "        only updated with iterations in the outer loop.  The inner loop is\n",
        "        over loop over ``xrange(dwell)``, and new points are generated for\n",
        "        every iteration in the inner loop.  Whether the proposed new points\n",
        "        are accepted is probabilistic.\n",
        "        \n",
        "        For readability, let ``d`` denote the dimension of the inputs to func.\n",
        "        Also, let ``x_old`` denote the previous state, and ``k`` denote the\n",
        "        iteration number of the outer loop.  All other variables not\n",
        "        defined below are input variables to `anneal` itself.\n",
        "        \n",
        "        In the 'fast' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(0, 1, size = d)\n",
        "            y = sgn(u - 0.5) * T * ((1 + 1/T)**abs(2*u - 1) - 1.0)\n",
        "        \n",
        "            xc = y * (upper - lower)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            c = n * exp(-n * quench)\n",
        "            T_new = T0 * exp(-c * k**quench)\n",
        "        \n",
        "        In the 'cauchy' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(-pi/2, pi/2, size=d)\n",
        "            xc = learn_rate * T * tan(u)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            T_new = T0 / (1 + k)\n",
        "        \n",
        "        In the 'boltzmann' schedule the updates are::\n",
        "        \n",
        "            std = minimum(sqrt(T) * ones(d), (upper - lower) / (3*learn_rate))\n",
        "            y ~ Normal(0, std, size = d)\n",
        "            x_new = x_old + learn_rate * y\n",
        "        \n",
        "            T_new = T0 / log(1 + k)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        [1] P. J. M. van Laarhoven and E. H. L. Aarts, \"Simulated Annealing: Theory\n",
        "            and Applications\", Kluwer Academic Publishers, 1987.\n",
        "        \n",
        "        [2] W.H. Press et al., \"Numerical Recipies: The Art of Scientific Computing\",\n",
        "            Cambridge U. Press, 1987.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        *Example 1.* We illustrate the use of `anneal` to seek the global minimum\n",
        "        of a function of two variables that is equal to the sum of a positive-\n",
        "        definite quadratic and two deep \"Gaussian-shaped\" craters.  Specifically,\n",
        "        define the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``, ``params``, and the functions are\n",
        "        as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        >>> x0 = np.array([2., 2.])     # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(555)   # Seeded to allow replication.\n",
        "        >>> res = optimize.anneal(f, x0, args=params, schedule='boltzmann',\n",
        "                                  full_output=True, maxiter=500, lower=-10,\n",
        "                                  upper=10, dwell=250, disp=True)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res[0]  # obtained minimum\n",
        "        array([-1.03914194,  1.81330654])\n",
        "        >>> res[1]  # function value at minimum\n",
        "        -3.3817...\n",
        "        \n",
        "        So this run settled on the point [-1.039, 1.813] with a minimum function\n",
        "        value of about -3.382.  The final temperature was about 212. The run used\n",
        "        125301 function evaluations, 501 iterations (including the initial guess as\n",
        "        a iteration), and accepted 61162 points. The status flag of 3 also\n",
        "        indicates that `maxiter` was reached.\n",
        "        \n",
        "        This problem's true global minimum lies near the point [-1.057, 1.808]\n",
        "        and has a value of about -3.409.  So these `anneal` results are pretty\n",
        "        good and could be used as the starting guess in a local optimizer to\n",
        "        seek a more exact local minimum.\n",
        "        \n",
        "        *Example 2.* To minimize the same objective function using\n",
        "        the `minimize` approach, we need to (a) convert the options to an\n",
        "        \"options dictionary\" using the keys prescribed for this method,\n",
        "        (b) call the `minimize` function with the name of the method (which\n",
        "        in this case is 'Anneal'), and (c) take account of the fact that\n",
        "        the returned value will be a `Result` object (`i.e.`, a dictionary,\n",
        "        as defined in `optimize.py`).\n",
        "        \n",
        "        All of the allowable options for 'Anneal' when using the `minimize`\n",
        "        approach are listed in the ``myopts`` dictionary given below, although\n",
        "        in practice only the non-default values would be needed.  Some of their\n",
        "        names differ from those used in the `anneal` approach.  We can proceed\n",
        "        as follows:\n",
        "        \n",
        "        >>> myopts = {\n",
        "                'schedule'     : 'boltzmann',   # Non-default value.\n",
        "                'maxfev'       : None,  # Default, formerly `maxeval`.\n",
        "                'maxiter'      : 500,   # Non-default value.\n",
        "                'maxaccept'    : None,  # Default value.\n",
        "                'ftol'         : 1e-6,  # Default, formerly `feps`.\n",
        "                'T0'           : None,  # Default value.\n",
        "                'Tf'           : 1e-12, # Default value.\n",
        "                'boltzmann'    : 1.0,   # Default value.\n",
        "                'learn_rate'   : 0.5,   # Default value.\n",
        "                'quench'       : 1.0,   # Default value.\n",
        "                'm'            : 1.0,   # Default value.\n",
        "                'n'            : 1.0,   # Default value.\n",
        "                'lower'        : -10,   # Non-default value.\n",
        "                'upper'        : +10,   # Non-default value.\n",
        "                'dwell'        : 250,   # Non-default value.\n",
        "                'disp'         : True   # Default value.\n",
        "                }\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(777)  # Seeded to allow replication.\n",
        "        >>> res2 = optimize.minimize(f, x0, args=params, method='Anneal',\n",
        "                                     options=myopts)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res2\n",
        "          status: 3\n",
        "         success: False\n",
        "          accept: 61742\n",
        "            nfev: 125301\n",
        "               T: 214.20624873839623\n",
        "             fun: -3.4084065576676053\n",
        "               x: array([-1.05757366,  1.8071427 ])\n",
        "         message: 'Maximum cooling iterations reached'\n",
        "         nit: 501\n",
        "    \n",
        "    approx_fprime(xk, f, epsilon, *args)\n",
        "        Finite-difference approximation of the gradient of a scalar function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        xk : array_like\n",
        "            The coordinate vector at which to determine the gradient of `f`.\n",
        "        f : callable\n",
        "            The function of which to determine the gradient (partial derivatives).\n",
        "            Should take `xk` as first argument, other arguments to `f` can be\n",
        "            supplied in ``*args``.  Should return a scalar, the value of the\n",
        "            function at `xk`.\n",
        "        epsilon : array_like\n",
        "            Increment to `xk` to use for determining the function gradient.\n",
        "            If a scalar, uses the same finite difference delta for all partial\n",
        "            derivatives.  If an array, should contain one value per element of\n",
        "            `xk`.\n",
        "        \\*args : args, optional\n",
        "            Any other arguments that are to be passed to `f`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        grad : ndarray\n",
        "            The partial derivatives of `f` to `xk`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        check_grad : Check correctness of gradient function against approx_fprime.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The function gradient is determined by the forward finite difference\n",
        "        formula::\n",
        "        \n",
        "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
        "            f'[i] = ---------------------------------\n",
        "                                epsilon[i]\n",
        "        \n",
        "        The main use of `approx_fprime` is in scalar function optimizers like\n",
        "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c0, c1):\n",
        "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
        "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
        "        \n",
        "        >>> x = np.ones(2)\n",
        "        >>> c0, c1 = (1, 200)\n",
        "        >>> eps = np.sqrt(np.finfo(np.float).eps)\n",
        "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
        "        array([   2.        ,  400.00004198])\n",
        "    \n",
        "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None)\n",
        "        Find the global minimum of a function using the basin-hopping algorithm\n",
        "        \n",
        "        .. versionadded:: 0.12.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            Function to be optimized.  ``args`` can be passed as an optional item\n",
        "            in the dict ``minimizer_kwargs``\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        niter : integer, optional\n",
        "            The number of basin hopping iterations\n",
        "        T : float, optional\n",
        "            The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
        "            \"temperatures\" mean that larger jumps in function value will be\n",
        "            accepted.  For best results ``T`` should be comparable to the\n",
        "            separation\n",
        "            (in function value) between local minima.\n",
        "        stepsize : float, optional\n",
        "            initial step size for use in the random displacement.\n",
        "        minimizer_kwargs : dict, optional\n",
        "            Extra keyword arguments to be passed to the minimizer\n",
        "            ``scipy.optimize.minimize()`` Some important options could be:\n",
        "                method : str\n",
        "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
        "                args : tuple\n",
        "                    Extra arguments passed to the objective function (``func``) and\n",
        "                    its derivatives (Jacobian, Hessian).\n",
        "        \n",
        "        take_step : callable ``take_step(x)``, optional\n",
        "            Replace the default step taking routine with this routine.  The default\n",
        "            step taking routine is a random displacement of the coordinates, but\n",
        "            other step taking algorithms may be better for some systems.\n",
        "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
        "            If this attribute exists, then ``basinhopping`` will adjust\n",
        "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
        "            search.\n",
        "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
        "            Define a test which will be used to judge whether or not to accept the\n",
        "            step.  This will be used in addition to the Metropolis test based on\n",
        "            \"temperature\" ``T``.  The acceptable return values are True,\n",
        "            False, or ``\"force accept\"``.  If the latter, then this will\n",
        "            override any other tests in order to accept the step.  This can be\n",
        "            used, for example, to forcefully escape from a local minimum that\n",
        "            ``basinhopping`` is trapped in.\n",
        "        callback : callable, ``callback(x, f, accept)``, optional\n",
        "            A callback function which will be called for all minimum found.  ``x``\n",
        "            and ``f`` are the coordinates and function value of the trial minima,\n",
        "            and ``accept`` is whether or not that minima was accepted.  This can be\n",
        "            used, for example, to save the lowest N minima found.  Also,\n",
        "            ``callback`` can be used to specify a user defined stop criterion by\n",
        "            optionally returning True to stop the ``basinhopping`` routine.\n",
        "        interval : integer, optional\n",
        "            interval for how often to update the ``stepsize``\n",
        "        disp : bool, optional\n",
        "            Set to True to print status messages\n",
        "        niter_success : integer, optional\n",
        "            Stop the run if the global minimum candidate remains the same for this\n",
        "            number of iterations.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.  Important\n",
        "            attributes are: ``x`` the solution array, ``fun`` the value of the\n",
        "            function at the solution, and ``message`` which describes the cause of\n",
        "            the termination. See `Result` for a description of other attributes.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize :\n",
        "            The local minimization function called once for each basinhopping step.\n",
        "            ``minimizer_kwargs`` is passed to this routine.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
        "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
        "        [4]_.  The algorithm in its current form was described by David Wales and\n",
        "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
        "        \n",
        "        The algorithm is iterative with each cycle composed of the following\n",
        "        features\n",
        "        \n",
        "        1) random perturbation of the coordinates\n",
        "        \n",
        "        2) local minimization\n",
        "        \n",
        "        3) accept or reject the new coordinates based on the minimized function\n",
        "           value\n",
        "        \n",
        "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
        "        Carlo algorithms, although there are many other possibilities [3]_.\n",
        "        \n",
        "        This global minimization method has been shown to be extremely efficient\n",
        "        for a wide variety of problems in physics and chemistry.  It is\n",
        "        particularly useful when the function has many minima separated by large\n",
        "        barriers. See the Cambridge Cluster Database\n",
        "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
        "        that have been optimized primarily using basin-hopping.  This database\n",
        "        includes minimization problems exceeding 300 degrees of freedom.\n",
        "        \n",
        "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
        "        a Fortran implementation of basin-hopping.  This implementation has many\n",
        "        different variations of the procedure described above, including more\n",
        "        advanced step taking algorithms and alternate acceptance criterion.\n",
        "        \n",
        "        For stochastic global optimization there is no way to determine if the true\n",
        "        global minimum has actually been found. Instead, as a consistency check,\n",
        "        the algorithm can be run from a number of different random starting points\n",
        "        to ensure the lowest minimum found in each example has converged to the\n",
        "        global minimum.  For this reason ``basinhopping`` will by default simply\n",
        "        run for the number of iterations ``niter`` and return the lowest minimum\n",
        "        found.  It is left to the user to ensure that this is in fact the global\n",
        "        minimum.\n",
        "        \n",
        "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
        "        depends on the problem being solved.  Ideally it should be comparable to\n",
        "        the typical separation between local minima of the function being\n",
        "        optimized.  ``basinhopping`` will, by default, adjust ``stepsize`` to find\n",
        "        an optimal value, but this may take many iterations.  You will get quicker\n",
        "        results if you set a sensible value for ``stepsize``.\n",
        "        \n",
        "        Choosing ``T``: The parameter ``T`` is the temperature used in the\n",
        "        metropolis criterion.  Basinhopping steps are accepted with probability\n",
        "        ``1`` if ``func(xnew) < func(xold)``, or otherwise with probability::\n",
        "        \n",
        "            exp( -(func(xnew) - func(xold)) / T )\n",
        "        \n",
        "        So, for best results, ``T`` should to be comparable to the typical\n",
        "        difference in function value between between local minima\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
        "            Cambridge, UK.\n",
        "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
        "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
        "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
        "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
        "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
        "            1987, 84, 6611.\n",
        "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
        "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following example is a one-dimensional minimization problem,  with many\n",
        "        local minima superimposed on a parabola.\n",
        "        \n",
        "        >>> func = lambda x: cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
        "        >>> x0=[1.]\n",
        "        \n",
        "        Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
        "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
        "        use and how to set up that minimizer.  This parameter will be passed to\n",
        "        ``scipy.optimize.minimize()``.\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
        "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
        "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
        "        \n",
        "        Next consider a two-dimensional minimization problem. Also, this time we\n",
        "        will use gradient information to significantly speed up the search.\n",
        "        \n",
        "        >>> def func2d(x):\n",
        "        ...     f = cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
        "        ...                                                         0.2) * x[0]\n",
        "        ...     df = np.zeros(2)\n",
        "        ...     df[0] = -14.5 * sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
        "        ...     df[1] = 2. * x[1] + 0.2\n",
        "        ...     return f, df\n",
        "        \n",
        "        We'll also use a different local minimization algorithm.  Also we must tell\n",
        "        the minimizer that our function returns both energy and gradient (jacobian)\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
        "        >>> x0 = [1.0, 1.0]\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Here is an example using a custom step taking routine.  Imagine you want\n",
        "        the first coordinate to take larger steps then the rest of the coordinates.\n",
        "        This can be implemented like so:\n",
        "        \n",
        "        >>> class MyTakeStep(object):\n",
        "        ...    def __init__(self, stepsize=0.5):\n",
        "        ...        self.stepsize = stepsize\n",
        "        ...    def __call__(self, x):\n",
        "        ...        s = self.stepsize\n",
        "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
        "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
        "        ...        return x\n",
        "        \n",
        "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
        "        of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
        "        before\n",
        "        \n",
        "        >>> mytakestep = MyTakeStep()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200, take_step=mytakestep)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Now let's do an example using a custom callback function which prints the\n",
        "        value of every minimum found\n",
        "        \n",
        "        >>> def print_fun(x, f, accepted):\n",
        "        ...         print(\"at minima %.4f accepted %d\" % (f, int(accepted)))\n",
        "        \n",
        "        We'll run it for only 10 basinhopping steps this time.\n",
        "        \n",
        "        >>> np.random.seed(1)\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, callback=print_fun)\n",
        "        at minima 0.4159 accepted 1\n",
        "        at minima -0.9073 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 2.2945 accepted 0\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        \n",
        "        \n",
        "        The minima at -1.0109 is actually the global minimum, found already on the\n",
        "        8th iteration.\n",
        "        \n",
        "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
        "        \n",
        "        >>> class MyBounds(object):\n",
        "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
        "        ...         self.xmax = np.array(xmax)\n",
        "        ...         self.xmin = np.array(xmin)\n",
        "        ...     def __call__(self, **kwargs):\n",
        "        ...         x = kwargs[\"x_new\"]\n",
        "        ...         tmax = bool(np.all(x <= self.xmax))\n",
        "        ...         tmin = bool(np.all(x >= self.xmin))\n",
        "        ...         return tmax and tmin\n",
        "        \n",
        "        >>> mybounds = MyBounds()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, accept_test=mybounds)\n",
        "    \n",
        "    bisect(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of a function within an interval.\n",
        "        \n",
        "        Basic bisection routine to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. `f(a)` and `f(b)` can not have the same signs.\n",
        "        Slow but sure.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  `f` must be continuous, and\n",
        "            f(a) and f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within `xtol` of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in `maxiter` iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
        "            a `RootResults` object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        fsolve : n-dimensional root-finding\n",
        "    \n",
        "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
        "        Bracket the minimum of the function.\n",
        "        \n",
        "        Given a function and distinct initial points, search in the\n",
        "        downhill direction (as defined by the initital points) and return\n",
        "        new points xa, xb, xc that bracket the minimum of the function\n",
        "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
        "        solution will satisfy xa<=x<=xb\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to minimize.\n",
        "        xa, xb : float, optional\n",
        "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
        "        args : tuple, optional\n",
        "            Additional arguments (if present), passed to `func`.\n",
        "        grow_limit : float, optional\n",
        "            Maximum grow limit.  Defaults to 110.0\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Defaults to 1000.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xa, xb, xc : float\n",
        "            Bracket.\n",
        "        fa, fb, fc : float\n",
        "            Objective function values in bracket.\n",
        "        funcalls : int\n",
        "            Number of function evaluations made.\n",
        "    \n",
        "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
        "        Given a function of one-variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        args\n",
        "            Additional arguments (if present).\n",
        "        brack : tuple\n",
        "            Triple (a,b,c) where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,c)\n",
        "            then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that the obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            Stop if between iteration change is less than `tol`.\n",
        "        full_output : bool\n",
        "            If True, return all output args (xmin, fval, iter,\n",
        "            funcalls).\n",
        "        maxiter : int\n",
        "            Maximum number of iterations in solution.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            Optimum point.\n",
        "        fval : float\n",
        "            Optimum value.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of objective function evaluations made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Brent' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses inverse parabolic interpolation when possible to speed up\n",
        "        convergence of golden section method.\n",
        "    \n",
        "    brenth(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of f in [a,b].\n",
        "        \n",
        "        A variation on the classic Brent routine to find a zero of the function f\n",
        "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
        "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
        "        f(a) and f(b) can not have the same signs. Generally on a par with the\n",
        "        brent routine, but not as heavily tested.  It is a safe version of the\n",
        "        secant method that uses hyperbolic extrapolation. The version here is by\n",
        "        Chuck Harris.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        fmin, fmin_powell, fmin_cg,\n",
        "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
        "        \n",
        "        leastsq : nonlinear least squares minimizer\n",
        "        \n",
        "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
        "        \n",
        "        anneal, brute : global optimizers\n",
        "        \n",
        "        fminbound, brent, golden, bracket : local scalar minimizers\n",
        "        \n",
        "        fsolve : n-dimensional root-finding\n",
        "        \n",
        "        brentq, brenth, ridder, bisect, newton : one-dimensional root-finding\n",
        "        \n",
        "        fixed_point : scalar fixed-point finder\n",
        "    \n",
        "    brentq(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in given interval.\n",
        "        \n",
        "        Return float, a zero of `f` between `a` and `b`.  `f` must be a continuous\n",
        "        function, and [a,b] must be a sign changing interval.\n",
        "        \n",
        "        Description:\n",
        "        Uses the classic Brent (1973) method to find a zero of the function `f` on\n",
        "        the sign changing interval [a , b].  Generally considered the best of the\n",
        "        rootfinding routines here.  It is a safe version of the secant method that\n",
        "        uses inverse quadratic extrapolation.  Brent's method combines root\n",
        "        bracketing, interval bisection, and inverse quadratic interpolation.  It is\n",
        "        sometimes known as the van Wijngaarden-Deker-Brent method.  Brent (1973)\n",
        "        claims convergence is guaranteed for functions computable within [a,b].\n",
        "        \n",
        "        [Brent1973]_ provides the classic description of the algorithm.  Another\n",
        "        description can be found in a recent edition of Numerical Recipes, including\n",
        "        [PressEtal1992]_.  Another description is at\n",
        "        http://mathworld.wolfram.com/BrentsMethod.html.  It should be easy to\n",
        "        understand the algorithm just by reading our code.  Our code diverges a bit\n",
        "        from standard presentations: we choose a different formula for the\n",
        "        extrapolation step.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        multivariate local optimizers\n",
        "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
        "        nonlinear least squares minimizer\n",
        "          `leastsq`\n",
        "        constrained multivariate optimizers\n",
        "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
        "        global optimizers\n",
        "          `anneal`, `basinhopping`, `brute`\n",
        "        local scalar minimizers\n",
        "          `fminbound`, `brent`, `golden`, `bracket`\n",
        "        n-dimensional root-finding\n",
        "          `fsolve`\n",
        "        one-dimensional root-finding\n",
        "          `brentq`, `brenth`, `ridder`, `bisect`, `newton`\n",
        "        scalar fixed-point finder\n",
        "          `fixed_point`\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Brent1973]\n",
        "           Brent, R. P.,\n",
        "           *Algorithms for Minimization Without Derivatives*.\n",
        "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
        "        \n",
        "        .. [PressEtal1992]\n",
        "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
        "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
        "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
        "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
        "    \n",
        "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \\\"Broyden's good method\\\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
        "        \n",
        "        which corresponds to Broyden's first Jacobian update\n",
        "        \n",
        "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \\\"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \"Broyden's bad method\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
        "        \n",
        "        corresponding to Broyden's second method.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin>, disp=False)\n",
        "        Minimize a function over a given range by brute force.\n",
        "        \n",
        "        Uses the \"brute force\" method, i.e. computes the function's value\n",
        "        at each point of a multidimensional grid of points, to find the global\n",
        "        minimum of the function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized. Must be in the\n",
        "            form ``f(x, *args)``, where ``x`` is the argument in\n",
        "            the form of a 1-D array and ``args`` is a tuple of any\n",
        "            additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        ranges : tuple\n",
        "            Each component of the `ranges` tuple must be either a\n",
        "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
        "            The program uses these to create the grid of points on which\n",
        "            the objective function will be computed. See `Note 2` for\n",
        "            more detail.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        Ns : int, optional\n",
        "            Number of grid points along the axes, if not otherwise\n",
        "            specified. See `Note2`.\n",
        "        full_output : bool, optional\n",
        "            If True, return the evaluation grid and the objective function's\n",
        "            values on it.\n",
        "        finish : callable, optional\n",
        "            An optimization function that is called with the result of brute force\n",
        "            minimization as initial guess.  `finish` should take the initial guess\n",
        "            as positional argument, and take `args`, `full_output` and `disp`\n",
        "            as keyword arguments.  Use None if no \"polishing\" function is to be\n",
        "            used.  See Notes for more details.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : ndarray\n",
        "            A 1-D array containing the coordinates of a point at which the\n",
        "            objective function had its minimum value. (See `Note 1` for\n",
        "            which point is returned.)\n",
        "        fval : float\n",
        "            Function value at the point `x0`.\n",
        "        grid : tuple\n",
        "            Representation of the evaluation grid.  It has the same\n",
        "            length as `x0`. (Returned when `full_output` is True.)\n",
        "        Jout : ndarray\n",
        "            Function values at each point of the evaluation\n",
        "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
        "            when `full_output` is True.)\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        anneal : Another approach to seeking the global minimum of\n",
        "        multivariate, multimodal functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
        "        of the objective function occurs.  If `finish` is None, that is the\n",
        "        point returned.  When the global minimum occurs within (or not very far\n",
        "        outside) the grid's boundaries, and the grid is fine enough, that\n",
        "        point will be in the neighborhood of the gobal minimum.\n",
        "        \n",
        "        However, users often employ some other optimization program to\n",
        "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
        "        (local) minimum near `brute's` best gridpoint.\n",
        "        The `brute` function's `finish` option provides a convenient way to do\n",
        "        that.  Any polishing program used must take `brute's` output as its\n",
        "        initial guess as a positional argument, and take `brute's` input values\n",
        "        for `args` and `full_output` as keyword arguments, otherwise an error\n",
        "        will be raised.\n",
        "        \n",
        "        `brute` assumes that the `finish` function returns a tuple in the form:\n",
        "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing value\n",
        "        of the argument, ``Jmin`` is the minimum value of the objective function,\n",
        "        \"...\" may be some other returned values (which are not used by `brute`),\n",
        "        and ``statuscode`` is the status code of the `finish` program.\n",
        "        \n",
        "        Note that when `finish` is not None, the values returned are those\n",
        "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
        "        while `brute` confines its search to the input grid points,\n",
        "        the `finish` program's results usually will not coincide with any\n",
        "        gridpoint, and may fall outside the grid's boundary.\n",
        "        \n",
        "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
        "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
        "        Each component of the `ranges` tuple can be either a slice object or a\n",
        "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
        "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
        "        range, `brute` internally converts it to a slice object that interpolates\n",
        "        `Ns` points from its low-value to its high-value, inclusive.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        We illustrate the use of `brute` to seek the global minimum of a function\n",
        "        of two variables that is given as the sum of a positive-definite\n",
        "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
        "        the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
        "        are as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        Thus, the objective function may have local minima near the minimum\n",
        "        of each of the three functions of which it is composed.  To\n",
        "        use `fmin` to polish its gridpoint result, we may then continue as\n",
        "        follows:\n",
        "        \n",
        "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
        "        >>> from scipy import optimize\n",
        "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
        "                                      finish=optimize.fmin)\n",
        "        >>> resbrute[0]  # global minimum\n",
        "        array([-1.05665192,  1.80834843])\n",
        "        >>> resbrute[1]  # function value at global minimum\n",
        "        -3.4085818767\n",
        "        \n",
        "        Note that if `finish` had been set to None, we would have gotten the\n",
        "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
        "    \n",
        "    check_grad(func, grad, x0, *args)\n",
        "        Check the correctness of a gradient function by comparing it against a\n",
        "        (forward) finite-difference approximation of the gradient.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x0,*args)\n",
        "            Function whose derivative is to be checked.\n",
        "        grad : callable grad(x0, *args)\n",
        "            Gradient of `func`.\n",
        "        x0 : ndarray\n",
        "            Points to check `grad` against forward difference approximation of grad\n",
        "            using `func`.\n",
        "        args : \\*args, optional\n",
        "            Extra arguments passed to `func` and `grad`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        err : float\n",
        "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
        "            difference between ``grad(x0, *args)`` and the finite difference\n",
        "            approximation of `grad` using func at the points `x0`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        approx_fprime\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The step size used for the finite difference approximation is\n",
        "        `sqrt(numpy.finfo(float).eps)`, which is approximately 1.49e-08.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> def func(x): return x[0]**2 - 0.5 * x[1]**3\n",
        "        >>> def grad(x): return [2 * x[0], -1.5 * x[1]**2]\n",
        "        >>> check_grad(func, grad, [1.5, -1.5])\n",
        "        2.9802322387695312e-08\n",
        "    \n",
        "    curve_fit(f, xdata, ydata, p0=None, sigma=None, **kw)\n",
        "        Use non-linear least squares to fit a function, f, to data.\n",
        "        \n",
        "        Assumes ``ydata = f(xdata, *params) + eps``\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable\n",
        "            The model function, f(x, ...).  It must take the independent\n",
        "            variable as the first argument and the parameters to fit as\n",
        "            separate remaining arguments.\n",
        "        xdata : An N-length sequence or an (k,N)-shaped array\n",
        "            for functions with k predictors.\n",
        "            The independent variable where the data is measured.\n",
        "        ydata : N-length sequence\n",
        "            The dependent data --- nominally f(xdata, ...)\n",
        "        p0 : None, scalar, or M-length sequence\n",
        "            Initial guess for the parameters.  If None, then the initial\n",
        "            values will all be 1 (if the number of parameters for the function\n",
        "            can be determined using introspection, otherwise a ValueError\n",
        "            is raised).\n",
        "        sigma : None or N-length sequence\n",
        "            If not None, this vector will be used as relative weights in the\n",
        "            least-squares problem.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        popt : array\n",
        "            Optimal values for the parameters so that the sum of the squared error\n",
        "            of ``f(xdata, *popt) - ydata`` is minimized\n",
        "        pcov : 2d array\n",
        "            The estimated covariance of popt.  The diagonals provide the variance\n",
        "            of the parameter estimate.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        leastsq\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The algorithm uses the Levenberg-Marquardt algorithm through `leastsq`.\n",
        "        Additional keyword arguments are passed directly to that algorithm.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> import numpy as np\n",
        "        >>> from scipy.optimize import curve_fit\n",
        "        >>> def func(x, a, b, c):\n",
        "        ...     return a*np.exp(-b*x) + c\n",
        "        \n",
        "        >>> x = np.linspace(0,4,50)\n",
        "        >>> y = func(x, 2.5, 1.3, 0.5)\n",
        "        >>> yn = y + 0.2*np.random.normal(size=len(x))\n",
        "        \n",
        "        >>> popt, pcov = curve_fit(func, x, yn)\n",
        "    \n",
        "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
        "        \n",
        "        The Jacobian approximation is derived from previous iterations, by\n",
        "        retaining only the diagonal of Broyden matrices.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
        "        \n",
        "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial Jacobian approximation is (-1/alpha).\n",
        "        alphamax : float, optional\n",
        "            The entries of the diagonal Jacobian are kept in the range\n",
        "            ``[alpha, alphamax]``.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500)\n",
        "        Find a fixed point of the function.\n",
        "        \n",
        "        Given a function of one or more variables and a starting point, find a\n",
        "        fixed-point of the function: i.e. where ``func(x0) == x0``.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            Function to evaluate.\n",
        "        x0 : array_like\n",
        "            Fixed point of function.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to `func`.\n",
        "        xtol : float, optional\n",
        "            Convergence tolerance, defaults to 1e-08.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations, defaults to 500.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses Steffensen's Method using Aitken's ``Del^2`` convergence acceleration.\n",
        "        See Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c1, c2):\n",
        "        ....    return np.sqrt(c1/(x+c2))\n",
        "        >>> c1 = np.array([10,12.])\n",
        "        >>> c2 = np.array([3, 5.])\n",
        "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
        "        array([ 1.4920333 ,  1.37228132])\n",
        "    \n",
        "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the downhill simplex algorithm.\n",
        "        \n",
        "        This algorithm only uses function values, not derivatives or second\n",
        "        derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            The objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        xtol : float, optional\n",
        "            Relative error in xopt acceptable for convergence.\n",
        "        ftol : number, optional\n",
        "            Relative error in func(xopt) acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : number, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            Set to True if fopt and warnflag outputs are desired.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        retall : bool, optional\n",
        "            Set to True to return list of solutions at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter that minimizes function.\n",
        "        fopt : float\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        iter : int\n",
        "            Number of iterations performed.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            1 : Maximum number of function evaluations made.\n",
        "            2 : Maximum number of iterations reached.\n",
        "        allvecs : list\n",
        "            Solution at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Nelder-Mead' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "        one or more variables.\n",
        "        \n",
        "        This algorithm has a long history of successful use in applications.\n",
        "        But it will usually be slower than an algorithm that uses first or\n",
        "        second derivative information. In practice it can have poor\n",
        "        performance in high-dimensional problems and is not robust to\n",
        "        minimizing complicated functions. Additionally, there currently is no\n",
        "        complete theory describing when the algorithm will successfully\n",
        "        converge to the minimum, or how fast it will if it does.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "               minimization\", The Computer Journal, 7, pp. 308-313\n",
        "        \n",
        "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "               Harlow, UK, pp. 191-208.\n",
        "    \n",
        "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the BFGS algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable f'(x,*args), optional\n",
        "            Gradient of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f and fprime.\n",
        "        gtol : float, optional\n",
        "            Gradient norm must be less than gtol before succesful termination.\n",
        "        norm : float, optional\n",
        "            Order of norm (Inf is max, -Inf is min)\n",
        "        epsilon : int or ndarray, optional\n",
        "            If fprime is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function to call after each\n",
        "            iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
        "            in addition to xopt.\n",
        "        disp : bool, optional\n",
        "            Print convergence message if True.\n",
        "        retall : bool, optional\n",
        "            Return a list of results at each iteration if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
        "        fopt : float\n",
        "            Minimum value.\n",
        "        gopt : ndarray\n",
        "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
        "        Bopt : ndarray\n",
        "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
        "        func_calls : int\n",
        "            Number of function_calls made.\n",
        "        grad_calls : int\n",
        "            Number of gradient calls made.\n",
        "        warnflag : integer\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "            2 : Gradient and/or function calls not changing.\n",
        "        allvecs  :  list\n",
        "            Results at each iteration.  Only returned if retall is True.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'BFGS' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Optimize the function, f, whose gradient is given by fprime\n",
        "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
        "        and Shanno (BFGS)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
        "    \n",
        "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable, ``f(x, *args)``\n",
        "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
        "            the variables that are to be changed in the search for a minimum, and\n",
        "            `args` are the other (fixed) parameters of `f`.\n",
        "        x0 : ndarray\n",
        "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
        "            It must be a 1-D array of values.\n",
        "        fprime : callable, ``fprime(x, *args)``, optional\n",
        "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
        "            are as described above for `f`. The returned value must be a 1-D array.\n",
        "            Defaults to None, in which case the gradient is approximated\n",
        "            numerically (see `epsilon`, below).\n",
        "        args : tuple, optional\n",
        "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
        "            additional fixed parameters are needed to completely specify the\n",
        "            functions `f` and `fprime`.\n",
        "        gtol : float, optional\n",
        "            Stop when the norm of the gradient is less than `gtol`.\n",
        "        norm : float, optional\n",
        "            Order to use for the norm of the gradient\n",
        "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
        "        epsilon : float or ndarray, optional\n",
        "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
        "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
        "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
        "            1.5e-8.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
        "        full_output : bool, optional\n",
        "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
        "            addition to `xopt`.  See the Returns section below for additional\n",
        "            information on optional return values.\n",
        "        disp : bool, optional\n",
        "            If True, return a convergence message, followed by `xopt`.\n",
        "        retall : bool, optional\n",
        "            If True, add to the returned values the results of each iteration.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each iteration.\n",
        "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float, optional\n",
        "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
        "        func_calls : int, optional\n",
        "            The number of function_calls made.  Only returned if `full_output`\n",
        "            is True.\n",
        "        grad_calls : int, optional\n",
        "            The number of gradient calls made. Only returned if `full_output` is\n",
        "            True.\n",
        "        warnflag : int, optional\n",
        "            Integer value with warning status, only returned if `full_output` is\n",
        "            True.\n",
        "        \n",
        "            0 : Success.\n",
        "        \n",
        "            1 : The maximum number of iterations was exceeded.\n",
        "        \n",
        "            2 : Gradient and/or function calls were not changing.  May indicate\n",
        "                that precision was lost, i.e., the routine did not converge.\n",
        "        \n",
        "        allvecs : list of ndarray, optional\n",
        "            List of arrays, containing the results at each iteration.\n",
        "            Only returned if `retall` is True.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize : common interface to all `scipy.optimize` algorithms for\n",
        "                   unconstrained and constrained minimization of multivariate\n",
        "                   functions.  It provides an alternative way to call\n",
        "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
        "        [1]_.\n",
        "        \n",
        "        Conjugate gradient methods tend to work better when:\n",
        "        \n",
        "        1. `f` has a unique global minimizing point, and no local minima or\n",
        "           other stationary points,\n",
        "        2. `f` is, at least locally, reasonably well approximated by a\n",
        "           quadratic function of the variables,\n",
        "        3. `f` is continuous and has a continuous gradient,\n",
        "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
        "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
        "           minimizing point, `xopt`.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Example 1: seek the minimum value of the expression\n",
        "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
        "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
        "        \n",
        "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
        "        >>> def f(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
        "        >>> def gradf(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
        "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
        "        ...     return np.asarray((gu, gv))\n",
        "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
        "        >>> print 'res1 = ', res1\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 1.617021\n",
        "                 Iterations: 2\n",
        "                 Function evaluations: 5\n",
        "                 Gradient evaluations: 5\n",
        "        res1 =  [-1.80851064 -0.25531915]\n",
        "        \n",
        "        Example 2: solve the same problem using the `minimize` function.\n",
        "        (This `myopts` dictionary shows all of the available options,\n",
        "        although in practice only non-default values would be needed.\n",
        "        The returned value will be a dictionary.)\n",
        "        \n",
        "        >>> opts = {'maxiter' : None,    # default value.\n",
        "        ...         'disp' : True,    # non-default value.\n",
        "        ...         'gtol' : 1e-5,    # default value.\n",
        "        ...         'norm' : np.inf,  # default value.\n",
        "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
        "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
        "        ...                          method='CG', options=opts)\n",
        "        Optimization terminated successfully.\n",
        "                Current function value: 1.617021\n",
        "                Iterations: 2\n",
        "                Function evaluations: 5\n",
        "                Gradient evaluations: 5\n",
        "        >>> res2.x  # minimum found\n",
        "        array([-1.80851064 -0.25531915])\n",
        "    \n",
        "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, iprint=1, maxfun=1000, disp=None)\n",
        "        Minimize a function using the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
        "        implentation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            Function to minimize. In the form func(x, \\*args).\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        cons : sequence\n",
        "            Constraint functions; must all be ``>=0`` (a single function\n",
        "            if only 1 constraint). Each function takes the parameters `x`\n",
        "            as its first argument.\n",
        "        args : tuple\n",
        "            Extra arguments to pass to function.\n",
        "        consargs : tuple\n",
        "            Extra arguments to pass to constraint functions (default of None means\n",
        "            use same extra arguments as those passed to func).\n",
        "            Use ``()`` for no extra arguments.\n",
        "        rhobeg :\n",
        "            Reasonable initial changes to the variables.\n",
        "        rhoend :\n",
        "            Final accuracy in the optimization (not precisely guaranteed). This\n",
        "            is a lower bound on the size of the trust region.\n",
        "        iprint : {0, 1, 2, 3}\n",
        "            Controls the frequency of output; 0 implies no output.  Deprecated.\n",
        "        disp : {0, 1, 2, 3}\n",
        "            Over-rides the iprint interface.  Preferred.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The argument that minimises `f`.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'COBYLA' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm is based on linear approximations to the objective\n",
        "        function and each constraint. We briefly describe the algorithm.\n",
        "        \n",
        "        Suppose the function is being minimized over k variables. At the\n",
        "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
        "        an approximate solution x_j, and a radius RHO_j.\n",
        "        (i.e. linear plus a constant) approximations to the objective\n",
        "        function and constraint functions such that their function values\n",
        "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
        "        This gives a linear program to solve (where the linear approximations\n",
        "        of the constraint functions are constrained to be non-negative).\n",
        "        \n",
        "        However the linear approximations are likely only good\n",
        "        approximations near the current simplex, so the linear program is\n",
        "        given the further requirement that the solution, which\n",
        "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
        "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
        "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
        "        like a trust region algorithm.\n",
        "        \n",
        "        Additionally, the linear program may be inconsistent, or the\n",
        "        approximation may give poor improvement. For details about\n",
        "        how these issues are resolved, as well as how the points v_i are\n",
        "        updated, refer to the source code or the references below.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
        "        the objective and constraint functions by linear interpolation.\", in\n",
        "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
        "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
        "        \n",
        "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
        "        calculations\", Acta Numerica 7, 287-336\n",
        "        \n",
        "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
        "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
        "        \n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Minimize the objective function f(x,y) = x*y subject\n",
        "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
        "        \n",
        "            >>> def objective(x):\n",
        "            ...     return x[0]*x[1]\n",
        "            ...\n",
        "            >>> def constr1(x):\n",
        "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
        "            ...\n",
        "            >>> def constr2(x):\n",
        "            ...     return x[1]\n",
        "            ...\n",
        "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
        "        \n",
        "               Normal return from subroutine COBYLA\n",
        "        \n",
        "               NFVALS =   64   F =-5.000000E-01    MAXCV = 1.998401E-14\n",
        "               X =-7.071069E-01   7.071067E-01\n",
        "            array([-0.70710685,  0.70710671])\n",
        "        \n",
        "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
        "    \n",
        "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None)\n",
        "        Minimize a function func using the L-BFGS-B algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Function to minimise.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable fprime(x,*args)\n",
        "            The gradient of `func`.  If None, then `func` returns the function\n",
        "            value and the gradient (``f, g = func(x, *args)``), unless\n",
        "            `approx_grad` is True in which case `func` returns only ``f``.\n",
        "        args : sequence\n",
        "            Arguments to pass to `func` and `fprime`.\n",
        "        approx_grad : bool\n",
        "            Whether to approximate the gradient numerically (in which case\n",
        "            `func` returns only the function value).\n",
        "        bounds : list\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        m : int\n",
        "            The maximum number of variable metric corrections\n",
        "            used to define the limited memory matrix. (The limited memory BFGS\n",
        "            method does not store the full hessian but uses this many terms in an\n",
        "            approximation to it.)\n",
        "        factr : float\n",
        "            The iteration stops when\n",
        "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
        "            where ``eps`` is the machine precision, which is automatically\n",
        "            generated by the code. Typical values for `factr` are: 1e12 for\n",
        "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
        "            high accuracy.\n",
        "        pgtol : float\n",
        "            The iteration will stop when\n",
        "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
        "            where ``pg_i`` is the i-th component of the projected gradient.\n",
        "        epsilon : float\n",
        "            Step size used when `approx_grad` is True, for numerically\n",
        "            calculating the gradient\n",
        "        iprint : int\n",
        "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
        "            ``iprint == 0`` means write messages to stdout; ``iprint > 1`` in\n",
        "            addition means write logging information to a file named\n",
        "            ``iterate.dat`` in the current working directory.\n",
        "        disp : int, optional\n",
        "            If zero, then no output.  If a positive number, then this over-rides\n",
        "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        maxiter : int\n",
        "            Maximum number of iterations.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : array_like\n",
        "            Estimated position of the minimum.\n",
        "        f : float\n",
        "            Value of `func` at the minimum.\n",
        "        d : dict\n",
        "            Information dictionary.\n",
        "        \n",
        "            * d['warnflag'] is\n",
        "        \n",
        "              - 0 if converged,\n",
        "              - 1 if too many function evaluations or too many iterations,\n",
        "              - 2 if stopped for another reason, given in d['task']\n",
        "        \n",
        "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
        "            * d['funcalls'] is the number of function calls made.\n",
        "            * d['nit'] is the number of iterations.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'L-BFGS-B' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        License of L-BFGS-B (FORTRAN code):\n",
        "        \n",
        "        The version included here (in fortran code) is 3.0\n",
        "        (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd,\n",
        "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
        "        condition for use:\n",
        "        \n",
        "        This software is freely available, but we expect that all publications\n",
        "        describing work using this software, or all commercial products using it,\n",
        "        quote at least one of the references given below. This software is released\n",
        "        under the BSD License.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
        "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
        "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
        "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
        "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
        "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
        "          ACM Transactions on Mathematical Software, 38, 1.\n",
        "    \n",
        "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Unconstrained minimization of a function using the Newton-CG method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable ``f(x, *args)``\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable ``f'(x, *args)``\n",
        "            Gradient of f.\n",
        "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
        "            Function which computes the Hessian of f times an\n",
        "            arbitrary vector, p.\n",
        "        fhess : callable ``fhess(x, *args)``, optional\n",
        "            Function to compute the Hessian matrix of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
        "            (the same set of extra arguments is supplied to all of\n",
        "            these functions).\n",
        "        epsilon : float or ndarray, optional\n",
        "            If fhess is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function which is called after\n",
        "            each iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        avextol : float, optional\n",
        "            Convergence is assumed when the average relative error in\n",
        "            the minimizer falls below this amount.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True, return the optional outputs.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence message.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of results at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float\n",
        "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
        "        fcalls : int\n",
        "            Number of function calls made.\n",
        "        gcalls : int\n",
        "            Number of gradient calls made.\n",
        "        hcalls : int\n",
        "            Number of hessian calls made.\n",
        "        warnflag : int\n",
        "            Warnings generated by the algorithm.\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "        allvecs : list\n",
        "            The result at each iteration, if retall is True (see below).\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Newton-CG' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
        "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
        "        nor `fhess_p` is provided, then the hessian product will be\n",
        "        approximated using finite differences on `fprime`. `fhess_p`\n",
        "        must compute the hessian times an arbitrary vector. If it is not\n",
        "        given, finite-differences on `fprime` are used to compute\n",
        "        it.\n",
        "        \n",
        "        Newton-CG methods are also called truncated Newton methods. This\n",
        "        function differs from scipy.optimize.fmin_tnc because\n",
        "        \n",
        "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
        "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
        "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
        "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
        "            or box constrained minimization. (Box constraints give\n",
        "            lower and upper bounds for each variable seperately.)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
        "    \n",
        "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
        "        Minimize a function using modified Powell's method. This method\n",
        "        only uses function values, not derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each\n",
        "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        direc : ndarray, optional\n",
        "            Initial direction set.\n",
        "        xtol : float, optional\n",
        "            Line-search error tolerance.\n",
        "        ftol : float, optional\n",
        "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            If True, fopt, xi, direc, iter, funcalls, and\n",
        "            warnflag are returned.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence messages.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of the solution at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter which minimizes `func`.\n",
        "        fopt : number\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        direc : ndarray\n",
        "            Current direction set.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            Integer warning flag:\n",
        "                1 : Maximum number of function evaluations.\n",
        "                2 : Maximum number of iterations.\n",
        "        allvecs : list\n",
        "            List of solutions at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to unconstrained minimization algorithms for\n",
        "            multivariate functions. See the 'Powell' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a modification of Powell's method to find the minimum of\n",
        "        a function of N variables. Powell's method is a conjugate\n",
        "        direction method.\n",
        "        \n",
        "        The algorithm has two loops. The outer loop\n",
        "        merely iterates over the inner loop. The inner loop minimizes\n",
        "        over each current direction in the direction set. At the end\n",
        "        of the inner loop, if certain conditions are met, the direction\n",
        "        that gave the largest decrease is dropped and replaced with\n",
        "        the difference between the current estiamted x and the estimated\n",
        "        x from the beginning of the inner-loop.\n",
        "        \n",
        "        The technical conditions for replacing the direction of greatest\n",
        "        increase amount to checking that\n",
        "        \n",
        "        1. No further gain can be made along the direction of greatest increase\n",
        "           from that iteration.\n",
        "        2. The direction of greatest increase accounted for a large sufficient\n",
        "           fraction of the decrease in the function value from that iteration of\n",
        "           the inner loop.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
        "        function of several variables without calculating derivatives,\n",
        "        Computer Journal, 7 (2):155-162.\n",
        "        \n",
        "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
        "        Numerical Recipes (any edition), Cambridge University Press\n",
        "    \n",
        "    fmin_slsqp(func, x0, eqcons=[], f_eqcons=None, ieqcons=[], f_ieqcons=None, bounds=[], fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08)\n",
        "        Minimize a function using Sequential Least SQuares Programming\n",
        "        \n",
        "        Python interface function for the SLSQP Optimization subroutine\n",
        "        originally implemented by Dieter Kraft.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        x0 : 1-D ndarray of float\n",
        "            Initial guess for the independent variable(s).\n",
        "        eqcons : list\n",
        "            A list of functions of length n such that\n",
        "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_eqcons : callable f(x,*args)\n",
        "            Returns a 1-D array in which each element must equal 0.0 in a\n",
        "            successfully optimized problem.  If f_eqcons is specified,\n",
        "            eqcons is ignored.\n",
        "        ieqcons : list\n",
        "            A list of functions of length n such that\n",
        "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_ieqcons : callable f(x,*args)\n",
        "            Returns a 1-D ndarray in which each element must be greater or\n",
        "            equal to 0.0 in a successfully optimized problem.  If\n",
        "            f_ieqcons is specified, ieqcons is ignored.\n",
        "        bounds : list\n",
        "            A list of tuples specifying the lower and upper bound\n",
        "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
        "            Infinite values will be interpreted as large floating values.\n",
        "        fprime : callable `f(x,*args)`\n",
        "            A function that evaluates the partial derivatives of func.\n",
        "        fprime_eqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of equality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
        "        fprime_ieqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of inequality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
        "        args : sequence\n",
        "            Additional arguments passed to func and fprime.\n",
        "        iter : int\n",
        "            The maximum number of iterations.\n",
        "        acc : float\n",
        "            Requested accuracy.\n",
        "        iprint : int\n",
        "            The verbosity of fmin_slsqp :\n",
        "        \n",
        "            * iprint <= 0 : Silent operation\n",
        "            * iprint == 1 : Print summary upon completion (default)\n",
        "            * iprint >= 2 : Print status of each iterate and summary\n",
        "        disp : int\n",
        "            Over-rides the iprint interface (preferred).\n",
        "        full_output : bool\n",
        "            If False, return only the minimizer of func (default).\n",
        "            Otherwise, output final objective function and summary\n",
        "            information.\n",
        "        epsilon : float\n",
        "            The step size for finite-difference derivative estimates.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray of float\n",
        "            The final minimizer of func.\n",
        "        fx : ndarray of float, if full_output is true\n",
        "            The final value of the objective function.\n",
        "        its : int, if full_output is true\n",
        "            The number of iterations.\n",
        "        imode : int, if full_output is true\n",
        "            The exit mode from the optimizer (see below).\n",
        "        smode : string, if full_output is true\n",
        "            Message describing the exit mode from the optimizer.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'SLSQP' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Exit modes are defined as follows ::\n",
        "        \n",
        "            -1 : Gradient evaluation required (g & a)\n",
        "             0 : Optimization terminated successfully.\n",
        "             1 : Function evaluation required (f & c)\n",
        "             2 : More equality constraints than independent variables\n",
        "             3 : More than 3*n iterations in LSQ subproblem\n",
        "             4 : Inequality constraints incompatible\n",
        "             5 : Singular matrix E in LSQ subproblem\n",
        "             6 : Singular matrix C in LSQ subproblem\n",
        "             7 : Rank-deficient equality constraint subproblem HFTI\n",
        "             8 : Positive directional derivative for linesearch\n",
        "             9 : Iteration limit exceeded\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
        "    \n",
        "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
        "        Minimize a function with variables subject to bounds, using\n",
        "        gradient information in a truncated Newton algorithm. This\n",
        "        method wraps a C implementation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``func(x, *args)``\n",
        "            Function to minimize.  Must do one of:\n",
        "        \n",
        "            1. Return f and g, where f is the value of the function and g its\n",
        "               gradient (a list of floats).\n",
        "        \n",
        "            2. Return the function value but supply gradient function\n",
        "               seperately as `fprime`.\n",
        "        \n",
        "            3. Return the function value and set ``approx_grad=True``.\n",
        "        \n",
        "            If the function returns None, the minimization\n",
        "            is aborted.\n",
        "        x0 : array_like\n",
        "            Initial estimate of minimum.\n",
        "        fprime : callable ``fprime(x, *args)``\n",
        "            Gradient of `func`. If None, then either `func` must return the\n",
        "            function value and the gradient (``f,g = func(x, *args)``)\n",
        "            or `approx_grad` must be True.\n",
        "        args : tuple\n",
        "            Arguments to pass to function.\n",
        "        approx_grad : bool\n",
        "            If true, approximate the gradient numerically.\n",
        "        bounds : list\n",
        "            (min, max) pairs for each element in x0, defining the\n",
        "            bounds on that parameter. Use None or +/-inf for one of\n",
        "            min or max when there is no bound in that direction.\n",
        "        epsilon : float\n",
        "            Used if approx_grad is True. The stepsize in a finite\n",
        "            difference approximation for fprime.\n",
        "        scale : array_like\n",
        "            Scaling factors to apply to each variable.  If None, the\n",
        "            factors are up-low for interval bounded variables and\n",
        "            1+|x| for the others.  Defaults to None.\n",
        "        offset : array_like\n",
        "            Value to substract from each variable.  If None, the\n",
        "            offsets are (up+low)/2 for interval bounded variables\n",
        "            and x for the others.\n",
        "        messages :\n",
        "            Bit mask used to select messages display during\n",
        "            minimization values defined in the MSGS dict.  Defaults to\n",
        "            MGS_ALL.\n",
        "        disp : int\n",
        "            Integer interface to messages.  0 = no message, 5 = all messages\n",
        "        maxCGit : int\n",
        "            Maximum number of hessian*vector evaluations per main\n",
        "            iteration.  If maxCGit == 0, the direction chosen is\n",
        "            -gradient if maxCGit < 0, maxCGit is set to\n",
        "            max(1,min(50,n/2)).  Defaults to -1.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluation.  if None, maxfun is\n",
        "            set to max(100, 10*len(x0)).  Defaults to None.\n",
        "        eta : float\n",
        "            Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "            Defaults to -1.\n",
        "        stepmx : float\n",
        "            Maximum step for the line search.  May be increased during\n",
        "            call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "        accuracy : float\n",
        "            Relative precision for finite difference calculations.  If\n",
        "            <= machine_precision, set to sqrt(machine_precision).\n",
        "            Defaults to 0.\n",
        "        fmin : float\n",
        "            Minimum function value estimate.  Defaults to 0.\n",
        "        ftol : float\n",
        "            Precision goal for the value of f in the stoping criterion.\n",
        "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "        xtol : float\n",
        "            Precision goal for the value of x in the stopping\n",
        "            criterion (after applying x scaling factors).  If xtol <\n",
        "            0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "            -1.\n",
        "        pgtol : float\n",
        "            Precision goal for the value of the projected gradient in\n",
        "            the stopping criterion (after applying x scaling factors).\n",
        "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
        "            Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "        rescale : float\n",
        "            Scaling factor (in log10) used to trigger f value\n",
        "            rescaling.  If 0, rescale at each iteration.  If a large\n",
        "            value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution.\n",
        "        nfeval : int\n",
        "            The number of function evaluations.\n",
        "        rc : int\n",
        "            Return code as defined in the RCSTRINGS dict.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'TNC' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The underlying algorithm is truncated Newton, also called\n",
        "        Newton Conjugate-Gradient. This method differs from\n",
        "        scipy.optimize.fmin_ncg in that\n",
        "        \n",
        "        1. It wraps a C implementation of the algorithm\n",
        "        2. It allows each variable to be given an upper and lower bound.\n",
        "        \n",
        "        The algorithm incoporates the bound constraints by determining\n",
        "        the descent direction as in an unconstrained truncated Newton,\n",
        "        but never taking a step-size large enough to leave the space\n",
        "        of feasible x's. The algorithm keeps track of a set of\n",
        "        currently active constraints, and ignores them when computing\n",
        "        the minimum allowable step size. (The x's associated with the\n",
        "        active constraint are kept fixed.) If the maximum allowable\n",
        "        step size is zero then a new constraint is added. At the end\n",
        "        of each iteration one of the constraints may be deemed no\n",
        "        longer active and removed. A constraint is considered\n",
        "        no longer active is if it is currently active\n",
        "        but the gradient for that variable points inward from the\n",
        "        constraint. The specific constraint removed is the one\n",
        "        associated with the variable of largest index whose\n",
        "        constraint is no longer active.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
        "        \n",
        "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
        "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
        "    \n",
        "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
        "        Bounded minimization for scalar functions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized (must accept and return scalars).\n",
        "        x1, x2 : float or array scalar\n",
        "            The optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to function.\n",
        "        xtol : float, optional\n",
        "            The convergence tolerance.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations allowed.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        disp : int, optional\n",
        "            If non-zero, print messages.\n",
        "                0 : no message printing.\n",
        "                1 : non-convergence notification messages only.\n",
        "                2 : print a message on convergence too.\n",
        "                3 : print iteration results.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters (over given interval) which minimize the\n",
        "            objective function.\n",
        "        fval : number\n",
        "            The function value at the minimum point.\n",
        "        ierr : int\n",
        "            An error flag (0 if converged, 1 if maximum number of\n",
        "            function calls reached).\n",
        "        numfunc : int\n",
        "          The number of function calls made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Bounded' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Finds a local minimizer of the scalar function `func` in the\n",
        "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
        "        for auto-bracketing).\n",
        "    \n",
        "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
        "        Find the roots of a function.\n",
        "        \n",
        "        Return the roots of the (non-linear) equations defined by\n",
        "        ``func(x) = 0`` given a starting estimate.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            A function that takes at least one (possibly vector) argument.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the roots of ``func(x) = 0``.\n",
        "        args : tuple, optional\n",
        "            Any extra arguments to `func`.\n",
        "        fprime : callable(x), optional\n",
        "            A function to compute the Jacobian of `func` with derivatives\n",
        "            across the rows. By default, the Jacobian will be estimated.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        col_deriv : bool, optional\n",
        "            Specify whether the Jacobian function computes derivatives down\n",
        "            the columns (faster, because there is no transpose operation).\n",
        "        xtol : float\n",
        "            The calculation will terminate if the relative error between two\n",
        "            consecutive iterates is at most `xtol`.\n",
        "        maxfev : int, optional\n",
        "            The maximum number of calls to the function. If zero, then\n",
        "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "            in `x0`.\n",
        "        band : tuple, optional\n",
        "            If set to a two-sequence containing the number of sub- and\n",
        "            super-diagonals within the band of the Jacobi matrix, the\n",
        "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "        epsfcn : float, optional\n",
        "            A suitable step length for the forward-difference\n",
        "            approximation of the Jacobian (for ``fprime=None``). If\n",
        "            `epsfcn` is less than the machine precision, it is assumed\n",
        "            that the relative errors in the functions are of the order of\n",
        "            the machine precision.\n",
        "        factor : float, optional\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``).  Should be in the interval\n",
        "            ``(0.1, 100)``.\n",
        "        diag : sequence, optional\n",
        "            N positive entries that serve as a scale factors for the\n",
        "            variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for\n",
        "            an unsuccessful call).\n",
        "        infodict : dict\n",
        "            A dictionary of optional outputs with the keys:\n",
        "        \n",
        "            ``nfev``\n",
        "                number of function calls\n",
        "            ``njev``\n",
        "                number of Jacobian calls\n",
        "            ``fvec``\n",
        "                function evaluated at the output\n",
        "            ``fjac``\n",
        "                the orthogonal matrix, q, produced by the QR\n",
        "                factorization of the final approximate Jacobian\n",
        "                matrix, stored column wise\n",
        "            ``r``\n",
        "                upper triangular matrix produced by QR factorization\n",
        "                of the same matrix\n",
        "            ``qtf``\n",
        "                the vector ``(transpose(q) * fvec)``\n",
        "        \n",
        "        ier : int\n",
        "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
        "            to `mesg` for more information.\n",
        "        mesg : str\n",
        "            If no solution is found, `mesg` details the cause of failure.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        root : Interface to root finding algorithms for multivariate\n",
        "        functions. See the 'hybr' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
        "    \n",
        "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0)\n",
        "        Return the minimum of a function of one variable.\n",
        "        \n",
        "        Given a function of one variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            Objective function to minimize.\n",
        "        args : tuple\n",
        "            Additional arguments (if present), passed to func.\n",
        "        brack : tuple\n",
        "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,\n",
        "            c), then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            x tolerance stop criterion\n",
        "        full_output : bool\n",
        "            If True, return optional outputs.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Golden' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses analog of bisection method to decrease the bracketed\n",
        "        interval.\n",
        "    \n",
        "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
        "        Minimize the sum of squares of a set of equations.\n",
        "        \n",
        "        ::\n",
        "        \n",
        "            x = arg min(sum(func(y)**2,axis=0))\n",
        "                     y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            should take at least one (possibly length N vector) argument and\n",
        "            returns M floating point numbers.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the minimization.\n",
        "        args : tuple\n",
        "            Any extra arguments to func are placed in this tuple.\n",
        "        Dfun : callable\n",
        "            A function or method to compute the Jacobian of func with derivatives\n",
        "            across the rows. If this is None, the Jacobian will be estimated.\n",
        "        full_output : bool\n",
        "            non-zero to return all optional outputs.\n",
        "        col_deriv : bool\n",
        "            non-zero to specify that the Jacobian function computes derivatives\n",
        "            down the columns (faster, because there is no transpose operation).\n",
        "        ftol : float\n",
        "            Relative error desired in the sum of squares.\n",
        "        xtol : float\n",
        "            Relative error desired in the approximate solution.\n",
        "        gtol : float\n",
        "            Orthogonality desired between the function vector and the columns of\n",
        "            the Jacobian.\n",
        "        maxfev : int\n",
        "            The maximum number of calls to the function. If zero, then 100*(N+1) is\n",
        "            the maximum where N is the number of elements in x0.\n",
        "        epsfcn : float\n",
        "            A suitable step length for the forward-difference approximation of the\n",
        "            Jacobian (for Dfun=None). If epsfcn is less than the machine precision,\n",
        "            it is assumed that the relative errors in the functions are of the\n",
        "            order of the machine precision.\n",
        "        factor : float\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "        diag : sequence\n",
        "            N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for an unsuccessful\n",
        "            call).\n",
        "        cov_x : ndarray\n",
        "            Uses the fjac and ipvt optional outputs to construct an\n",
        "            estimate of the jacobian around the solution. None if a\n",
        "            singular matrix encountered (indicates very flat curvature in\n",
        "            some direction).  This matrix must be multiplied by the\n",
        "            residual variance to get the covariance of the\n",
        "            parameter estimates -- see curve_fit.\n",
        "        infodict : dict\n",
        "            a dictionary of optional outputs with the key s:\n",
        "        \n",
        "            ``nfev``\n",
        "                The number of function calls\n",
        "            ``fvec``\n",
        "                The function evaluated at the output\n",
        "            ``fjac``\n",
        "                A permutation of the R matrix of a QR\n",
        "                factorization of the final approximate\n",
        "                Jacobian matrix, stored column wise.\n",
        "                Together with ipvt, the covariance of the\n",
        "                estimate can be approximated.\n",
        "            ``ipvt``\n",
        "                An integer array of length N which defines\n",
        "                a permutation matrix, p, such that\n",
        "                fjac*p = q*r, where r is upper triangular\n",
        "                with diagonal elements of nonincreasing\n",
        "                magnitude. Column j of p is column ipvt(j)\n",
        "                of the identity matrix.\n",
        "            ``qtf``\n",
        "                The vector (transpose(q) * fvec).\n",
        "        \n",
        "        mesg : str\n",
        "            A string message giving information about the cause of failure.\n",
        "        ier : int\n",
        "            An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\n",
        "            found.  Otherwise, the solution was not found. In either case, the\n",
        "            optional output variable 'mesg' gives more information.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
        "        \n",
        "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
        "        objective function.\n",
        "        This approximation assumes that the objective function is based on the\n",
        "        difference between some observed target data (ydata) and a (non-linear)\n",
        "        function of the parameters `f(xdata, params)` ::\n",
        "        \n",
        "               func(params) = ydata - f(xdata, params)\n",
        "        \n",
        "        so that the objective function is ::\n",
        "        \n",
        "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
        "             params\n",
        "    \n",
        "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)\n",
        "        Find alpha that satisfies strong Wolfe conditions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function.\n",
        "        myfprime : callable f'(x,*args)\n",
        "            Objective function gradient.\n",
        "        xk : ndarray\n",
        "            Starting point.\n",
        "        pk : ndarray\n",
        "            Search direction.\n",
        "        gfk : ndarray, optional\n",
        "            Gradient value for x=xk (xk being the current parameter\n",
        "            estimate). Will be recomputed if omitted.\n",
        "        old_fval : float, optional\n",
        "            Function value for x=xk. Will be recomputed if omitted.\n",
        "        old_old_fval : float, optional\n",
        "            Function value for the point preceding x=xk\n",
        "        args : tuple, optional\n",
        "            Additional arguments passed to objective function.\n",
        "        c1 : float, optional\n",
        "            Parameter for Armijo condition rule.\n",
        "        c2 : float, optional\n",
        "            Parameter for curvature condition rule.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        alpha0 : float\n",
        "            Alpha for which ``x_new = x0 + alpha * pk``.\n",
        "        fc : int\n",
        "            Number of function evaluations made.\n",
        "        gc : int\n",
        "            Number of gradient evaluations made.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses the line search algorithm to enforce strong Wolfe\n",
        "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
        "        1999, pg. 59-60.\n",
        "        \n",
        "        For the zoom phase it uses an algorithm by [...].\n",
        "    \n",
        "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a scalar Jacobian approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            The Jacobian approximation is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    minimize(fun, x0, args=(), method='BFGS', jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
        "        Minimization of scalar function of one or more variables.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its\n",
        "            derivatives (Jacobian, Hessian).\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Nelder-Mead'\n",
        "                - 'Powell'\n",
        "                - 'CG'\n",
        "                - 'BFGS'\n",
        "                - 'Newton-CG'\n",
        "                - 'Anneal'\n",
        "                - 'L-BFGS-B'\n",
        "                - 'TNC'\n",
        "                - 'COBYLA'\n",
        "                - 'SLSQP'\n",
        "                - 'dogleg'\n",
        "                - 'trust-ncg'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            Jacobian of objective function. Only for CG, BFGS, Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of the\n",
        "            objective. In this case, it must accept the same arguments as `fun`.\n",
        "        hess, hessp : callable, optional\n",
        "            Hessian of objective function or Hessian of objective function\n",
        "            times an arbitrary vector p.  Only for Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
        "            provided, then `hessp` will be ignored.  If neither `hess` nor\n",
        "            `hessp` is provided, then the hessian product will be approximated\n",
        "            using finite differences on `jac`. `hessp` must compute the Hessian\n",
        "            times an arbitrary vector.\n",
        "        bounds : sequence, optional\n",
        "            Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        constraints : dict or sequence of dict, optional\n",
        "            Constraints definition (only for COBYLA and SLSQP).\n",
        "            Each constraint is defined in a dictionary with fields:\n",
        "                type : str\n",
        "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
        "                fun : callable\n",
        "                    The function defining the constraint.\n",
        "                jac : callable, optional\n",
        "                    The Jacobian of `fun` (only for SLSQP).\n",
        "                args : sequence, optional\n",
        "                    Extra arguments to be passed to the function and Jacobian.\n",
        "            Equality constraint means that the constraint function result is to\n",
        "            be zero whereas inequality means that it is to be non-negative.\n",
        "            Note that COBYLA only supports inequality constraints.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. All methods accept the following\n",
        "            generic options:\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "            For method-specific options, see `show_options('minimize', method)`.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *BFGS*.\n",
        "        \n",
        "        **Unconstrained minimization**\n",
        "        \n",
        "        Method *Nelder-Mead* uses the Simplex algorithm [1]_, [2]_. This\n",
        "        algorithm has been successful in many applications but other algorithms\n",
        "        using the first and/or second derivatives information might be preferred\n",
        "        for their better performances and robustness in general.\n",
        "        \n",
        "        Method *Powell* is a modification of Powell's method [3]_, [4]_ which\n",
        "        is a conjugate direction method. It performs sequential one-dimensional\n",
        "        minimizations along each vector of the directions set (`direc` field in\n",
        "        `options` and `info`), which is updated at each iteration of the main\n",
        "        minimization loop. The function need not be differentiable, and no\n",
        "        derivatives are taken.\n",
        "        \n",
        "        Method *CG* uses a nonlinear conjugate gradient algorithm by Polak and\n",
        "        Ribiere, a variant of the Fletcher-Reeves method described in [5]_ pp.\n",
        "        120-122. Only the first derivatives are used.\n",
        "        \n",
        "        Method *BFGS* uses the quasi-Newton method of Broyden, Fletcher,\n",
        "        Goldfarb, and Shanno (BFGS) [5]_ pp. 136. It uses the first derivatives\n",
        "        only. BFGS has proven good performance even for non-smooth\n",
        "        optimizations. This method also returns an approximation of the Hessian\n",
        "        inverse, stored as `hess_inv` in the Result object.\n",
        "        \n",
        "        Method *Newton-CG* uses a Newton-CG algorithm [5]_ pp. 168 (also known\n",
        "        as the truncated Newton method). It uses a CG method to the compute the\n",
        "        search direction. See also *TNC* method for a box-constrained\n",
        "        minimization with a similar algorithm.\n",
        "        \n",
        "        Method *Anneal* uses simulated annealing, which is a probabilistic\n",
        "        metaheuristic algorithm for global optimization. It uses no derivative\n",
        "        information from the function being optimized.\n",
        "        \n",
        "        Method *dogleg* uses the dog-leg trust-region algorithm [5]_\n",
        "        for unconstrained minimization. This algorithm requires the gradient\n",
        "        and Hessian; furthermore the Hessian is required to be positive definite.\n",
        "        \n",
        "        Method *trust-ncg* uses the Newton conjugate gradient trust-region\n",
        "        algorithm [5]_ for unconstrained minimization. This algorithm requires\n",
        "        the gradient and either the Hessian or a function that computes the\n",
        "        product of the Hessian with a given vector.\n",
        "        \n",
        "        **Constrained minimization**\n",
        "        \n",
        "        Method *L-BFGS-B* uses the L-BFGS-B algorithm [6]_, [7]_ for bound\n",
        "        constrained minimization.\n",
        "        \n",
        "        Method *TNC* uses a truncated Newton algorithm [5]_, [8]_ to minimize a\n",
        "        function with variables subject to bounds. This algorithm uses\n",
        "        gradient information; it is also called Newton Conjugate-Gradient. It\n",
        "        differs from the *Newton-CG* method described above as it wraps a C\n",
        "        implementation and allows each variable to be given upper and lower\n",
        "        bounds.\n",
        "        \n",
        "        Method *COBYLA* uses the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method [9]_, [10]_, [11]_. The algorithm is\n",
        "        based on linear approximations to the objective function and each\n",
        "        constraint. The method wraps a FORTRAN implementation of the algorithm.\n",
        "        \n",
        "        Method *SLSQP* uses Sequential Least SQuares Programming to minimize a\n",
        "        function of several variables with any combination of bounds, equality\n",
        "        and inequality constraints. The method wraps the SLSQP Optimization\n",
        "        subroutine originally implemented by Dieter Kraft [12]_. Note that the\n",
        "        wrapper handles infinite values in bounds by converting them into large\n",
        "        floating values.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
        "            Minimization. The Computer Journal 7: 308-13.\n",
        "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
        "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
        "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
        "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
        "            191-208.\n",
        "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
        "           a function of several variables without calculating derivatives. The\n",
        "           Computer Journal 7: 155-162.\n",
        "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
        "           Numerical Recipes (any edition), Cambridge University Press.\n",
        "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
        "           Springer New York.\n",
        "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
        "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
        "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
        "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
        "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
        "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
        "           550-560.\n",
        "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
        "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
        "        .. [9] Powell, M J D. A direct search optimization method that models\n",
        "           the objective and constraint functions by linear interpolation.\n",
        "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
        "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
        "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
        "           calculations. 1998. Acta Numerica 7: 287-336.\n",
        "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
        "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
        "           2007/NA03\n",
        "        .. [12] Kraft, D. A software package for sequential quadratic\n",
        "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
        "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
        "        function (and its respective derivatives) is implemented in `rosen`\n",
        "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
        "        \n",
        "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
        "        \n",
        "        A simple application of the *Nelder-Mead* method is:\n",
        "        \n",
        "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
        "        >>> res = minimize(rosen, x0, method='Nelder-Mead')\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        \n",
        "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
        "        options:\n",
        "        \n",
        "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
        "        ...                options={'gtol': 1e-6, 'disp': True})\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 0.000000\n",
        "                 Iterations: 52\n",
        "                 Function evaluations: 64\n",
        "                 Gradient evaluations: 64\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        >>> print res.message\n",
        "        Optimization terminated successfully.\n",
        "        >>> res.hess\n",
        "        [[ 0.00749589  0.01255155  0.02396251  0.04750988  0.09495377]\n",
        "         [ 0.01255155  0.02510441  0.04794055  0.09502834  0.18996269]\n",
        "         [ 0.02396251  0.04794055  0.09631614  0.19092151  0.38165151]\n",
        "         [ 0.04750988  0.09502834  0.19092151  0.38341252  0.7664427 ]\n",
        "         [ 0.09495377  0.18996269  0.38165151  0.7664427   1.53713523]]\n",
        "        \n",
        "        \n",
        "        Next, consider a minimization problem with several constraints (namely\n",
        "        Example 16.4 from [5]_). The objective function is:\n",
        "        \n",
        "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
        "        \n",
        "        There are three constraints defined as:\n",
        "        \n",
        "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
        "        \n",
        "        And variables must be positive, hence the following bounds:\n",
        "        \n",
        "        >>> bnds = ((0, None), (0, None))\n",
        "        \n",
        "        The optimization problem is solved using the SLSQP method as:\n",
        "        \n",
        "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
        "        ...                constraints=cons)\n",
        "        \n",
        "        It should converge to the theoretical solution (1.4 ,1.7).\n",
        "    \n",
        "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
        "        Minimization of scalar function of one variable.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "            Scalar function, must return a scalar.\n",
        "        bracket : sequence, optional\n",
        "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
        "            interval and can either have three items `(a, b, c)` so that `a < b\n",
        "            < c` and `fun(b) < fun(a), fun(c)` or two items `a` and `c` which\n",
        "            are assumed to be a starting interval for a downhill bracket search\n",
        "            (see `bracket`); it doesn't always mean that the obtained solution\n",
        "            will satisfy `a <= x <= c`.\n",
        "        bounds : sequence, optional\n",
        "            For method 'bounded', `bounds` is mandatory and must have two items\n",
        "            corresponding to the optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Brent'\n",
        "                - 'Bounded'\n",
        "                - 'Golden'\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options.\n",
        "                xtol : float\n",
        "                    Relative error in solution `xopt` acceptable for\n",
        "                    convergence.\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for scalar multivariate\n",
        "            functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *Brent*.\n",
        "        \n",
        "        Method *Brent* uses Brent's algorithm to find a local minimum.\n",
        "        The algorithm uses inverse parabolic interpolation when possible to\n",
        "        speed up convergence of the golden section method.\n",
        "        \n",
        "        Method *Golden* uses the golden section search technique. It uses\n",
        "        analog of the bisection method to decrease the bracketed interval. It\n",
        "        is usually preferable to use the *Brent* method.\n",
        "        \n",
        "        Method *Bounded* can perform bounded minimization. It uses the Brent\n",
        "        method to find a local minimum in the interval x1 < xopt < x2.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Consider the problem of minimizing the following function.\n",
        "        \n",
        "        >>> def f(x):\n",
        "        ...     return (x - 2) * x * (x + 2)**2\n",
        "        \n",
        "        Using the *Brent* method, we find the local minimum as:\n",
        "        \n",
        "        >>> from scipy.optimize import minimize_scalar\n",
        "        >>> res = minimize_scalar(f)\n",
        "        >>> res.x\n",
        "        1.28077640403\n",
        "        \n",
        "        Using the *Bounded* method, we find a local minimum with specified\n",
        "        bounds as:\n",
        "        \n",
        "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
        "        >>> res.x\n",
        "        -2.0000002026\n",
        "    \n",
        "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)\n",
        "        Find a zero using the Newton-Raphson or secant method.\n",
        "        \n",
        "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
        "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
        "        is provided, otherwise the secant method is used.  If the second order\n",
        "        derivate `fprime2` of `func` is provided, parabolic Halley's method\n",
        "        is used.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            The function whose zero is wanted. It must be a function of a\n",
        "            single variable of the form f(x,a,b,c...), where a,b,c... are extra\n",
        "            arguments that can be passed in the `args` parameter.\n",
        "        x0 : float\n",
        "            An initial estimate of the zero that should be somewhere near the\n",
        "            actual zero.\n",
        "        fprime : function, optional\n",
        "            The derivative of the function when available and convenient. If it\n",
        "            is None (default), then the secant method is used.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to be used in the function call.\n",
        "        tol : float, optional\n",
        "            The allowable error of the zero value.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations.\n",
        "        fprime2 : function, optional\n",
        "            The second order derivative of the function when available and\n",
        "            convenient. If it is None (default), then the normal Newton-Raphson\n",
        "            or the secant method is used. If it is given, parabolic Halley's\n",
        "            method is used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        zero : float\n",
        "            Estimated location where function is zero.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, ridder, bisect\n",
        "        fsolve : find zeroes in n dimensions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The convergence rate of the Newton-Raphson method is quadratic,\n",
        "        the Halley method is cubic, and the secant method is\n",
        "        sub-quadratic.  This means that if the function is well behaved\n",
        "        the actual error in the estimated zero is approximately the square\n",
        "        (cube for Halley) of the requested tolerance up to roundoff\n",
        "        error. However, the stopping criterion used here is the step size\n",
        "        and there is no guarantee that a zero has been found. Consequently\n",
        "        the result should be verified. Safer algorithms are brentq,\n",
        "        brenth, ridder, and bisect, but they all require that the root\n",
        "        first be bracketed in an interval where the function changes\n",
        "        sign. The brentq algorithm is recommended for general use in one\n",
        "        dimensional problems when such an interval has been found.\n",
        "    \n",
        "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
        "        \n",
        "        This method is suitable for solving large-scale problems.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        rdiff : float, optional\n",
        "            Relative step size to use in numerical differentiation.\n",
        "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
        "            Krylov method to use to approximate the Jacobian.\n",
        "            Can be a string, or a function implementing the same interface as\n",
        "            the iterative solvers in `scipy.sparse.linalg`.\n",
        "        \n",
        "            The default is `scipy.sparse.linalg.lgmres`.\n",
        "        inner_M : LinearOperator or InverseJacobian\n",
        "            Preconditioner for the inner Krylov iteration.\n",
        "            Note that you can use also inverse Jacobians as (adaptive)\n",
        "            preconditioners. For example,\n",
        "        \n",
        "            >>> jac = BroydenFirst()\n",
        "            >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "            If the preconditioner has a method named 'update', it will be called\n",
        "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
        "            the current point, and ``f`` the current function value.\n",
        "        inner_tol, inner_maxiter, ...\n",
        "            Parameters to pass on to the \\\"inner\\\" Krylov solver.\n",
        "            See `scipy.sparse.linalg.gmres` for details.\n",
        "        outer_k : int, optional\n",
        "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
        "            See `scipy.sparse.linalg.lgmres` for details.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.sparse.linalg.gmres\n",
        "        scipy.sparse.linalg.lgmres\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This function implements a Newton-Krylov solver. The basic idea is\n",
        "        to compute the inverse of the Jacobian with an iterative Krylov\n",
        "        method. These methods require only evaluating the Jacobian-vector\n",
        "        products, which are conveniently approximated by numerical\n",
        "        differentiation:\n",
        "        \n",
        "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
        "        \n",
        "        Due to the use of iterative matrix inverses, these methods can\n",
        "        deal with large nonlinear problems.\n",
        "        \n",
        "        Scipy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
        "        solvers to choose from. The default here is `lgmres`, which is a\n",
        "        variant of restarted GMRES iteration that reuses some of the\n",
        "        information obtained in the previous Newton steps to invert\n",
        "        Jacobians in subsequent steps.\n",
        "        \n",
        "        For a review on Newton-Krylov methods, see for example [KK]_,\n",
        "        and for the LGMRES sparse inverse method, see [BJM]_.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [KK] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2003).\n",
        "        .. [BJM] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
        "                 SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
        "    \n",
        "    nnls(A, b)\n",
        "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
        "        for a FORTAN non-negative least squares solver.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : ndarray\n",
        "            Matrix ``A`` as shown above.\n",
        "        b : ndarray\n",
        "            Right-hand side vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            Solution vector.\n",
        "        rnorm : float\n",
        "            The residual, ``|| Ax-b ||_2``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The FORTRAN code was published in the book below. The algorithm\n",
        "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
        "        conditions for the non-negative least squares problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
        "    \n",
        "    ridder(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in an interval.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.\n",
        "            In particular, ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton : one-dimensional root-finding\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
        "        generally as fast as the Brent rountines. [Ridders1979]_ provides the\n",
        "        classic description and source of the algorithm. A description can also be\n",
        "        found in any recent edition of Numerical Recipes.\n",
        "        \n",
        "        The routine used here diverges slightly from standard presentations in\n",
        "        order to be a bit more careful of tolerance.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ridders1979]\n",
        "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
        "           Single Root of a Real Continuous Function.\"\n",
        "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
        "    \n",
        "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
        "        Find a root of a vector function.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            A vector function to find a root of.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its Jacobian.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'hybr'\n",
        "                - 'lm'\n",
        "                - 'broyden1'\n",
        "                - 'broyden2'\n",
        "                - 'anderson'\n",
        "                - 'linearmixing'\n",
        "                - 'diagbroyden'\n",
        "                - 'excitingmixing'\n",
        "                - 'krylov'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
        "            this case, it must accept the same arguments as `fun`.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. E.g. `xtol` or `maxiter`, see\n",
        "            ``show_options('root', method)`` for details.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : Result\n",
        "            The solution represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the algorithm exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *hybr*.\n",
        "        \n",
        "        Method *hybr* uses a modification of the Powell hybrid method as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Method *lm* solves the system of nonlinear equations in a least squares\n",
        "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
        "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
        "        with backtracking or full line searches [2]_. Each method corresponds\n",
        "        to a particular Jacobian approximations. See `nonlin` for details.\n",
        "        \n",
        "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
        "          known as Broyden's good method.\n",
        "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
        "          is known as Broyden's bad method.\n",
        "        - Method *anderson* uses (extended) Anderson mixing.\n",
        "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
        "          is suitable for large-scale problem.\n",
        "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
        "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
        "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
        "          approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "            The algorithms implemented for methods *diagbroyden*,\n",
        "            *linearmixing* and *excitingmixing* may be useful for specific\n",
        "            problems, but whether they will work may depend strongly on the\n",
        "            problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
        "           1980. User Guide for MINPACK-1.\n",
        "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
        "            Equations. Society for Industrial and Applied Mathematics.\n",
        "            <http://www.siam.org/books/kelley/>\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following functions define a system of nonlinear equations and its\n",
        "        jacobian.\n",
        "        \n",
        "        >>> def fun(x):\n",
        "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
        "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
        "        \n",
        "        >>> def jac(x):\n",
        "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
        "        ...                       -1.5 * (x[0] - x[1])**2],\n",
        "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
        "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
        "        \n",
        "        A solution can be obtained as follows.\n",
        "        \n",
        "        >>> from scipy import optimize\n",
        "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
        "        >>> sol.x\n",
        "        array([ 0.8411639,  0.1588361])\n",
        "    \n",
        "    rosen(x)\n",
        "        The Rosenbrock function.\n",
        "        \n",
        "        The function computed is::\n",
        "        \n",
        "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Rosenbrock function is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        f : float\n",
        "            The value of the Rosenbrock function.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen_der, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_der(x)\n",
        "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the derivative is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_der : (N,) ndarray\n",
        "            The gradient of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess(x)\n",
        "        The Hessian matrix of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess_prod(x, p)\n",
        "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        p : array_like\n",
        "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess_prod : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
        "            by the vector `p`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess\n",
        "    \n",
        "    show_options(solver, method=None)\n",
        "        Show documentation for additional options of optimization solvers.\n",
        "        \n",
        "        These are method-specific options that can be supplied through the\n",
        "        ``options`` dict.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        solver : str\n",
        "            Type of optimization solver. One of {`minimize`, `root`}.\n",
        "        method : str, optional\n",
        "            If not given, shows all methods of the specified solver. Otherwise,\n",
        "            show only the options for the specified method. Valid values\n",
        "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
        "            'minimize').\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \n",
        "        ** minimize options\n",
        "        \n",
        "        * BFGS options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Nelder-Mead options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "        \n",
        "        * Newton-CG options:\n",
        "            xtol : float\n",
        "                Average relative error in solution `xopt` acceptable for\n",
        "                convergence.\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * CG options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Powell options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            direc : ndarray\n",
        "                Initial set of direction vectors for the Powell method.\n",
        "        \n",
        "        * Anneal options:\n",
        "            ftol : float\n",
        "                Relative error in ``fun(x)`` acceptable for convergence.\n",
        "            schedule : str\n",
        "                Annealing schedule to use. One of: 'fast', 'cauchy' or\n",
        "                'boltzmann'.\n",
        "            T0 : float\n",
        "                Initial Temperature (estimated as 1.2 times the largest\n",
        "                cost-function deviation over random points in the range).\n",
        "            Tf : float\n",
        "                Final goal temperature.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            maxaccept : int\n",
        "                Maximum changes to accept.\n",
        "            boltzmann : float\n",
        "                Boltzmann constant in acceptance test (increase for less\n",
        "                stringent test at each temperature).\n",
        "            learn_rate : float\n",
        "                Scale constant for adjusting guesses.\n",
        "            quench, m, n : float\n",
        "                Parameters to alter fast_sa schedule.\n",
        "            lower, upper : float or ndarray\n",
        "                Lower and upper bounds on `x`.\n",
        "            dwell : int\n",
        "                The number of times to search the space at each temperature.\n",
        "        \n",
        "        * L-BFGS-B options:\n",
        "            ftol : float\n",
        "                The iteration stops when ``(f^k -\n",
        "                f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
        "            gtol : float\n",
        "                The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
        "                <= gtol`` where ``pg_i`` is the i-th component of the\n",
        "                projected gradient.\n",
        "            maxcor : int\n",
        "                The maximum number of variable metric corrections used to\n",
        "                define the limited memory matrix. (The limited memory BFGS\n",
        "                method does not store the full hessian but uses this many terms\n",
        "                in an approximation to it.)\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * TNC options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stoping criterion.\n",
        "                If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "            xtol : float\n",
        "                Precision goal for the value of x in the stopping\n",
        "                criterion (after applying x scaling factors).  If xtol <\n",
        "                0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "                -1.\n",
        "            gtol : float\n",
        "                Precision goal for the value of the projected gradient in\n",
        "                the stopping criterion (after applying x scaling factors).\n",
        "                If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
        "                Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "            scale : list of floats\n",
        "                Scaling factors to apply to each variable.  If None, the\n",
        "                factors are up-low for interval bounded variables and\n",
        "                1+|x] fo the others.  Defaults to None\n",
        "            offset : float\n",
        "                Value to substract from each variable.  If None, the\n",
        "                offsets are (up+low)/2 for interval bounded variables\n",
        "                and x for the others.\n",
        "            maxCGit : int\n",
        "                Maximum number of hessian*vector evaluations per main\n",
        "                iteration.  If maxCGit == 0, the direction chosen is\n",
        "                -gradient if maxCGit < 0, maxCGit is set to\n",
        "                max(1,min(50,n/2)).  Defaults to -1.\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluation.  if None, `maxiter` is\n",
        "                set to max(100, 10*len(x0)).  Defaults to None.\n",
        "            eta : float\n",
        "                Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "                Defaults to -1.\n",
        "            stepmx : float\n",
        "                Maximum step for the line search.  May be increased during\n",
        "                call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "            accuracy : float\n",
        "                Relative precision for finite difference calculations.  If\n",
        "                <= machine_precision, set to sqrt(machine_precision).\n",
        "                Defaults to 0.\n",
        "            minfev : float\n",
        "                Minimum function value estimate.  Defaults to 0.\n",
        "            rescale : float\n",
        "                Scaling factor (in log10) used to trigger f value\n",
        "                rescaling.  If 0, rescale at each iteration.  If a large\n",
        "                value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        \n",
        "        * COBYLA options:\n",
        "            tol : float\n",
        "                Final accuracy in the optimization (not precisely guaranteed).\n",
        "                This is a lower bound on the size of the trust region.\n",
        "            rhobeg : float\n",
        "                Reasonable initial changes to the variables.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * SLSQP options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stopping criterion.\n",
        "            eps : float\n",
        "                Step size used for numerical approximation of the jacobian.\n",
        "            maxiter : int\n",
        "                Maximum number of iterations.\n",
        "        \n",
        "        * dogleg options:\n",
        "            initial_trust_radius : float\n",
        "                Initial trust-region radius.\n",
        "            max_trust_radius : float\n",
        "                Maximum value of the trust-region radius. No steps that are longer\n",
        "                than this value will be proposed.\n",
        "            eta : float\n",
        "                Trust region related acceptance stringency for proposed steps.\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "        \n",
        "        * trust-ncg options:\n",
        "            see dogleg options.\n",
        "        \n",
        "        ** root options\n",
        "        \n",
        "        * hybrd options:\n",
        "            col_deriv : bool\n",
        "                Specify whether the Jacobian function computes derivatives down\n",
        "                the columns (faster, because there is no transpose operation).\n",
        "            xtol : float\n",
        "                The calculation will terminate if the relative error between\n",
        "                two consecutive iterates is at most `xtol`.\n",
        "            maxfev : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "                in `x0`.\n",
        "            band : sequence\n",
        "                If set to a two-sequence containing the number of sub- and\n",
        "                super-diagonals within the band of the Jacobi matrix, the\n",
        "                Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation\n",
        "                of the Jacobian (for ``fprime=None``). If `epsfcn` is less than\n",
        "                the machine precision, it is assumed that the relative errors\n",
        "                in the functions are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound (``factor * ||\n",
        "                diag * x||``).  Should be in the interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the\n",
        "                variables.\n",
        "        \n",
        "        * LM options:\n",
        "            col_deriv : bool\n",
        "                non-zero to specify that the Jacobian function computes derivatives\n",
        "                down the columns (faster, because there is no transpose operation).\n",
        "            ftol : float\n",
        "                Relative error desired in the sum of squares.\n",
        "            xtol : float\n",
        "                Relative error desired in the approximate solution.\n",
        "            gtol : float\n",
        "                Orthogonality desired between the function vector and the columns\n",
        "                of the Jacobian.\n",
        "            maxiter : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                100*(N+1) is the maximum where N is the number of elements in x0.\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation of\n",
        "                the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
        "                precision, it is assumed that the relative errors in the functions\n",
        "                are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound\n",
        "                (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        * Broyden1 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Broyden2 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Anderson options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    M : float, optional\n",
        "                        Number of previous vectors to retain. Defaults to 5.\n",
        "                    w0 : float, optional\n",
        "                        Regularization parameter for numerical stability.\n",
        "                        Compared to unity, good values of the order of 0.01.\n",
        "        \n",
        "        * LinearMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * DiagBroyden options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * ExcitingMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial Jacobian approximation is (-1/alpha).\n",
        "                    alphamax : float, optional\n",
        "                        The entries of the diagonal Jacobian are kept in the range\n",
        "                        ``[alpha, alphamax]``.\n",
        "        \n",
        "        * Krylov options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    rdiff : float, optional\n",
        "                        Relative step size to use in numerical differentiation.\n",
        "                    method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or\n",
        "                        function\n",
        "                        Krylov method to use to approximate the Jacobian.\n",
        "                        Can be a string, or a function implementing the same\n",
        "                        interface as the iterative solvers in\n",
        "                        `scipy.sparse.linalg`.\n",
        "        \n",
        "                        The default is `scipy.sparse.linalg.lgmres`.\n",
        "                    inner_M : LinearOperator or InverseJacobian\n",
        "                        Preconditioner for the inner Krylov iteration.\n",
        "                        Note that you can use also inverse Jacobians as (adaptive)\n",
        "                        preconditioners. For example,\n",
        "        \n",
        "                        >>> jac = BroydenFirst()\n",
        "                        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "                        If the preconditioner has a method named 'update', it will\n",
        "                        be called as ``update(x, f)`` after each nonlinear step,\n",
        "                        with ``x`` giving the current point, and ``f`` the current\n",
        "                        function value.\n",
        "                    inner_tol, inner_maxiter, ...\n",
        "                        Parameters to pass on to the \"inner\" Krylov solver.\n",
        "                        See `scipy.sparse.linalg.gmres` for details.\n",
        "                    outer_k : int, optional\n",
        "                        Size of the subspace kept across LGMRES nonlinear\n",
        "                        iterations.\n",
        "        \n",
        "                        See `scipy.sparse.linalg.lgmres` for details.\n",
        "\n",
        "DATA\n",
        "    __all__ = ['OptimizeWarning', 'Result', 'absolute_import', 'anderson',...\n",
        "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(optimize.fmin)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function fmin in module scipy.optimize.optimize:\n",
        "\n",
        "fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "    Minimize a function using the downhill simplex algorithm.\n",
        "    \n",
        "    This algorithm only uses function values, not derivatives or second\n",
        "    derivatives.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    func : callable func(x,*args)\n",
        "        The objective function to be minimized.\n",
        "    x0 : ndarray\n",
        "        Initial guess.\n",
        "    args : tuple, optional\n",
        "        Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "    callback : callable, optional\n",
        "        Called after each iteration, as callback(xk), where xk is the\n",
        "        current parameter vector.\n",
        "    xtol : float, optional\n",
        "        Relative error in xopt acceptable for convergence.\n",
        "    ftol : number, optional\n",
        "        Relative error in func(xopt) acceptable for convergence.\n",
        "    maxiter : int, optional\n",
        "        Maximum number of iterations to perform.\n",
        "    maxfun : number, optional\n",
        "        Maximum number of function evaluations to make.\n",
        "    full_output : bool, optional\n",
        "        Set to True if fopt and warnflag outputs are desired.\n",
        "    disp : bool, optional\n",
        "        Set to True to print convergence messages.\n",
        "    retall : bool, optional\n",
        "        Set to True to return list of solutions at each iteration.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    xopt : ndarray\n",
        "        Parameter that minimizes function.\n",
        "    fopt : float\n",
        "        Value of function at minimum: ``fopt = func(xopt)``.\n",
        "    iter : int\n",
        "        Number of iterations performed.\n",
        "    funcalls : int\n",
        "        Number of function calls made.\n",
        "    warnflag : int\n",
        "        1 : Maximum number of function evaluations made.\n",
        "        2 : Maximum number of iterations reached.\n",
        "    allvecs : list\n",
        "        Solution at each iteration.\n",
        "    \n",
        "    See also\n",
        "    --------\n",
        "    minimize: Interface to minimization algorithms for multivariate\n",
        "        functions. See the 'Nelder-Mead' `method` in particular.\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "    one or more variables.\n",
        "    \n",
        "    This algorithm has a long history of successful use in applications.\n",
        "    But it will usually be slower than an algorithm that uses first or\n",
        "    second derivative information. In practice it can have poor\n",
        "    performance in high-dimensional problems and is not robust to\n",
        "    minimizing complicated functions. Additionally, there currently is no\n",
        "    complete theory describing when the algorithm will successfully\n",
        "    converge to the minimum, or how fast it will if it does.\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "           minimization\", The Computer Journal, 7, pp. 308-313\n",
        "    \n",
        "    .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "           Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "           1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "           Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "           Harlow, UK, pp. 191-208.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f_test(x):\n",
      "    return np.dot(x,x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(f_test, np.array([1,2]), xtol = 1.E-10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 85\n",
        "         Function evaluations: 164\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 156,
       "text": [
        "array([  3.31355231e-11,  -1.82983207e-11])"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (y_rand - A * sin(b*x))**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_rand_cut = y_rand[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_cut = x[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def objective_func(A):\n",
      "    return sum((y_rand - A[0] * sin(A[1] * x))**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_fit = optimize.fmin(objective_func, np.array([1,0.9]), xtol = 1.E-10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 4007.916753\n",
        "         Iterations: 80\n",
        "         Function evaluations: 166\n"
       ]
      }
     ],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 175,
       "text": [
        "array([ 0.12495258,  0.9033509 ])"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "objective_func(A_fit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 170,
       "text": [
        "4046.4407917753956"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x, y_rand, 'b', linestyle = ':')\n",
      "plb.plot(x, A_fit[0] * sin(A_fit[1] * x), 'r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 171,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b1c0650>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b1c0850>"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}